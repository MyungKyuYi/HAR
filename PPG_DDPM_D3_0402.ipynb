{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c959315-11ef-4c28-aabe-4ee52629dc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from abc import abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd79bc33-41a4-4000-8e54-31d93dcfce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f986da2-d421-461d-8a9e-e1866b8a0951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "de663635-d08b-4fbe-a81a-9aa67ef95972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.json - PPG length: 280000, BP measurements: 4\n",
      "2.json - PPG length: 356000, BP measurements: 6\n",
      "3.json - PPG length: 346000, BP measurements: 6\n",
      "4.json - PPG length: 555000, BP measurements: 7\n",
      "5.json - PPG length: 509000, BP measurements: 7\n",
      "6.json - PPG length: 270000, BP measurements: 5\n",
      "7.json - PPG length: 479000, BP measurements: 6\n",
      "8.json - PPG length: 438000, BP measurements: 6\n",
      "9.json - PPG length: 342000, BP measurements: 6\n",
      "10.json - PPG length: 299000, BP measurements: 5\n",
      "11.json - PPG length: 225000, BP measurements: 6\n",
      "12.json - PPG length: 387000, BP measurements: 6\n",
      "13.json - PPG length: 404000, BP measurements: 6\n",
      "14.json - PPG length: 413000, BP measurements: 6\n",
      "15.json - PPG length: 355000, BP measurements: 6\n",
      "16.json - PPG length: 396000, BP measurements: 6\n",
      "17.json - PPG length: 346000, BP measurements: 6\n",
      "18.json - PPG length: 346000, BP measurements: 6\n",
      "19.json - PPG length: 383000, BP measurements: 6\n",
      "20.json - PPG length: 393000, BP measurements: 6\n",
      "21.json - PPG length: 398000, BP measurements: 6\n",
      "22.json - PPG length: 518000, BP measurements: 7\n",
      "23.json - PPG length: 312000, BP measurements: 4\n",
      "24.json - PPG length: 313000, BP measurements: 5\n",
      "25.json - PPG length: 358000, BP measurements: 6\n",
      "26.json - PPG length: 361000, BP measurements: 6\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from os.path import join\n",
    "\n",
    "input_path = 'E:/dataset/BP-ES/'\n",
    "\n",
    "for pid in range(1, 27):\n",
    "    with open(join(input_path, f'{pid}.json'), 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    ppg_len = len(data['data_PPG'])\n",
    "    num_bp = len(data['data_BP'])\n",
    "    \n",
    "    print(f\"{pid}.json - PPG length: {ppg_len}, BP measurements: {num_bp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "13faceb4-90cc-4804-b286-330a0583d4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ìµœì¢… ìƒ˜í”Œ ìˆ˜: 152\n",
      "âœ… ì»¬ëŸ¼ ìˆ˜: 2005\n",
      "   UID  age  height  SBP  DBP  PPG_0  PPG_1  PPG_2  PPG_3  PPG_4  ...  \\\n",
      "0    1   25     175  141   82   2067   2074   2066   2072   2072  ...   \n",
      "1    1   25     175  134   73   2226   2223   2222   2226   2219  ...   \n",
      "\n",
      "   PPG_1990  PPG_1991  PPG_1992  PPG_1993  PPG_1994  PPG_1995  PPG_1996  \\\n",
      "0      2248      2155      2248      2249      2250      2250      2066   \n",
      "1      2085      2088      2090      2090      2094      2094      2098   \n",
      "\n",
      "   PPG_1997  PPG_1998  PPG_1999  \n",
      "0      2250      2251      2250  \n",
      "1      2099      2102      2104  \n",
      "\n",
      "[2 rows x 2005 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from os.path import join\n",
    "from scipy import signal\n",
    "\n",
    "# FSR ê¸°ë°˜ ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def rolling_window(a, window):\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
    "\n",
    "def find_mins(a, num_mins, window):\n",
    "    found_mins = []\n",
    "    amax = a.max()\n",
    "    hwindow = window // 2\n",
    "    a = np.array(a)\n",
    "    for _ in range(num_mins):\n",
    "        found_min = np.argmin(a)\n",
    "        found_mins.append(found_min)\n",
    "        a[found_min - hwindow:found_min + hwindow] = amax\n",
    "    return sorted(found_mins)\n",
    "\n",
    "# -----------------------------\n",
    "input_path = 'E:/dataset/BP-ES/'\n",
    "ppg_window = 1000  # Â±1000\n",
    "segment_length = 2000\n",
    "min_window = 15000\n",
    "diff_n = 1000\n",
    "roll_window = 21\n",
    "\n",
    "records = []\n",
    "\n",
    "for pid in range(1, 27):\n",
    "    with open(join(input_path, f'{pid}.json'), 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    age = data.get('age', None)\n",
    "    height = data.get('height', None)\n",
    "    data_PPG = np.array(data['data_PPG'])\n",
    "    data_FSR = -np.array(data['data_FSR'], dtype=np.float64)\n",
    "    data_BP = data['data_BP']\n",
    "    ppg_len = len(data_PPG)\n",
    "    num_bp = len(data_BP)\n",
    "\n",
    "    # ì´ìƒê°’ ì œê±°\n",
    "    max_diff = 50\n",
    "    data_FSR_outliers = np.abs(data_FSR[1:] - data_FSR[:-1]) > max_diff\n",
    "    data_FSR_outliers = np.append(data_FSR_outliers, False)\n",
    "    data_FSR[data_FSR_outliers] = np.nan\n",
    "\n",
    "    # NaN ë³´ê°„\n",
    "    data_FSR_roll_mean = np.nanmean(rolling_window(data_FSR, 10), axis=-1)\n",
    "    data_FSR[np.isnan(data_FSR)] = data_FSR_roll_mean[np.isnan(data_FSR)[:1 - 10]]\n",
    "    data_FSR_smooth = signal.savgol_filter(data_FSR, 51, 0)\n",
    "\n",
    "    # FSR ë³€í™”ëŸ‰\n",
    "    data_FSR_diff = data_FSR_smooth[diff_n:] - data_FSR_smooth[:-diff_n]\n",
    "    data_FSR_diff_roll = rolling_window(data_FSR_diff, roll_window).mean(axis=-1)\n",
    "\n",
    "    # BP ì¸¡ì • ì‹œì  ì°¾ê¸°\n",
    "    data_FSR_mins = find_mins(data_FSR_diff_roll, num_bp, min_window)\n",
    "    aligned_mins = [int(m + diff_n / 2) for m in data_FSR_mins]\n",
    "\n",
    "    # ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ\n",
    "    for i, center in enumerate(aligned_mins):\n",
    "        sbp = data_BP[i]['SBP']\n",
    "        dbp = data_BP[i]['DBP']\n",
    "        start = center - ppg_window\n",
    "        end = center + ppg_window\n",
    "\n",
    "        if start >= 0 and end <= ppg_len:\n",
    "            segment = data_PPG[start:end]\n",
    "            row = {\n",
    "                'UID': pid,\n",
    "                'age': age,\n",
    "                'height': height,\n",
    "                'SBP': sbp,\n",
    "                'DBP': dbp\n",
    "            }\n",
    "            # PPG_0 ~ PPG_1999\n",
    "            for j, val in enumerate(segment):\n",
    "                row[f'PPG_{j}'] = val\n",
    "            records.append(row)\n",
    "        else:\n",
    "            print(f\"ðŸ”¸ Skip: UID={pid}, BP_idx={i+1}, out-of-bounds [{start}:{end}]\")\n",
    "\n",
    "# DataFrame ìƒì„±\n",
    "df_all_flat = pd.DataFrame(records)\n",
    "print(\"âœ… ìµœì¢… ìƒ˜í”Œ ìˆ˜:\", len(df_all_flat))\n",
    "print(\"âœ… ì»¬ëŸ¼ ìˆ˜:\", df_all_flat.shape[1])\n",
    "print(df_all_flat.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "57bf01c0-6bae-4cdd-ade8-3aca278eae31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 2005)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "89d5d18a-5dbb-45d5-95ff-4d7175599522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UID\n",
       "4     7\n",
       "5     7\n",
       "22    7\n",
       "14    6\n",
       "2     6\n",
       "25    6\n",
       "21    6\n",
       "20    6\n",
       "19    6\n",
       "18    6\n",
       "17    6\n",
       "16    6\n",
       "15    6\n",
       "26    6\n",
       "13    6\n",
       "12    6\n",
       "11    6\n",
       "9     6\n",
       "8     6\n",
       "7     6\n",
       "3     6\n",
       "10    5\n",
       "6     5\n",
       "24    5\n",
       "23    4\n",
       "1     4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_flat['UID'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fcbc1e24-7103-4c82-892b-2fdecebcc962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>SBP</th>\n",
       "      <th>DBP</th>\n",
       "      <th>PPG_0</th>\n",
       "      <th>PPG_1</th>\n",
       "      <th>PPG_2</th>\n",
       "      <th>PPG_3</th>\n",
       "      <th>PPG_4</th>\n",
       "      <th>...</th>\n",
       "      <th>PPG_1990</th>\n",
       "      <th>PPG_1991</th>\n",
       "      <th>PPG_1992</th>\n",
       "      <th>PPG_1993</th>\n",
       "      <th>PPG_1994</th>\n",
       "      <th>PPG_1995</th>\n",
       "      <th>PPG_1996</th>\n",
       "      <th>PPG_1997</th>\n",
       "      <th>PPG_1998</th>\n",
       "      <th>PPG_1999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>175</td>\n",
       "      <td>141</td>\n",
       "      <td>82</td>\n",
       "      <td>2067</td>\n",
       "      <td>2074</td>\n",
       "      <td>2066</td>\n",
       "      <td>2072</td>\n",
       "      <td>2072</td>\n",
       "      <td>...</td>\n",
       "      <td>2248</td>\n",
       "      <td>2155</td>\n",
       "      <td>2248</td>\n",
       "      <td>2249</td>\n",
       "      <td>2250</td>\n",
       "      <td>2250</td>\n",
       "      <td>2066</td>\n",
       "      <td>2250</td>\n",
       "      <td>2251</td>\n",
       "      <td>2250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>175</td>\n",
       "      <td>134</td>\n",
       "      <td>73</td>\n",
       "      <td>2226</td>\n",
       "      <td>2223</td>\n",
       "      <td>2222</td>\n",
       "      <td>2226</td>\n",
       "      <td>2219</td>\n",
       "      <td>...</td>\n",
       "      <td>2085</td>\n",
       "      <td>2088</td>\n",
       "      <td>2090</td>\n",
       "      <td>2090</td>\n",
       "      <td>2094</td>\n",
       "      <td>2094</td>\n",
       "      <td>2098</td>\n",
       "      <td>2099</td>\n",
       "      <td>2102</td>\n",
       "      <td>2104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>175</td>\n",
       "      <td>125</td>\n",
       "      <td>70</td>\n",
       "      <td>2130</td>\n",
       "      <td>2130</td>\n",
       "      <td>2126</td>\n",
       "      <td>2123</td>\n",
       "      <td>2130</td>\n",
       "      <td>...</td>\n",
       "      <td>2347</td>\n",
       "      <td>2351</td>\n",
       "      <td>2351</td>\n",
       "      <td>2354</td>\n",
       "      <td>2354</td>\n",
       "      <td>2354</td>\n",
       "      <td>2354</td>\n",
       "      <td>2354</td>\n",
       "      <td>2354</td>\n",
       "      <td>2354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>175</td>\n",
       "      <td>118</td>\n",
       "      <td>57</td>\n",
       "      <td>2114</td>\n",
       "      <td>2113</td>\n",
       "      <td>2112</td>\n",
       "      <td>2112</td>\n",
       "      <td>2112</td>\n",
       "      <td>...</td>\n",
       "      <td>2216</td>\n",
       "      <td>2216</td>\n",
       "      <td>2207</td>\n",
       "      <td>2207</td>\n",
       "      <td>2207</td>\n",
       "      <td>2207</td>\n",
       "      <td>2207</td>\n",
       "      <td>2206</td>\n",
       "      <td>2200</td>\n",
       "      <td>2200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>179</td>\n",
       "      <td>152</td>\n",
       "      <td>84</td>\n",
       "      <td>1961</td>\n",
       "      <td>1962</td>\n",
       "      <td>1960</td>\n",
       "      <td>1962</td>\n",
       "      <td>1960</td>\n",
       "      <td>...</td>\n",
       "      <td>2070</td>\n",
       "      <td>2066</td>\n",
       "      <td>2063</td>\n",
       "      <td>2062</td>\n",
       "      <td>2062</td>\n",
       "      <td>2060</td>\n",
       "      <td>2057</td>\n",
       "      <td>2056</td>\n",
       "      <td>2056</td>\n",
       "      <td>2056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>26</td>\n",
       "      <td>24</td>\n",
       "      <td>170</td>\n",
       "      <td>146</td>\n",
       "      <td>80</td>\n",
       "      <td>1674</td>\n",
       "      <td>1674</td>\n",
       "      <td>1676</td>\n",
       "      <td>1675</td>\n",
       "      <td>1674</td>\n",
       "      <td>...</td>\n",
       "      <td>1960</td>\n",
       "      <td>1960</td>\n",
       "      <td>1955</td>\n",
       "      <td>1955</td>\n",
       "      <td>1954</td>\n",
       "      <td>1950</td>\n",
       "      <td>1950</td>\n",
       "      <td>1946</td>\n",
       "      <td>1946</td>\n",
       "      <td>1946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>26</td>\n",
       "      <td>24</td>\n",
       "      <td>170</td>\n",
       "      <td>142</td>\n",
       "      <td>68</td>\n",
       "      <td>1809</td>\n",
       "      <td>1807</td>\n",
       "      <td>1804</td>\n",
       "      <td>1804</td>\n",
       "      <td>1801</td>\n",
       "      <td>...</td>\n",
       "      <td>1912</td>\n",
       "      <td>1906</td>\n",
       "      <td>1906</td>\n",
       "      <td>1902</td>\n",
       "      <td>1902</td>\n",
       "      <td>1902</td>\n",
       "      <td>1899</td>\n",
       "      <td>1897</td>\n",
       "      <td>1896</td>\n",
       "      <td>1896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>26</td>\n",
       "      <td>24</td>\n",
       "      <td>170</td>\n",
       "      <td>135</td>\n",
       "      <td>73</td>\n",
       "      <td>1771</td>\n",
       "      <td>1768</td>\n",
       "      <td>1768</td>\n",
       "      <td>1767</td>\n",
       "      <td>1766</td>\n",
       "      <td>...</td>\n",
       "      <td>1813</td>\n",
       "      <td>1817</td>\n",
       "      <td>1811</td>\n",
       "      <td>1807</td>\n",
       "      <td>1809</td>\n",
       "      <td>1810</td>\n",
       "      <td>1807</td>\n",
       "      <td>1807</td>\n",
       "      <td>1805</td>\n",
       "      <td>1802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>26</td>\n",
       "      <td>24</td>\n",
       "      <td>170</td>\n",
       "      <td>129</td>\n",
       "      <td>70</td>\n",
       "      <td>1760</td>\n",
       "      <td>1754</td>\n",
       "      <td>1753</td>\n",
       "      <td>1755</td>\n",
       "      <td>1749</td>\n",
       "      <td>...</td>\n",
       "      <td>1935</td>\n",
       "      <td>1931</td>\n",
       "      <td>1930</td>\n",
       "      <td>1930</td>\n",
       "      <td>1928</td>\n",
       "      <td>1923</td>\n",
       "      <td>1920</td>\n",
       "      <td>1920</td>\n",
       "      <td>1915</td>\n",
       "      <td>1913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>26</td>\n",
       "      <td>24</td>\n",
       "      <td>170</td>\n",
       "      <td>129</td>\n",
       "      <td>70</td>\n",
       "      <td>1912</td>\n",
       "      <td>1907</td>\n",
       "      <td>1907</td>\n",
       "      <td>1902</td>\n",
       "      <td>1903</td>\n",
       "      <td>...</td>\n",
       "      <td>1766</td>\n",
       "      <td>1770</td>\n",
       "      <td>1775</td>\n",
       "      <td>1779</td>\n",
       "      <td>1782</td>\n",
       "      <td>1786</td>\n",
       "      <td>1792</td>\n",
       "      <td>1792</td>\n",
       "      <td>1797</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152 rows Ã— 2005 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     UID  age  height  SBP  DBP  PPG_0  PPG_1  PPG_2  PPG_3  PPG_4  ...  \\\n",
       "0      1   25     175  141   82   2067   2074   2066   2072   2072  ...   \n",
       "1      1   25     175  134   73   2226   2223   2222   2226   2219  ...   \n",
       "2      1   25     175  125   70   2130   2130   2126   2123   2130  ...   \n",
       "3      1   25     175  118   57   2114   2113   2112   2112   2112  ...   \n",
       "4      2   24     179  152   84   1961   1962   1960   1962   1960  ...   \n",
       "..   ...  ...     ...  ...  ...    ...    ...    ...    ...    ...  ...   \n",
       "147   26   24     170  146   80   1674   1674   1676   1675   1674  ...   \n",
       "148   26   24     170  142   68   1809   1807   1804   1804   1801  ...   \n",
       "149   26   24     170  135   73   1771   1768   1768   1767   1766  ...   \n",
       "150   26   24     170  129   70   1760   1754   1753   1755   1749  ...   \n",
       "151   26   24     170  129   70   1912   1907   1907   1902   1903  ...   \n",
       "\n",
       "     PPG_1990  PPG_1991  PPG_1992  PPG_1993  PPG_1994  PPG_1995  PPG_1996  \\\n",
       "0        2248      2155      2248      2249      2250      2250      2066   \n",
       "1        2085      2088      2090      2090      2094      2094      2098   \n",
       "2        2347      2351      2351      2354      2354      2354      2354   \n",
       "3        2216      2216      2207      2207      2207      2207      2207   \n",
       "4        2070      2066      2063      2062      2062      2060      2057   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "147      1960      1960      1955      1955      1954      1950      1950   \n",
       "148      1912      1906      1906      1902      1902      1902      1899   \n",
       "149      1813      1817      1811      1807      1809      1810      1807   \n",
       "150      1935      1931      1930      1930      1928      1923      1920   \n",
       "151      1766      1770      1775      1779      1782      1786      1792   \n",
       "\n",
       "     PPG_1997  PPG_1998  PPG_1999  \n",
       "0        2250      2251      2250  \n",
       "1        2099      2102      2104  \n",
       "2        2354      2354      2354  \n",
       "3        2206      2200      2200  \n",
       "4        2056      2056      2056  \n",
       "..        ...       ...       ...  \n",
       "147      1946      1946      1946  \n",
       "148      1897      1896      1896  \n",
       "149      1807      1805      1802  \n",
       "150      1920      1915      1913  \n",
       "151      1792      1797      1800  \n",
       "\n",
       "[152 rows x 2005 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3e5dce6e-65a3-418e-8ae7-dfeee5abc7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   UID  SBP  DBP  Hypertion\n",
      "0    1  141   82          1\n",
      "1    1  134   73          0\n",
      "2    1  125   70          0\n",
      "3    1  118   57          0\n",
      "4    2  152   84          1\n",
      "5    2  141   81          1\n",
      "6    2  122   76          0\n",
      "7    2  123   69          0\n",
      "8    2  122   70          0\n",
      "9    2  120   69          0\n",
      "ê³ í˜ˆì•• ìƒ˜í”Œ ìˆ˜: 58\n",
      "ì •ìƒ ìƒ˜í”Œ ìˆ˜: 94\n"
     ]
    }
   ],
   "source": [
    "# ê³ í˜ˆì•• ì—¬ë¶€ íŒë‹¨ í•¨ìˆ˜\n",
    "def is_hypertension(sbp, dbp):\n",
    "    return int(sbp >= 140 or dbp >= 90)\n",
    "\n",
    "# Hypertion ì»¬ëŸ¼ ì¶”ê°€\n",
    "df_all_flat['Hypertion'] = df_all_flat.apply(lambda row: is_hypertension(row['SBP'], row['DBP']), axis=1)\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(df_all_flat[['UID', 'SBP', 'DBP', 'Hypertion']].head(10))\n",
    "print(\"ê³ í˜ˆì•• ìƒ˜í”Œ ìˆ˜:\", df_all_flat['Hypertion'].sum())\n",
    "print(\"ì •ìƒ ìƒ˜í”Œ ìˆ˜:\", (df_all_flat['Hypertion'] == 0).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0eaf4cea-0574-4f5c-8633-0797945a26b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 'ppg_all_flat.csv' íŒŒì¼ë¡œ ì €ìž¥ ì™„ë£Œ.\n"
     ]
    }
   ],
   "source": [
    "df_all_flat.to_csv('E:/dataset/BP-ES/ppg_all_flat.csv', index=False)\n",
    "print(\"âœ… 'ppg_all_flat.csv' íŒŒì¼ë¡œ ì €ìž¥ ì™„ë£Œ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4624ce9e-183f-4a11-9028-1ae9769b8d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   UID  Num_Samples  Hypertion\n",
      "0    1            4          1\n",
      "1    2            6          1\n",
      "2    3            6          0\n",
      "3    4            7          1\n",
      "4    5            7          1\n",
      "5    6            5          1\n",
      "6    7            6          1\n",
      "7    8            6          1\n",
      "8    9            6          1\n",
      "9   10            5          0\n",
      "\n",
      "ì „ì²´ í™˜ìž ìˆ˜: 26\n",
      "  - ê³ í˜ˆì•• í™˜ìž ìˆ˜: 19\n",
      "  - ì •ìƒ í™˜ìž ìˆ˜: 7\n"
     ]
    }
   ],
   "source": [
    "# UIDë³„ ìƒ˜í”Œ ìˆ˜ ì§‘ê³„\n",
    "uid_sample_counts = df_all_flat.groupby('UID').size().reset_index(name='Num_Samples')\n",
    "\n",
    "# UIDë³„ ê³ í˜ˆì•• ì—¬ë¶€ ì§‘ê³„ (1ê°œë¼ë„ ê³ í˜ˆì••ì´ë©´ ì „ì²´ ê³ í˜ˆì••ìžë¡œ ê°„ì£¼)\n",
    "uid_hyper_status = df_all_flat.groupby('UID')['Hypertion'].max().reset_index()\n",
    "\n",
    "# ë³‘í•©\n",
    "uid_summary = pd.merge(uid_sample_counts, uid_hyper_status, on='UID')\n",
    "\n",
    "# Hypertionì„ intë¡œ\n",
    "uid_summary['Hypertion'] = uid_summary['Hypertion'].astype(int)\n",
    "\n",
    "# ì •ë ¬ (ì„ íƒ)\n",
    "uid_summary = uid_summary.sort_values('UID').reset_index(drop=True)\n",
    "\n",
    "# ì¶œë ¥\n",
    "print(uid_summary.head(10))\n",
    "print(f\"\\nì „ì²´ í™˜ìž ìˆ˜: {len(uid_summary)}\")\n",
    "print(f\"  - ê³ í˜ˆì•• í™˜ìž ìˆ˜: {uid_summary['Hypertion'].sum()}\")\n",
    "print(f\"  - ì •ìƒ í™˜ìž ìˆ˜: {(uid_summary['Hypertion'] == 0).sum()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1fea8fc8-8326-4ee9-8954-82f712d2c3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   UID  SBP  DBP  Hypertion\n",
      "0    1  141   82          1\n",
      "1    1  134   73          0\n",
      "2    1  125   70          0\n",
      "3    1  118   57          0\n",
      "4    2  152   84          1\n",
      "5    2  141   81          1\n",
      "6    2  122   76          0\n",
      "7    2  123   69          0\n",
      "8    2  122   70          0\n",
      "9    2  120   69          0\n",
      "ê³ í˜ˆì•• ìƒ˜í”Œ ìˆ˜: 58\n",
      "ì •ìƒ ìƒ˜í”Œ ìˆ˜: 94\n"
     ]
    }
   ],
   "source": [
    "# ê³ í˜ˆì•• ì—¬ë¶€ íŒë‹¨ í•¨ìˆ˜\n",
    "def is_hypertension(sbp, dbp):\n",
    "    return int(sbp >= 140 or dbp >= 90)\n",
    "\n",
    "# Hypertion ì»¬ëŸ¼ ì¶”ê°€\n",
    "df_all_flat['Hypertion'] = df_all_flat.apply(lambda row: is_hypertension(row['SBP'], row['DBP']), axis=1)\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(df_all_flat[['UID', 'SBP', 'DBP', 'hypertension']].head(10))\n",
    "print(\"ê³ í˜ˆì•• ìƒ˜í”Œ ìˆ˜:\", df_all_flat['hypertension'].sum())\n",
    "print(\"ì •ìƒ ìƒ˜í”Œ ìˆ˜:\", (df_all_flat['hypertension'] == 0).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aef5ba32-6ad2-4f62-8565-e2ad032dd263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    UID  Total_Segments  Normal_Segments  Hyper_Segments\n",
      "0     1               4                3               1\n",
      "1     2               6                4               2\n",
      "2     3               6                6               0\n",
      "3     4               7                0               7\n",
      "4     5               7                5               2\n",
      "5     6               5                2               3\n",
      "6     7               6                1               5\n",
      "7     8               6                2               4\n",
      "8     9               6                1               5\n",
      "9    10               5                5               0\n",
      "10   11               6                2               4\n",
      "11   12               6                3               3\n",
      "12   13               6                3               3\n",
      "13   14               6                3               3\n",
      "14   15               6                6               0\n",
      "15   16               6                5               1\n",
      "16   17               6                6               0\n",
      "17   18               6                6               0\n",
      "18   19               6                3               3\n",
      "19   20               6                6               0\n",
      "20   21               6                3               3\n",
      "21   22               7                4               3\n",
      "22   23               4                3               1\n",
      "23   24               5                3               2\n",
      "24   25               6                6               0\n",
      "25   26               6                3               3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜\n",
    "total_per_uid = df_all_flat.groupby('UID').size().rename('Total_Segments')\n",
    "\n",
    "# ì •ìƒ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ (Hypertion == 0)\n",
    "normal_per_uid = df_all_flat[df_all_flat['Hypertion'] == 0].groupby('UID').size().rename('Normal_Segments')\n",
    "\n",
    "# ê³ í˜ˆì•• ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ (Hypertion == 1)\n",
    "hyper_per_uid = df_all_flat[df_all_flat['Hypertion'] == 1].groupby('UID').size().rename('Hyper_Segments')\n",
    "\n",
    "# ë³‘í•©\n",
    "uid_segment_summary = pd.concat([total_per_uid, normal_per_uid, hyper_per_uid], axis=1).fillna(0)\n",
    "uid_segment_summary = uid_segment_summary.astype(int).reset_index()\n",
    "\n",
    "# UID ìˆœ ì •ë ¬\n",
    "uid_segment_summary = uid_segment_summary.sort_values('UID').reset_index(drop=True)\n",
    "\n",
    "# ì¶œë ¥\n",
    "print(uid_segment_summary)\n",
    "\n",
    "# ì„ íƒ ì €ìž¥\n",
    "uid_segment_summary.to_csv(\"uid_segment_summary_table.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a48b1835-ff34-4705-99f9-def4c666fea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ê³ í˜ˆì••(Hypertion=1) ì„¸ê·¸ë¨¼íŠ¸ê°€ í•˜ë‚˜ë„ ì—†ëŠ” UID ëª©ë¡:\n",
      "[3, 10, 15, 17, 18, 20, 25]\n",
      "ì´ 7ëª…\n"
     ]
    }
   ],
   "source": [
    "# UIDë³„ ê³ í˜ˆì•• ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ ì§‘ê³„\n",
    "hyper_per_uid = df_all_flat[df_all_flat['Hypertion'] == 1].groupby('UID').size().rename('Hyper_Segments')\n",
    "\n",
    "# ì „ì²´ UID ëª©ë¡\n",
    "all_uids = df_all_flat['UID'].unique()\n",
    "\n",
    "# ê³ í˜ˆì•• ì„¸ê·¸ë¨¼íŠ¸ê°€ 0ê°œì¸ UID = ì „ì²´ UID ì¤‘ hyper_per_uidì— ì—†ëŠ” UID\n",
    "uids_no_hypertension = [uid for uid in all_uids if uid not in hyper_per_uid.index]\n",
    "\n",
    "print(\"âœ… ê³ í˜ˆì••(Hypertion=1) ì„¸ê·¸ë¨¼íŠ¸ê°€ í•˜ë‚˜ë„ ì—†ëŠ” UID ëª©ë¡:\")\n",
    "print(uids_no_hypertension)\n",
    "\n",
    "print(f\"ì´ {len(uids_no_hypertension)}ëª…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d4d58ce4-b3a4-4e79-aa9d-ddc00a5e00b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í›ˆë ¨ ë°ì´í„°:\n",
      " - ìƒ˜í”Œ ìˆ˜: 29\n",
      " - UID ìˆ˜: 5\n",
      " - ê³ í˜ˆì•• ìƒ˜í”Œ ìˆ˜: 0\n",
      "\n",
      "âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°:\n",
      " - ìƒ˜í”Œ ìˆ˜: 123\n",
      " - UID ìˆ˜: 21\n",
      " - ê³ í˜ˆì•• ìƒ˜í”Œ ìˆ˜: 58\n"
     ]
    }
   ],
   "source": [
    "# í›ˆë ¨ìš© UID ëª©ë¡\n",
    "train_uids = [3, 10, 15, 17, 18]\n",
    "\n",
    "# ë°ì´í„° ë¶„í• \n",
    "df_train = df_all_flat[df_all_flat['UID'].isin(train_uids)].reset_index(drop=True)\n",
    "df_test = df_all_flat[~df_all_flat['UID'].isin(train_uids)].reset_index(drop=True)\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(\"âœ… í›ˆë ¨ ë°ì´í„°:\")\n",
    "print(f\" - ìƒ˜í”Œ ìˆ˜: {len(df_train)}\")\n",
    "print(f\" - UID ìˆ˜: {df_train['UID'].nunique()}\")\n",
    "print(f\" - ê³ í˜ˆì•• ìƒ˜í”Œ ìˆ˜: {df_train['hypertension'].sum()}\")\n",
    "\n",
    "print(\"\\nâœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°:\")\n",
    "print(f\" - ìƒ˜í”Œ ìˆ˜: {len(df_test)}\")\n",
    "print(f\" - UID ìˆ˜: {df_test['UID'].nunique()}\")\n",
    "print(f\" - ê³ í˜ˆì•• ìƒ˜í”Œ ìˆ˜: {df_test['hypertension'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b105f08b-2dd3-403e-9384-f61fc61611fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def timestep_embedding(timesteps, dim, max_period=1000, repeat_only=False):\n",
    "    \"\"\"\n",
    "    Create sinusoidal timestep embeddings.\n",
    "    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n",
    "                        These may be fractional.\n",
    "    :param dim: the dimension of the output.\n",
    "    :param max_period: controls the minimum frequency of the embeddings.\n",
    "    :return: an [N x dim] Tensor of positional embeddings.\n",
    "    \"\"\"\n",
    "    if not repeat_only:\n",
    "        half = dim // 2\n",
    "        freqs = torch.exp(\n",
    "            -math.log(max_period)\n",
    "            * torch.arange(start=0, end=half, dtype=torch.float32)\n",
    "            / half\n",
    "        ).to(device=timesteps.device)\n",
    "        args = timesteps[:, None].float() * freqs[None]\n",
    "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "        if dim % 2:\n",
    "            embedding = torch.cat(\n",
    "                [embedding, torch.zeros_like(embedding[:, :1])], dim=-1\n",
    "            )\n",
    "    else:\n",
    "        embedding = repeat(timesteps, \"b -> b d\", d=dim)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def zero_module(module):\n",
    "    \"\"\"\n",
    "    Zero out the parameters of a module and return it.\n",
    "    \"\"\"\n",
    "    for p in module.parameters():\n",
    "        p.detach().zero_()\n",
    "    return module\n",
    "\n",
    "\n",
    "class TimestepBlock(nn.Module):\n",
    "    def forward(self, x, emb):\n",
    "        \"\"\"\n",
    "        Apply the module to `x` given `emb` timestep embeddings.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n",
    "    \"\"\"\n",
    "    A sequential module that passes timestep embeddings to the children that\n",
    "    support it as an extra input.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x, emb, context=None):\n",
    "        for layer in self:\n",
    "            if isinstance(layer, TimestepBlock):\n",
    "                x = layer(x, emb)  # Pass emb to TimestepBlock layers\n",
    "            else:\n",
    "                x = layer(x)  # Regular layers do not receive emb\n",
    "        return x\n",
    "\n",
    "def Normalize(in_channels):\n",
    "    return nn.GroupNorm(\n",
    "        num_groups=32, num_channels=in_channels, eps=1e-6, affine=True\n",
    "    )\n",
    "\n",
    "\n",
    "def count_flops_attn(model, _x, y):\n",
    "    \"\"\"\n",
    "    A counter for the `thop` package to count the operations in an\n",
    "    attention operation.\n",
    "    Meant to be used like:\n",
    "        macs, params = thop.profile(\n",
    "            model,\n",
    "            inputs=(inputs, timestamps),\n",
    "            custom_ops={QKVAttention: QKVAttention.count_flops},\n",
    "        )\n",
    "    \"\"\"\n",
    "    b, c, *spatial = y[0].shape\n",
    "    num_spatial = int(np.prod(spatial))\n",
    "    # We perform two matmuls with the same number of ops.\n",
    "    # The first computes the weight matrix, the second computes\n",
    "    # the combination of the value vectors.\n",
    "    matmul_ops = 2 * b * (num_spatial**2) * c\n",
    "    model.total_ops += th.DoubleTensor([matmul_ops])\n",
    "\n",
    "\n",
    "class QKVAttentionLegacy(nn.Module):\n",
    "    \"\"\"\n",
    "    A module which performs QKV attention.\n",
    "    Matches legacy QKVAttention + input/ouput heads shaping\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        \"\"\"\n",
    "        Apply QKV attention.\n",
    "        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n",
    "        :return: an [N x (H * C) x T] tensor after attention.\n",
    "        \"\"\"\n",
    "        bs, width, length = qkv.shape\n",
    "        assert width % (3 * self.n_heads) == 0\n",
    "        ch = width // (3 * self.n_heads)\n",
    "        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(\n",
    "            ch, dim=1\n",
    "        )\n",
    "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
    "        weight = torch.einsum(\n",
    "            \"bct,bcs->bts\", q * scale, k * scale\n",
    "        )  # More stable with f16 than dividing afterwards\n",
    "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
    "        a = torch.einsum(\"bts,bcs->bct\", weight, v)\n",
    "        return a.reshape(bs, -1, length)\n",
    "\n",
    "    @staticmethod\n",
    "    def count_flops(model, _x, y):\n",
    "        return count_flops_attn(model, _x, y)\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    An attention block that allows spatial positions to attend to each other.\n",
    "    Originally ported from here, but adapted to the N-d case.\n",
    "    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        num_heads=1,\n",
    "        num_head_channels=-1,\n",
    "        use_checkpoint=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        if num_head_channels == -1:\n",
    "            self.num_heads = num_heads\n",
    "        else:\n",
    "            assert channels % num_head_channels == 0, (\n",
    "                f\"q,k,v channels {channels} is \"\n",
    "                f\"not divisible by num_head_channels {num_head_channels}\"\n",
    "            )\n",
    "            self.num_heads = channels // num_head_channels\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.norm = Normalize(channels)\n",
    "        self.qkv = nn.Conv1d(channels, channels * 3, 1)\n",
    "        self.attention = QKVAttentionLegacy(self.num_heads)\n",
    "\n",
    "        self.proj_out = zero_module(nn.Conv1d(channels, channels, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward(\n",
    "            x,\n",
    "        )\n",
    "\n",
    "    def _forward(self, x):\n",
    "        b, c, *spatial = x.shape\n",
    "        x = x.reshape(b, c, -1)\n",
    "        qkv = self.qkv(self.norm(x))\n",
    "        h = self.attention(qkv)\n",
    "        h = self.proj_out(h)\n",
    "        return (x + h).reshape(b, c, *spatial)\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    \"\"\"\n",
    "    A downsampling layer with an optional convolution.\n",
    "    :param channels: channels in the inputs and outputs.\n",
    "    :param use_conv: a bool determining if a convolution is applied.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, use_conv, out_channels=None, padding=1):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        if use_conv:\n",
    "            self.op = nn.Conv1d(\n",
    "                self.channels, self.out_channels, 3, stride=2, padding=padding\n",
    "            )#TODO:Mudar\n",
    "        else:\n",
    "            assert self.channels == self.out_channels\n",
    "            self.op = nn.AvgPool1d(kernel_size=2, stride=2)#TODO: Mudar\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        return self.op(x)\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    \"\"\"\n",
    "    An upsampling layer with an optional convolution.\n",
    "    :param channels: channels in the inputs and outputs.\n",
    "    :param use_conv: a bool determining if a convolution is applied.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, use_conv, out_channels=None, padding=1):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        if use_conv:\n",
    "            self.conv = nn.Conv1d(\n",
    "                self.channels, self.out_channels, 3, padding=padding\n",
    "            )#TODO:Mudar\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        if self.use_conv:\n",
    "            x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResBlock(TimestepBlock):  # Ensure ResBlock inherits from TimestepBlock\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        emb_channels,\n",
    "        dropout,\n",
    "        out_channels=None,\n",
    "        use_conv=False,\n",
    "        use_scale_shift_norm=False,\n",
    "        up=False,\n",
    "        down=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.emb_channels = emb_channels\n",
    "        self.dropout = dropout\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        self.use_scale_shift_norm = use_scale_shift_norm\n",
    "\n",
    "        self.in_layers = nn.Sequential(\n",
    "            Normalize(channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv1d(channels, self.out_channels, 3, padding=1),\n",
    "        )\n",
    "\n",
    "        self.updown = up or down\n",
    "\n",
    "        if up:\n",
    "            self.h_upd = Upsample(channels, False)\n",
    "            self.x_upd = Upsample(channels, False)\n",
    "        elif down:\n",
    "            self.h_upd = Downsample(channels, False)\n",
    "            self.x_upd = Downsample(channels, False)\n",
    "        else:\n",
    "            self.h_upd = self.x_upd = nn.Identity()\n",
    "\n",
    "        self.emb_layers = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(\n",
    "                emb_channels,\n",
    "                2 * self.out_channels\n",
    "                if use_scale_shift_norm\n",
    "                else self.out_channels,\n",
    "            ),\n",
    "        )\n",
    "        self.out_layers = nn.Sequential(\n",
    "            Normalize(self.out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            zero_module(\n",
    "                nn.Conv1d(self.out_channels, self.out_channels, 3, padding=1)\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if self.out_channels == channels:\n",
    "            self.skip_connection = nn.Identity()\n",
    "        elif use_conv:\n",
    "            self.skip_connection = nn.Conv1d(\n",
    "                channels, self.out_channels, kernel_size=1\n",
    "            )\n",
    "        else:\n",
    "            self.skip_connection = nn.Conv1d(channels, self.out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        \"\"\"\n",
    "        Apply the ResBlock to `x` with timestep embeddings `emb`.\n",
    "        \"\"\"\n",
    "        if self.updown:\n",
    "            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n",
    "            h = in_rest(x)\n",
    "            h = self.h_upd(h)\n",
    "            x = self.x_upd(x)\n",
    "            h = in_conv(h)\n",
    "        else:\n",
    "            h = self.in_layers(x)\n",
    "\n",
    "        emb_out = self.emb_layers(emb).type(h.dtype)\n",
    "        while len(emb_out.shape) < len(h.shape):\n",
    "            emb_out = emb_out[..., None]\n",
    "\n",
    "        if self.use_scale_shift_norm:\n",
    "            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n",
    "            scale, shift = torch.chunk(emb_out, 2, dim=1)\n",
    "            h = out_norm(h) * (1 + scale) + shift\n",
    "            h = out_rest(h)\n",
    "        else:\n",
    "            h = h + emb_out\n",
    "            h = self.out_layers(h)\n",
    "\n",
    "        return self.skip_connection(x) + h\n",
    "\n",
    "class UNetModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size=11000,\n",
    "        in_channels=1,\n",
    "        model_channels=32,\n",
    "        out_channels=1,\n",
    "        num_res_blocks=2,\n",
    "        attention_resolutions=[16, 8],\n",
    "        dropout=0.1,\n",
    "        channel_mult=(2, 4, 8),\n",
    "        num_heads=4,\n",
    "        use_scale_shift_norm=False,\n",
    "        resblock_updown=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.in_channels = in_channels\n",
    "        self.model_channels = model_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.attention_resolutions = attention_resolutions\n",
    "        self.dropout = dropout\n",
    "        self.channel_mult = channel_mult\n",
    "        self.num_heads = num_heads\n",
    "        self.use_scale_shift_norm = use_scale_shift_norm\n",
    "        self.resblock_updown = resblock_updown\n",
    "\n",
    "        time_embed_dim = model_channels * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(model_channels, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_embed_dim, time_embed_dim),\n",
    "        )\n",
    "\n",
    "        self.input_blocks = nn.ModuleList([\n",
    "            TimestepEmbedSequential(nn.Conv1d(in_channels, model_channels, 3, padding=1))\n",
    "        ])\n",
    "        input_block_chans = [model_channels]\n",
    "        ch = model_channels\n",
    "        ds = 1\n",
    "\n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            for _ in range(num_res_blocks):\n",
    "                layers = [ResBlock(ch, time_embed_dim, dropout, out_channels=mult * model_channels, use_scale_shift_norm=use_scale_shift_norm)]\n",
    "                ch = mult * model_channels\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(AttentionBlock(ch, num_heads=num_heads))\n",
    "                self.input_blocks.append(TimestepEmbedSequential(*layers))\n",
    "                input_block_chans.append(ch)\n",
    "            if level != len(channel_mult) - 1:\n",
    "                out_ch = ch\n",
    "                self.input_blocks.append(TimestepEmbedSequential(Downsample(ch, True, out_channels=out_ch)))\n",
    "                ch = out_ch\n",
    "                input_block_chans.append(ch)\n",
    "                ds *= 2\n",
    "\n",
    "        self.middle_block = TimestepEmbedSequential(\n",
    "            ResBlock(ch, time_embed_dim, dropout, use_scale_shift_norm=use_scale_shift_norm),\n",
    "            AttentionBlock(ch, num_heads=num_heads),\n",
    "            ResBlock(ch, time_embed_dim, dropout, use_scale_shift_norm=use_scale_shift_norm),\n",
    "        )\n",
    "\n",
    "        self.output_blocks = nn.ModuleList([])\n",
    "        for level, mult in list(enumerate(channel_mult))[::-1]:\n",
    "            for i in range(num_res_blocks + 1):\n",
    "                ich = input_block_chans.pop()\n",
    "                layers = [ResBlock(ch + ich, time_embed_dim, dropout, out_channels=model_channels * mult, use_scale_shift_norm=use_scale_shift_norm)]\n",
    "                ch = model_channels * mult\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(AttentionBlock(ch, num_heads=num_heads))\n",
    "                if level and i == num_res_blocks:\n",
    "                    out_ch = ch\n",
    "                    layers.append(Upsample(ch, True, out_channels=out_ch))\n",
    "                    ds //= 2\n",
    "                self.output_blocks.append(TimestepEmbedSequential(*layers))\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            Normalize(ch),\n",
    "            nn.SiLU(),\n",
    "            zero_module(nn.Conv1d(ch, out_channels, 3, padding=1)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, timesteps=None, context=None, y=None):\n",
    "        assert timesteps is not None, \"timesteps must be provided\"\n",
    "        hs = []\n",
    "        t_emb = timestep_embedding(timesteps, self.model_channels, repeat_only=False)\n",
    "        emb = self.time_embed(t_emb)\n",
    "\n",
    "        h = x\n",
    "        for module in self.input_blocks:\n",
    "            h = module(h, emb, context)\n",
    "            hs.append(h)\n",
    "        h = self.middle_block(h, emb, context)\n",
    "\n",
    "        for module in self.output_blocks:\n",
    "            h_pop = hs.pop()\n",
    "            if h.shape[2] != h_pop.shape[2]:\n",
    "                h_pop = F.interpolate(h_pop, size=h.shape[2], mode='nearest')\n",
    "            h = torch.cat([h, h_pop], dim=1)\n",
    "            h = module(h, emb, context)\n",
    "\n",
    "        return self.out(h)\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "from inspect import isfunction\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "\n",
    "def noise_like(shape, device, repeat=False):\n",
    "    repeat_noise = lambda: torch.randn((1, *shape[1:]), device=device).repeat(\n",
    "        shape[0], *((1,) * (len(shape) - 1))\n",
    "    )\n",
    "    noise = lambda: torch.randn(shape, device=device)\n",
    "    return repeat_noise() if repeat else noise()\n",
    "\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "\n",
    "def make_beta_schedule(\n",
    "    schedule, n_timestep, linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3\n",
    "):\n",
    "    if schedule == \"linear\":\n",
    "        betas = (\n",
    "            torch.linspace(\n",
    "                linear_start**0.5,\n",
    "                linear_end**0.5,\n",
    "                n_timestep,\n",
    "                dtype=torch.float64,\n",
    "            )\n",
    "            ** 2\n",
    "        )\n",
    "\n",
    "    elif schedule == \"cosine\":\n",
    "        timesteps = (\n",
    "            torch.arange(n_timestep + 1, dtype=torch.float64) / n_timestep\n",
    "            + cosine_s\n",
    "        )\n",
    "        alphas = timesteps / (1 + cosine_s) * np.pi / 2\n",
    "        alphas = torch.cos(alphas).pow(2)\n",
    "        alphas = alphas / alphas[0]\n",
    "        betas = 1 - alphas[1:] / alphas[:-1]\n",
    "        betas = np.clip(betas, a_min=0, a_max=0.999)\n",
    "\n",
    "    elif schedule == \"sqrt_linear\":\n",
    "        betas = torch.linspace(\n",
    "            linear_start, linear_end, n_timestep, dtype=torch.float64\n",
    "        )\n",
    "    elif schedule == \"sqrt\":\n",
    "        betas = (\n",
    "            torch.linspace(\n",
    "                linear_start, linear_end, n_timestep, dtype=torch.float64\n",
    "            )\n",
    "            ** 0.5\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"schedule '{schedule}' unknown.\")\n",
    "    return betas.numpy()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "class DDIM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        unet_config,\n",
    "        timesteps=1000,\n",
    "        ddim_steps=50,\n",
    "        beta_schedule=\"linear\",\n",
    "        clip_denoised=False,\n",
    "        linear_start=1e-4,\n",
    "        linear_end=2e-2,\n",
    "        original_elbo_weight=0.0,\n",
    "        parameterization=\"eps\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert parameterization in [\"eps\", \"x0\"], 'Only \"eps\" and \"x0\" are supported.'\n",
    "        self.parameterization = parameterization\n",
    "        self.model = UNetModel(**unet_config.get(\"params\", {}))\n",
    "        self.clip_denoised = clip_denoised\n",
    "        self.original_elbo_weight = original_elbo_weight\n",
    "        self.ddim_steps = ddim_steps\n",
    "        self.register_schedule(beta_schedule, timesteps, linear_start, linear_end)\n",
    "\n",
    "    def register_schedule(self, beta_schedule, timesteps, linear_start, linear_end):\n",
    "        betas = np.linspace(linear_start, linear_end, timesteps, dtype=np.float64)\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = np.cumprod(alphas, axis=0)\n",
    "        alphas_cumprod_prev = np.append(1.0, alphas_cumprod[:-1])\n",
    "\n",
    "        self.num_timesteps = int(timesteps)\n",
    "        to_torch = partial(torch.tensor, dtype=torch.float32)\n",
    "\n",
    "        self.register_buffer(\"betas\", to_torch(betas))\n",
    "        self.register_buffer(\"alphas_cumprod\", to_torch(alphas_cumprod))\n",
    "        self.register_buffer(\"alphas_cumprod_prev\", to_torch(alphas_cumprod_prev))\n",
    "        self.register_buffer(\"sqrt_recip_alphas_cumprod\", to_torch(np.sqrt(1.0 / alphas_cumprod)))\n",
    "        self.register_buffer(\"sqrt_one_minus_alphas_cumprod\", to_torch(np.sqrt(1.0 - alphas_cumprod)))\n",
    "\n",
    "    def ddim_sample(self, x, t, eta=0.0):\n",
    "        model_output = self.model(x, t)\n",
    "\n",
    "        if self.parameterization == \"eps\":\n",
    "            pred_x0 = extract(self.sqrt_recip_alphas_cumprod, t, x.shape) * x - \\\n",
    "                        extract(self.sqrt_one_minus_alphas_cumprod, t, x.shape) * model_output\n",
    "        else:\n",
    "            pred_x0 = model_output\n",
    "\n",
    "        if self.clip_denoised:\n",
    "            pred_x0 = torch.clamp(pred_x0, -1.0, 1.0)\n",
    "\n",
    "        sigma = eta * (1 - extract(self.alphas_cumprod, t, x.shape)).sqrt()\n",
    "        noise = torch.randn_like(x)\n",
    "\n",
    "        return pred_x0 + sigma * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop_ddim(self, shape, eta=0.0):\n",
    "        device = self.betas.device\n",
    "        img = torch.randn(shape, device=device)\n",
    "\n",
    "        for i in tqdm(reversed(range(0, self.ddim_steps)), desc=\"DDIM sampling\"):\n",
    "            t = torch.full((shape[0],), i, device=device, dtype=torch.long)\n",
    "            img = self.ddim_sample(img, t, eta)\n",
    "\n",
    "        return img\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size=16, eta=0.0):\n",
    "        image_size = self.model.image_size\n",
    "        channels = self.model.in_channels\n",
    "        return self.p_sample_loop_ddim((batch_size, channels, image_size), eta=eta)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        return self.model(x, t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "677edaac-8c63-4702-84c9-f57f0f15f07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['UID', 'age', 'height', 'SBP', 'DBP', 'PPG_0', 'PPG_1', 'PPG_2',\n",
       "       'PPG_3', 'PPG_4',\n",
       "       ...\n",
       "       'PPG_1991', 'PPG_1992', 'PPG_1993', 'PPG_1994', 'PPG_1995', 'PPG_1996',\n",
       "       'PPG_1997', 'PPG_1998', 'PPG_1999', 'Hypertion'],\n",
       "      dtype='object', length=2006)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c3b2454d-0d07-4cc2-a5a0-5e522ca8440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# 'subject' ì—´ ì œê±°\n",
    "train_df_no_subject = df_train.drop(columns=['UID', 'age', 'height', 'SBP', 'DBP', 'hypertension'])\n",
    "\n",
    "# í…ì„œë¡œ ë³€í™˜\n",
    "train_data_tensor = torch.tensor(train_df_no_subject.values, dtype=torch.float32)\n",
    "\n",
    "# TensorDataset ë° DataLoader ìƒì„±\n",
    "train_dataset = TensorDataset(train_data_tensor)  # ë ˆì´ë¸” ì—†ìŒ\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d4518d5a-b9db-4e07-92b9-71ddbd79ea45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tensor shape: torch.Size([29, 2000])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train tensor shape:\", train_data_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aadcdba9-3289-449b-a6ac-7ada1fd98f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PPG_0</th>\n",
       "      <th>PPG_1</th>\n",
       "      <th>PPG_2</th>\n",
       "      <th>PPG_3</th>\n",
       "      <th>PPG_4</th>\n",
       "      <th>PPG_5</th>\n",
       "      <th>PPG_6</th>\n",
       "      <th>PPG_7</th>\n",
       "      <th>PPG_8</th>\n",
       "      <th>PPG_9</th>\n",
       "      <th>...</th>\n",
       "      <th>PPG_1990</th>\n",
       "      <th>PPG_1991</th>\n",
       "      <th>PPG_1992</th>\n",
       "      <th>PPG_1993</th>\n",
       "      <th>PPG_1994</th>\n",
       "      <th>PPG_1995</th>\n",
       "      <th>PPG_1996</th>\n",
       "      <th>PPG_1997</th>\n",
       "      <th>PPG_1998</th>\n",
       "      <th>PPG_1999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2106</td>\n",
       "      <td>2106</td>\n",
       "      <td>2104</td>\n",
       "      <td>2104</td>\n",
       "      <td>2104</td>\n",
       "      <td>2106</td>\n",
       "      <td>2104</td>\n",
       "      <td>2104</td>\n",
       "      <td>2104</td>\n",
       "      <td>2104</td>\n",
       "      <td>...</td>\n",
       "      <td>1986</td>\n",
       "      <td>1990</td>\n",
       "      <td>1987</td>\n",
       "      <td>1987</td>\n",
       "      <td>1988</td>\n",
       "      <td>1987</td>\n",
       "      <td>1987</td>\n",
       "      <td>1987</td>\n",
       "      <td>1987</td>\n",
       "      <td>1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2075</td>\n",
       "      <td>2074</td>\n",
       "      <td>2075</td>\n",
       "      <td>2074</td>\n",
       "      <td>2074</td>\n",
       "      <td>2074</td>\n",
       "      <td>2070</td>\n",
       "      <td>2072</td>\n",
       "      <td>2072</td>\n",
       "      <td>2067</td>\n",
       "      <td>...</td>\n",
       "      <td>2072</td>\n",
       "      <td>2072</td>\n",
       "      <td>2067</td>\n",
       "      <td>2066</td>\n",
       "      <td>2066</td>\n",
       "      <td>2062</td>\n",
       "      <td>2063</td>\n",
       "      <td>2062</td>\n",
       "      <td>2059</td>\n",
       "      <td>2059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2038</td>\n",
       "      <td>2036</td>\n",
       "      <td>2034</td>\n",
       "      <td>2034</td>\n",
       "      <td>2034</td>\n",
       "      <td>2029</td>\n",
       "      <td>2030</td>\n",
       "      <td>2028</td>\n",
       "      <td>2024</td>\n",
       "      <td>2027</td>\n",
       "      <td>...</td>\n",
       "      <td>1946</td>\n",
       "      <td>1946</td>\n",
       "      <td>1944</td>\n",
       "      <td>1944</td>\n",
       "      <td>1944</td>\n",
       "      <td>1944</td>\n",
       "      <td>1944</td>\n",
       "      <td>1944</td>\n",
       "      <td>1939</td>\n",
       "      <td>1939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2067</td>\n",
       "      <td>2072</td>\n",
       "      <td>2067</td>\n",
       "      <td>2066</td>\n",
       "      <td>2067</td>\n",
       "      <td>2062</td>\n",
       "      <td>2062</td>\n",
       "      <td>2057</td>\n",
       "      <td>2059</td>\n",
       "      <td>2056</td>\n",
       "      <td>...</td>\n",
       "      <td>1922</td>\n",
       "      <td>1926</td>\n",
       "      <td>1923</td>\n",
       "      <td>1928</td>\n",
       "      <td>1931</td>\n",
       "      <td>1934</td>\n",
       "      <td>1935</td>\n",
       "      <td>1935</td>\n",
       "      <td>1939</td>\n",
       "      <td>1939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2099</td>\n",
       "      <td>2104</td>\n",
       "      <td>2099</td>\n",
       "      <td>2098</td>\n",
       "      <td>2099</td>\n",
       "      <td>2099</td>\n",
       "      <td>2095</td>\n",
       "      <td>2099</td>\n",
       "      <td>2096</td>\n",
       "      <td>2095</td>\n",
       "      <td>...</td>\n",
       "      <td>1984</td>\n",
       "      <td>1984</td>\n",
       "      <td>1978</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>1971</td>\n",
       "      <td>1971</td>\n",
       "      <td>1967</td>\n",
       "      <td>1967</td>\n",
       "      <td>1963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2095</td>\n",
       "      <td>2098</td>\n",
       "      <td>2098</td>\n",
       "      <td>2104</td>\n",
       "      <td>2106</td>\n",
       "      <td>2106</td>\n",
       "      <td>2107</td>\n",
       "      <td>2107</td>\n",
       "      <td>2112</td>\n",
       "      <td>2106</td>\n",
       "      <td>...</td>\n",
       "      <td>1966</td>\n",
       "      <td>1960</td>\n",
       "      <td>1960</td>\n",
       "      <td>1960</td>\n",
       "      <td>1955</td>\n",
       "      <td>1951</td>\n",
       "      <td>1947</td>\n",
       "      <td>1947</td>\n",
       "      <td>1946</td>\n",
       "      <td>1944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1984</td>\n",
       "      <td>1986</td>\n",
       "      <td>1984</td>\n",
       "      <td>1984</td>\n",
       "      <td>1979</td>\n",
       "      <td>1979</td>\n",
       "      <td>1982</td>\n",
       "      <td>1979</td>\n",
       "      <td>1984</td>\n",
       "      <td>1979</td>\n",
       "      <td>...</td>\n",
       "      <td>1976</td>\n",
       "      <td>1970</td>\n",
       "      <td>1971</td>\n",
       "      <td>1970</td>\n",
       "      <td>1971</td>\n",
       "      <td>1970</td>\n",
       "      <td>1967</td>\n",
       "      <td>1970</td>\n",
       "      <td>1966</td>\n",
       "      <td>1966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1888</td>\n",
       "      <td>1888</td>\n",
       "      <td>1886</td>\n",
       "      <td>1888</td>\n",
       "      <td>1886</td>\n",
       "      <td>1886</td>\n",
       "      <td>1886</td>\n",
       "      <td>1886</td>\n",
       "      <td>1886</td>\n",
       "      <td>1883</td>\n",
       "      <td>...</td>\n",
       "      <td>1947</td>\n",
       "      <td>1944</td>\n",
       "      <td>1944</td>\n",
       "      <td>1944</td>\n",
       "      <td>1939</td>\n",
       "      <td>1944</td>\n",
       "      <td>1938</td>\n",
       "      <td>1938</td>\n",
       "      <td>1935</td>\n",
       "      <td>1934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1891</td>\n",
       "      <td>1888</td>\n",
       "      <td>1888</td>\n",
       "      <td>1890</td>\n",
       "      <td>1888</td>\n",
       "      <td>1888</td>\n",
       "      <td>1883</td>\n",
       "      <td>1886</td>\n",
       "      <td>1883</td>\n",
       "      <td>1883</td>\n",
       "      <td>...</td>\n",
       "      <td>2010</td>\n",
       "      <td>2012</td>\n",
       "      <td>2018</td>\n",
       "      <td>2012</td>\n",
       "      <td>2014</td>\n",
       "      <td>2011</td>\n",
       "      <td>2015</td>\n",
       "      <td>2014</td>\n",
       "      <td>2015</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1842</td>\n",
       "      <td>1839</td>\n",
       "      <td>1839</td>\n",
       "      <td>1839</td>\n",
       "      <td>1838</td>\n",
       "      <td>1842</td>\n",
       "      <td>1843</td>\n",
       "      <td>1838</td>\n",
       "      <td>1842</td>\n",
       "      <td>1839</td>\n",
       "      <td>...</td>\n",
       "      <td>1984</td>\n",
       "      <td>1984</td>\n",
       "      <td>1984</td>\n",
       "      <td>1984</td>\n",
       "      <td>1986</td>\n",
       "      <td>1984</td>\n",
       "      <td>1990</td>\n",
       "      <td>1987</td>\n",
       "      <td>1990</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1851</td>\n",
       "      <td>1849</td>\n",
       "      <td>1851</td>\n",
       "      <td>1848</td>\n",
       "      <td>1850</td>\n",
       "      <td>1850</td>\n",
       "      <td>1851</td>\n",
       "      <td>1850</td>\n",
       "      <td>1851</td>\n",
       "      <td>1850</td>\n",
       "      <td>...</td>\n",
       "      <td>1976</td>\n",
       "      <td>1978</td>\n",
       "      <td>1979</td>\n",
       "      <td>1976</td>\n",
       "      <td>1979</td>\n",
       "      <td>1978</td>\n",
       "      <td>1982</td>\n",
       "      <td>1979</td>\n",
       "      <td>1979</td>\n",
       "      <td>1984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2098</td>\n",
       "      <td>2092</td>\n",
       "      <td>2092</td>\n",
       "      <td>2090</td>\n",
       "      <td>2088</td>\n",
       "      <td>2086</td>\n",
       "      <td>2083</td>\n",
       "      <td>2082</td>\n",
       "      <td>2079</td>\n",
       "      <td>2079</td>\n",
       "      <td>...</td>\n",
       "      <td>1987</td>\n",
       "      <td>1986</td>\n",
       "      <td>1984</td>\n",
       "      <td>1978</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>1967</td>\n",
       "      <td>1967</td>\n",
       "      <td>1966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2012</td>\n",
       "      <td>2010</td>\n",
       "      <td>2003</td>\n",
       "      <td>2002</td>\n",
       "      <td>1996</td>\n",
       "      <td>1994</td>\n",
       "      <td>1992</td>\n",
       "      <td>1986</td>\n",
       "      <td>1984</td>\n",
       "      <td>1984</td>\n",
       "      <td>...</td>\n",
       "      <td>1848</td>\n",
       "      <td>1848</td>\n",
       "      <td>1854</td>\n",
       "      <td>1856</td>\n",
       "      <td>1858</td>\n",
       "      <td>1860</td>\n",
       "      <td>1864</td>\n",
       "      <td>1867</td>\n",
       "      <td>1870</td>\n",
       "      <td>1874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1906</td>\n",
       "      <td>1907</td>\n",
       "      <td>1910</td>\n",
       "      <td>1912</td>\n",
       "      <td>1912</td>\n",
       "      <td>1915</td>\n",
       "      <td>1918</td>\n",
       "      <td>1918</td>\n",
       "      <td>1922</td>\n",
       "      <td>1922</td>\n",
       "      <td>...</td>\n",
       "      <td>2360</td>\n",
       "      <td>2370</td>\n",
       "      <td>2382</td>\n",
       "      <td>2392</td>\n",
       "      <td>2399</td>\n",
       "      <td>2419</td>\n",
       "      <td>2431</td>\n",
       "      <td>2442</td>\n",
       "      <td>2450</td>\n",
       "      <td>2463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1874</td>\n",
       "      <td>1871</td>\n",
       "      <td>1880</td>\n",
       "      <td>1880</td>\n",
       "      <td>1886</td>\n",
       "      <td>1891</td>\n",
       "      <td>1891</td>\n",
       "      <td>1899</td>\n",
       "      <td>1902</td>\n",
       "      <td>1906</td>\n",
       "      <td>...</td>\n",
       "      <td>1851</td>\n",
       "      <td>1850</td>\n",
       "      <td>1854</td>\n",
       "      <td>1850</td>\n",
       "      <td>1852</td>\n",
       "      <td>1850</td>\n",
       "      <td>1856</td>\n",
       "      <td>1856</td>\n",
       "      <td>1852</td>\n",
       "      <td>1858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2008</td>\n",
       "      <td>2002</td>\n",
       "      <td>2002</td>\n",
       "      <td>1998</td>\n",
       "      <td>1996</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>1987</td>\n",
       "      <td>1987</td>\n",
       "      <td>...</td>\n",
       "      <td>1736</td>\n",
       "      <td>1738</td>\n",
       "      <td>1734</td>\n",
       "      <td>1736</td>\n",
       "      <td>1739</td>\n",
       "      <td>1734</td>\n",
       "      <td>1736</td>\n",
       "      <td>1735</td>\n",
       "      <td>1738</td>\n",
       "      <td>1736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1875</td>\n",
       "      <td>1875</td>\n",
       "      <td>1882</td>\n",
       "      <td>1883</td>\n",
       "      <td>1888</td>\n",
       "      <td>1891</td>\n",
       "      <td>1896</td>\n",
       "      <td>1899</td>\n",
       "      <td>1899</td>\n",
       "      <td>1906</td>\n",
       "      <td>...</td>\n",
       "      <td>1777</td>\n",
       "      <td>1777</td>\n",
       "      <td>1780</td>\n",
       "      <td>1781</td>\n",
       "      <td>1782</td>\n",
       "      <td>1779</td>\n",
       "      <td>1785</td>\n",
       "      <td>1786</td>\n",
       "      <td>1785</td>\n",
       "      <td>1792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1950</td>\n",
       "      <td>1947</td>\n",
       "      <td>1947</td>\n",
       "      <td>1947</td>\n",
       "      <td>1946</td>\n",
       "      <td>1944</td>\n",
       "      <td>1946</td>\n",
       "      <td>1946</td>\n",
       "      <td>1946</td>\n",
       "      <td>1944</td>\n",
       "      <td>...</td>\n",
       "      <td>1979</td>\n",
       "      <td>1978</td>\n",
       "      <td>1979</td>\n",
       "      <td>1978</td>\n",
       "      <td>1979</td>\n",
       "      <td>1978</td>\n",
       "      <td>1976</td>\n",
       "      <td>1978</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2034</td>\n",
       "      <td>2034</td>\n",
       "      <td>2034</td>\n",
       "      <td>2034</td>\n",
       "      <td>2034</td>\n",
       "      <td>2034</td>\n",
       "      <td>2032</td>\n",
       "      <td>2031</td>\n",
       "      <td>2031</td>\n",
       "      <td>2034</td>\n",
       "      <td>...</td>\n",
       "      <td>2399</td>\n",
       "      <td>2395</td>\n",
       "      <td>2395</td>\n",
       "      <td>2395</td>\n",
       "      <td>2395</td>\n",
       "      <td>2392</td>\n",
       "      <td>2392</td>\n",
       "      <td>2392</td>\n",
       "      <td>2386</td>\n",
       "      <td>2386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2034</td>\n",
       "      <td>2031</td>\n",
       "      <td>2034</td>\n",
       "      <td>2028</td>\n",
       "      <td>2030</td>\n",
       "      <td>2024</td>\n",
       "      <td>2029</td>\n",
       "      <td>2027</td>\n",
       "      <td>2027</td>\n",
       "      <td>2028</td>\n",
       "      <td>...</td>\n",
       "      <td>1999</td>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "      <td>1999</td>\n",
       "      <td>1999</td>\n",
       "      <td>1997</td>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1922</td>\n",
       "      <td>1922</td>\n",
       "      <td>1922</td>\n",
       "      <td>1922</td>\n",
       "      <td>1922</td>\n",
       "      <td>1918</td>\n",
       "      <td>1920</td>\n",
       "      <td>1920</td>\n",
       "      <td>1918</td>\n",
       "      <td>1920</td>\n",
       "      <td>...</td>\n",
       "      <td>1946</td>\n",
       "      <td>1950</td>\n",
       "      <td>1950</td>\n",
       "      <td>1951</td>\n",
       "      <td>1955</td>\n",
       "      <td>1954</td>\n",
       "      <td>1958</td>\n",
       "      <td>1962</td>\n",
       "      <td>1960</td>\n",
       "      <td>1963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1907</td>\n",
       "      <td>1912</td>\n",
       "      <td>1912</td>\n",
       "      <td>1912</td>\n",
       "      <td>1912</td>\n",
       "      <td>1912</td>\n",
       "      <td>1912</td>\n",
       "      <td>1912</td>\n",
       "      <td>1914</td>\n",
       "      <td>1912</td>\n",
       "      <td>...</td>\n",
       "      <td>2130</td>\n",
       "      <td>2123</td>\n",
       "      <td>2120</td>\n",
       "      <td>2113</td>\n",
       "      <td>2112</td>\n",
       "      <td>2106</td>\n",
       "      <td>2099</td>\n",
       "      <td>2098</td>\n",
       "      <td>2091</td>\n",
       "      <td>2086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1950</td>\n",
       "      <td>1947</td>\n",
       "      <td>1946</td>\n",
       "      <td>1944</td>\n",
       "      <td>1946</td>\n",
       "      <td>1944</td>\n",
       "      <td>1944</td>\n",
       "      <td>1944</td>\n",
       "      <td>1944</td>\n",
       "      <td>1944</td>\n",
       "      <td>...</td>\n",
       "      <td>1986</td>\n",
       "      <td>1987</td>\n",
       "      <td>1984</td>\n",
       "      <td>1984</td>\n",
       "      <td>1984</td>\n",
       "      <td>1979</td>\n",
       "      <td>1984</td>\n",
       "      <td>1984</td>\n",
       "      <td>1984</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1995</td>\n",
       "      <td>1995</td>\n",
       "      <td>1994</td>\n",
       "      <td>1992</td>\n",
       "      <td>1995</td>\n",
       "      <td>1994</td>\n",
       "      <td>1992</td>\n",
       "      <td>1994</td>\n",
       "      <td>1993</td>\n",
       "      <td>1995</td>\n",
       "      <td>...</td>\n",
       "      <td>2072</td>\n",
       "      <td>2067</td>\n",
       "      <td>2064</td>\n",
       "      <td>2066</td>\n",
       "      <td>2062</td>\n",
       "      <td>2062</td>\n",
       "      <td>2059</td>\n",
       "      <td>2057</td>\n",
       "      <td>2057</td>\n",
       "      <td>2056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>1978</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>...</td>\n",
       "      <td>2155</td>\n",
       "      <td>2152</td>\n",
       "      <td>2149</td>\n",
       "      <td>2143</td>\n",
       "      <td>2143</td>\n",
       "      <td>2142</td>\n",
       "      <td>2138</td>\n",
       "      <td>2139</td>\n",
       "      <td>2138</td>\n",
       "      <td>2131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2014</td>\n",
       "      <td>2008</td>\n",
       "      <td>2008</td>\n",
       "      <td>2010</td>\n",
       "      <td>2010</td>\n",
       "      <td>2012</td>\n",
       "      <td>2012</td>\n",
       "      <td>2008</td>\n",
       "      <td>2010</td>\n",
       "      <td>2011</td>\n",
       "      <td>...</td>\n",
       "      <td>2219</td>\n",
       "      <td>2223</td>\n",
       "      <td>2219</td>\n",
       "      <td>2226</td>\n",
       "      <td>2226</td>\n",
       "      <td>2227</td>\n",
       "      <td>2232</td>\n",
       "      <td>2234</td>\n",
       "      <td>2234</td>\n",
       "      <td>2232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1987</td>\n",
       "      <td>1990</td>\n",
       "      <td>1987</td>\n",
       "      <td>1987</td>\n",
       "      <td>1990</td>\n",
       "      <td>1986</td>\n",
       "      <td>1984</td>\n",
       "      <td>1984</td>\n",
       "      <td>1047</td>\n",
       "      <td>1985</td>\n",
       "      <td>...</td>\n",
       "      <td>2060</td>\n",
       "      <td>2060</td>\n",
       "      <td>2060</td>\n",
       "      <td>2063</td>\n",
       "      <td>2063</td>\n",
       "      <td>2059</td>\n",
       "      <td>2062</td>\n",
       "      <td>2059</td>\n",
       "      <td>2059</td>\n",
       "      <td>2057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2067</td>\n",
       "      <td>2060</td>\n",
       "      <td>2066</td>\n",
       "      <td>2063</td>\n",
       "      <td>2063</td>\n",
       "      <td>2063</td>\n",
       "      <td>2059</td>\n",
       "      <td>2062</td>\n",
       "      <td>2058</td>\n",
       "      <td>2059</td>\n",
       "      <td>...</td>\n",
       "      <td>2035</td>\n",
       "      <td>2040</td>\n",
       "      <td>2046</td>\n",
       "      <td>2048</td>\n",
       "      <td>2054</td>\n",
       "      <td>2059</td>\n",
       "      <td>2060</td>\n",
       "      <td>2066</td>\n",
       "      <td>2067</td>\n",
       "      <td>2074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2028</td>\n",
       "      <td>2027</td>\n",
       "      <td>2031</td>\n",
       "      <td>2030</td>\n",
       "      <td>2027</td>\n",
       "      <td>2027</td>\n",
       "      <td>2027</td>\n",
       "      <td>2027</td>\n",
       "      <td>2028</td>\n",
       "      <td>2025</td>\n",
       "      <td>...</td>\n",
       "      <td>2176</td>\n",
       "      <td>2179</td>\n",
       "      <td>2179</td>\n",
       "      <td>2179</td>\n",
       "      <td>2185</td>\n",
       "      <td>2184</td>\n",
       "      <td>2184</td>\n",
       "      <td>2186</td>\n",
       "      <td>2186</td>\n",
       "      <td>2190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29 rows Ã— 2000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    PPG_0  PPG_1  PPG_2  PPG_3  PPG_4  PPG_5  PPG_6  PPG_7  PPG_8  PPG_9  ...  \\\n",
       "0    2106   2106   2104   2104   2104   2106   2104   2104   2104   2104  ...   \n",
       "1    2075   2074   2075   2074   2074   2074   2070   2072   2072   2067  ...   \n",
       "2    2038   2036   2034   2034   2034   2029   2030   2028   2024   2027  ...   \n",
       "3    2067   2072   2067   2066   2067   2062   2062   2057   2059   2056  ...   \n",
       "4    2099   2104   2099   2098   2099   2099   2095   2099   2096   2095  ...   \n",
       "5    2095   2098   2098   2104   2106   2106   2107   2107   2112   2106  ...   \n",
       "6    1984   1986   1984   1984   1979   1979   1982   1979   1984   1979  ...   \n",
       "7    1888   1888   1886   1888   1886   1886   1886   1886   1886   1883  ...   \n",
       "8    1891   1888   1888   1890   1888   1888   1883   1886   1883   1883  ...   \n",
       "9    1842   1839   1839   1839   1838   1842   1843   1838   1842   1839  ...   \n",
       "10   1851   1849   1851   1848   1850   1850   1851   1850   1851   1850  ...   \n",
       "11   2098   2092   2092   2090   2088   2086   2083   2082   2079   2079  ...   \n",
       "12   2012   2010   2003   2002   1996   1994   1992   1986   1984   1984  ...   \n",
       "13   1906   1907   1910   1912   1912   1915   1918   1918   1922   1922  ...   \n",
       "14   1874   1871   1880   1880   1886   1891   1891   1899   1902   1906  ...   \n",
       "15   2008   2002   2002   1998   1996   1992   1992   1992   1987   1987  ...   \n",
       "16   1875   1875   1882   1883   1888   1891   1896   1899   1899   1906  ...   \n",
       "17   1950   1947   1947   1947   1946   1944   1946   1946   1946   1944  ...   \n",
       "18   2034   2034   2034   2034   2034   2034   2032   2031   2031   2034  ...   \n",
       "19   2034   2031   2034   2028   2030   2024   2029   2027   2027   2028  ...   \n",
       "20   1922   1922   1922   1922   1922   1918   1920   1920   1918   1920  ...   \n",
       "21   1907   1912   1912   1912   1912   1912   1912   1912   1914   1912  ...   \n",
       "22   1950   1947   1946   1944   1946   1944   1944   1944   1944   1944  ...   \n",
       "23   1995   1995   1994   1992   1995   1994   1992   1994   1993   1995  ...   \n",
       "24   1976   1976   1978   1976   1976   1976   1976   1976   1976   1976  ...   \n",
       "25   2014   2008   2008   2010   2010   2012   2012   2008   2010   2011  ...   \n",
       "26   1987   1990   1987   1987   1990   1986   1984   1984   1047   1985  ...   \n",
       "27   2067   2060   2066   2063   2063   2063   2059   2062   2058   2059  ...   \n",
       "28   2028   2027   2031   2030   2027   2027   2027   2027   2028   2025  ...   \n",
       "\n",
       "    PPG_1990  PPG_1991  PPG_1992  PPG_1993  PPG_1994  PPG_1995  PPG_1996  \\\n",
       "0       1986      1990      1987      1987      1988      1987      1987   \n",
       "1       2072      2072      2067      2066      2066      2062      2063   \n",
       "2       1946      1946      1944      1944      1944      1944      1944   \n",
       "3       1922      1926      1923      1928      1931      1934      1935   \n",
       "4       1984      1984      1978      1976      1976      1971      1971   \n",
       "5       1966      1960      1960      1960      1955      1951      1947   \n",
       "6       1976      1970      1971      1970      1971      1970      1967   \n",
       "7       1947      1944      1944      1944      1939      1944      1938   \n",
       "8       2010      2012      2018      2012      2014      2011      2015   \n",
       "9       1984      1984      1984      1984      1986      1984      1990   \n",
       "10      1976      1978      1979      1976      1979      1978      1982   \n",
       "11      1987      1986      1984      1978      1976      1976      1976   \n",
       "12      1848      1848      1854      1856      1858      1860      1864   \n",
       "13      2360      2370      2382      2392      2399      2419      2431   \n",
       "14      1851      1850      1854      1850      1852      1850      1856   \n",
       "15      1736      1738      1734      1736      1739      1734      1736   \n",
       "16      1777      1777      1780      1781      1782      1779      1785   \n",
       "17      1979      1978      1979      1978      1979      1978      1976   \n",
       "18      2399      2395      2395      2395      2395      2392      2392   \n",
       "19      1999      1998      1998      1999      1999      1997      1998   \n",
       "20      1946      1950      1950      1951      1955      1954      1958   \n",
       "21      2130      2123      2120      2113      2112      2106      2099   \n",
       "22      1986      1987      1984      1984      1984      1979      1984   \n",
       "23      2072      2067      2064      2066      2062      2062      2059   \n",
       "24      2155      2152      2149      2143      2143      2142      2138   \n",
       "25      2219      2223      2219      2226      2226      2227      2232   \n",
       "26      2060      2060      2060      2063      2063      2059      2062   \n",
       "27      2035      2040      2046      2048      2054      2059      2060   \n",
       "28      2176      2179      2179      2179      2185      2184      2184   \n",
       "\n",
       "    PPG_1997  PPG_1998  PPG_1999  \n",
       "0       1987      1987      1992  \n",
       "1       2062      2059      2059  \n",
       "2       1944      1939      1939  \n",
       "3       1935      1939      1939  \n",
       "4       1967      1967      1963  \n",
       "5       1947      1946      1944  \n",
       "6       1970      1966      1966  \n",
       "7       1938      1935      1934  \n",
       "8       2014      2015      2017  \n",
       "9       1987      1990      1988  \n",
       "10      1979      1979      1984  \n",
       "11      1967      1967      1966  \n",
       "12      1867      1870      1874  \n",
       "13      2442      2450      2463  \n",
       "14      1856      1852      1858  \n",
       "15      1735      1738      1736  \n",
       "16      1786      1785      1792  \n",
       "17      1978      1976      1976  \n",
       "18      2392      2386      2386  \n",
       "19      1998      1998      1996  \n",
       "20      1962      1960      1963  \n",
       "21      2098      2091      2086  \n",
       "22      1984      1984      1979  \n",
       "23      2057      2057      2056  \n",
       "24      2139      2138      2131  \n",
       "25      2234      2234      2232  \n",
       "26      2059      2059      2057  \n",
       "27      2066      2067      2074  \n",
       "28      2186      2186      2190  \n",
       "\n",
       "[29 rows x 2000 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_no_subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "03202c6c-557a-4134-b444-6b6da3a04657",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Avg Train Loss = 4401709.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Avg Train Loss = 4401672.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Avg Train Loss = 4401633.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Avg Train Loss = 4401594.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Avg Train Loss = 4401554.5000\n",
      "\n",
      "â±ï¸ í•™ìŠµ ì™„ë£Œ! ì´ ì†Œìš” ì‹œê°„: 0ë¶„ 21.96ì´ˆ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import optim # Import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# í•™ìŠµ í•¨ìˆ˜\n",
    "def train_ddim_model(model, train_loader, num_epochs=20, learning_rate=1e-4, device='cuda'):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            x = batch[0].to(device)  # ë ˆì´ë¸”ì´ ì—†ìœ¼ë¯€ë¡œ xë§Œ ê°€ì ¸ì˜´\n",
    "            # x = x.unsqueeze(1)  # Remove this line - it's causing the error\n",
    "\n",
    "            # Reshape x to have the expected shape (batch_size, in_channels, image_size)\n",
    "            x = x.view(x.shape[0], unet_config[\"params\"][\"in_channels\"], -1)\n",
    "            t = torch.randint(0, model.num_timesteps, (x.shape[0],), device=device).long()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(model(x, t), x)  # ëª¨ë¸ì˜ ì¶œë ¥ê³¼ ìž…ë ¥ ë¹„êµ (ìž¬êµ¬ì„± ì†ì‹¤)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}: Avg Train Loss = {avg_loss:.4f}\")\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "unet_config = {\n",
    "    \"params\": {\n",
    "        \"image_size\": 11000,          # ì‹œí€€ìŠ¤ ê¸¸ì´ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "        \"in_channels\": 1,          # 1ì±„ë„ ìž…ë ¥ (ê¸°ë³¸ì ì¸ ê²½ìš°)\n",
    "        \"model_channels\": 32,\n",
    "        \"out_channels\": 1,\n",
    "        \"num_res_blocks\": 2,\n",
    "        \"attention_resolutions\": [13],  # 52ë¥¼ downsampleí–ˆì„ ë•Œ í¬í•¨ë˜ëŠ” í•´ìƒë„\n",
    "        \"dropout\": 0.1,\n",
    "        \"channel_mult\": (2, 4, 8),\n",
    "        \"num_heads\": 4,\n",
    "        \"use_scale_shift_norm\": False,\n",
    "        \"resblock_updown\": True,\n",
    "    }\n",
    "}\n",
    "import time\n",
    "\n",
    "# í•™ìŠµ ì‹œê°„ ì¸¡ì • ì‹œìž‘\n",
    "start_time = time.time()\n",
    "\n",
    "ddim_model = DDIM(unet_config=unet_config, timesteps=2000, ddim_steps=50, parameterization='eps').to(device)\n",
    "\n",
    "# í•™ìŠµ ì‹œìž‘\n",
    "train_ddim_model(ddim_model, train_loader, num_epochs=5, learning_rate=1e-4, device=device)\n",
    "\n",
    "# í•™ìŠµ ì‹œê°„ ì¸¡ì • ì¢…ë£Œ\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# ì‹œê°„ ì¶œë ¥\n",
    "minutes, seconds = divmod(elapsed_time, 60)\n",
    "print(f\"\\nâ±ï¸ í•™ìŠµ ì™„ë£Œ! ì´ ì†Œìš” ì‹œê°„: {int(minutes)}ë¶„ {seconds:.2f}ì´ˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "41872803-7f50-41a4-b3ff-35395cd9ced6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123, 2006)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a9fda5cd-c152-4361-b698-c5d6e7c386a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['UID', 'age', 'height', 'SBP', 'DBP', 'PPG_0', 'PPG_1', 'PPG_2',\n",
       "       'PPG_3', 'PPG_4',\n",
       "       ...\n",
       "       'PPG_1991', 'PPG_1992', 'PPG_1993', 'PPG_1994', 'PPG_1995', 'PPG_1996',\n",
       "       'PPG_1997', 'PPG_1998', 'PPG_1999', 'Hypertion'],\n",
       "      dtype='object', length=2006)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d1bbdbcb-acd5-45d0-a3b2-13ea246a536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìž…ë ¥ í…ì„œ\n",
    "test_features = df_test.drop(columns=['UID', 'age', 'height','SBP', 'DBP','Hypertion']).values\n",
    "test_data_tensor = torch.tensor(test_features, dtype=torch.float32).unsqueeze(1)  # (44, 1, 11000)\n",
    "\n",
    "# ë ˆì´ë¸” í…ì„œ\n",
    "test_labels_tensor = torch.tensor(df_test[\"Hypertion\"].values, dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "# subject í…ì„œ\n",
    "subject_tensor = torch.tensor(df_test[\"UID\"].values, dtype=torch.int32).unsqueeze(-1)\n",
    "\n",
    "# TensorDataset ìƒì„±\n",
    "test_dataset = TensorDataset(test_data_tensor, test_labels_tensor, subject_tensor)\n",
    "\n",
    "# DataLoader ìƒì„±\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a04bc61f-b5b2-4b79-8758-52a16da0f1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([123, 1, 2000])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cec42bc3-af1c-4def-9307-b99fea8756d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:06<00:00,  1.54s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test_ddim_model(model, test_loader, device='cuda'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing Model\"):\n",
    "            if isinstance(batch, (tuple, list)):\n",
    "                if len(batch) == 2:\n",
    "                    x, y = batch\n",
    "                else:\n",
    "                    x, y = batch[0], batch[1]\n",
    "            else:\n",
    "                raise ValueError(\"Batch must be a tuple or list containing at least input and label.\")\n",
    "\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            # in_channels ì„¤ì •\n",
    "            in_channels = getattr(model, \"unet_config\", {}).get(\"params\", {}).get(\"in_channels\", 1)\n",
    "            x = x.view(x.shape[0], in_channels, -1)\n",
    "\n",
    "            t = torch.randint(0, model.num_timesteps, (x.shape[0],), device=device).long()\n",
    "            pred = model(x, t)\n",
    "\n",
    "            # ìƒ˜í”Œë³„ reconstruction loss ê³„ì‚°\n",
    "            sample_losses = ((x - pred) ** 2).mean(dim=[1, 2])  # [batch_size]\n",
    "\n",
    "            for i in range(x.shape[0]):\n",
    "                results.append({\n",
    "                    'Reconstruction_Loss': sample_losses[i].item(),\n",
    "                    'Label': y[i].item()\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "df_results = test_ddim_model(ddim_model, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5def09-fa8c-4a24-a929-0a38b0f60741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3084547-26bd-465f-90cb-a3b293e35a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcf85d6-c9e3-40ac-8639-82eb2f319b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def partial_reverse_ddim(model, x_noisy, start_step, end_step=0, eta=0.0):\n",
    "    assert start_step > end_step, \"start_step must be greater than end_step\"\n",
    "    img = x_noisy.to(model.betas.device)\n",
    "\n",
    "    for i in range(start_step, end_step, -1):\n",
    "        t = torch.full((img.shape[0],), i, device=img.device, dtype=torch.long)\n",
    "        img = model.ddim_sample(img, t, eta)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def test_partial_diffusion(ddpm, test_loader, device, start_step=900, end_step=800, eta=0.0):\n",
    "    ddpm.eval()\n",
    "    total_loss = 0\n",
    "    num_samples = 0\n",
    "    all_reconstruction_errors = []\n",
    "    true_labels = []\n",
    "    subject_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x, labels, subjects = batch\n",
    "            x = x.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            noise = torch.randn_like(x)  # ë…¸ì´ì¦ˆ ìƒì„±\n",
    "            reconstructed_x = partial_reverse_ddim(ddpm, noise, start_step=start_step, end_step=end_step, eta=eta)\n",
    "\n",
    "            # shape mismatch ë°©ì§€\n",
    "            min_len = min(x.shape[2], reconstructed_x.shape[2])\n",
    "            x = x[:, :, :min_len]\n",
    "            reconstructed_x = reconstructed_x[:, :, :min_len]\n",
    "\n",
    "            # MAE ê¸°ë°˜ reconstruction error ê³„ì‚°\n",
    "            reconstruction_error = F.l1_loss(reconstructed_x, x, reduction='none').mean(dim=[1, 2])\n",
    "            loss = reconstruction_error.mean()\n",
    "\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            num_samples += x.size(0)\n",
    "\n",
    "            all_reconstruction_errors.extend(reconstruction_error.cpu().numpy().tolist())\n",
    "            true_labels.extend(labels.cpu().numpy().flatten().tolist())\n",
    "            subject_ids.extend(subjects.cpu().numpy().flatten().tolist())\n",
    "\n",
    "    avg_loss = total_loss / num_samples\n",
    "    anomaly_df = pd.DataFrame({\n",
    "        \"Subject\": subject_ids,\n",
    "        \"True_Label\": true_labels,\n",
    "        \"Reconstruction_Error\": all_reconstruction_errors\n",
    "    })\n",
    "\n",
    "    print(f\"âœ… ìµœì¢… í…ŒìŠ¤íŠ¸ ì†ì‹¤ (Partial Diffusion MAE ê¸°ë°˜): {avg_loss:.4f}\")\n",
    "    return anomaly_df, avg_loss, None\n",
    "\n",
    "\n",
    "# ì‹¤í–‰ ì½”ë“œ ë¸”ë¡\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "    # í…ŒìŠ¤íŠ¸ ì‹œê°„ ì¸¡ì • ì‹œìž‘\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì‹œìž‘/ì¢…ë£Œ stepì€ í•„ìš” ì‹œ ì¡°ì •)\n",
    "    anomaly_df, test_loss, _ = test_partial_diffusion(ddim_model, test_loader, device, start_step=100, end_step=50)\n",
    "\n",
    "    # ê²°ê³¼ ì¶œë ¥\n",
    "    print(\"\\nðŸ“‹ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìƒìœ„ 5ê°œ:\")\n",
    "    print(anomaly_df.head())\n",
    "\n",
    "    print(f\"\\nâœ… Test Loss (MAE): {test_loss:.4f}\")\n",
    "\n",
    "\n",
    "    # í…ŒìŠ¤íŠ¸ ì‹œê°„ ì¸¡ì • ì¢…ë£Œ\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    # ì‹œê°„ ì¶œë ¥\n",
    "    minutes, seconds = divmod(elapsed_time, 60)\n",
    "    print(f\"\\nâ±ï¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ì´ ì†Œìš” ì‹œê°„: {int(minutes)}ë¶„ {seconds:.2f}ì´ˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6f2519-129f-4fe9-a2d9-9abdf6cd31a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f4496b-1bb0-4674-b325-044d134dfbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_df['True_Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5014cad8-626b-4155-bb57-2c2df7289590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë ˆì´ë¸”ë³„ ìµœì†Œê°’ê³¼ ìµœëŒ€ê°’ ê³„ì‚°\n",
    "group_stats = anomaly_df.groupby(\"True_Label\")[\"Reconstruction_Error\"].agg([\"min\", \"max\"]).reset_index()\n",
    "\n",
    "# ì¶œë ¥\n",
    "print(\"Reconstruction_Error - Min/Max by True Label\")\n",
    "for _, row in group_stats.iterrows():\n",
    "    label = row[\"True_Label\"]\n",
    "    min_val = row[\"min\"]\n",
    "    max_val = row[\"max\"]\n",
    "    print(f\"Label {label}: min = {min_val:.4f}, max = {max_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db5dc76-1da9-4ab5-8bd1-868de67f2b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ìŠ¤íƒ€ì¼ ì„¤ì •\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# ë°”ì´ì˜¬ë¦° í”Œë¡¯ ê·¸ë¦¬ê¸°\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.violinplot(data=anomaly_df, x=\"True_Label\", y=\"Reconstruction_Error\", inner=\"box\")\n",
    "\n",
    "# í•„ìš” ì‹œ ìž„ê³„ì„  ì¶”ê°€\n",
    "# plt.axhline(y=th, color='red', linestyle='--', label=f'Threshold = {th:.2f}')\n",
    "\n",
    "# ë¼ë²¨ ë° íƒ€ì´í‹€ ì„¤ì •\n",
    "plt.title(\"Reconstruction Error by True Label\")\n",
    "plt.xlabel(\"True Label (0: Normal, 1: Prehypertension, 2: Hypertension)\")\n",
    "plt.ylabel(\"Reconstruction Error\")\n",
    "\n",
    "# í•„ìš” ì‹œ ë¡œê·¸ ìŠ¤ì¼€ì¼\n",
    "# plt.yscale(\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe303e4d-eead-4926-bbeb-9d5638f47047",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.0906"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b6d1c5-b9f6-4949-a463-5e2b49221fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ìŠ¤íƒ€ì¼ ì„¤ì •\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# ë°”ì´ì˜¬ë¦° í”Œë¡¯ ê·¸ë¦¬ê¸°\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.violinplot(data=anomaly_df, x=\"True_Label\", y=\"Reconstruction_Error\", inner=\"box\")\n",
    "\n",
    "# í•„ìš” ì‹œ ìž„ê³„ì„  ì¶”ê°€\n",
    "# plt.axhline(y=th, color='red', linestyle='--', label=f'Threshold = {th:.2f}')\n",
    "\n",
    "# ë¼ë²¨ ë° íƒ€ì´í‹€ ì„¤ì •\n",
    "plt.title(\"Reconstruction Error by True Label\")\n",
    "plt.xlabel(\"True Label (0: Normal, 1: Prehypertension, 2: Hypertension)\")\n",
    "plt.ylabel(\"Reconstruction Error\")\n",
    "\n",
    "# í•„ìš” ì‹œ ë¡œê·¸ ìŠ¤ì¼€ì¼\n",
    "# plt.yscale(\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24239790-efba-41ef-b147-efaf1e5726d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ì´ìƒ íƒì§€ ì„±ê³µ (True Positive): Reconstruction_Error < threshold & Label in [1.0, 2.0]\n",
    "cond_anomaly_detected = (anomaly_df['Reconstruction_Error'] < threshold) & (anomaly_df['True_Label'].isin([1.0, 2.0]))\n",
    "anomaly_detected_count = cond_anomaly_detected.sum()\n",
    "\n",
    "# ì´ìƒ íƒì§€ ì‹¤íŒ¨ (False Negative): Reconstruction_Error >= threshold & Label in [1.0, 2.0]\n",
    "cond_anomaly_missed = (anomaly_df['Reconstruction_Error'] >= threshold) & (anomaly_df['True_Label'].isin([1.0, 2.0]))\n",
    "anomaly_missed_count = cond_anomaly_missed.sum()\n",
    "\n",
    "# ì •ìƒ íƒì§€ ì„±ê³µ (True Negative): Reconstruction_Error >= threshold & Label == 0.0\n",
    "cond_normal_detected = (anomaly_df['Reconstruction_Error'] >= threshold) & (anomaly_df['True_Label'] == 0.0)\n",
    "normal_detected_count = cond_normal_detected.sum()\n",
    "\n",
    "# ì •ìƒ íƒì§€ ì‹¤íŒ¨ (False Positive): Reconstruction_Error < threshold & Label == 0.0\n",
    "cond_normal_missed = (anomaly_df['Reconstruction_Error'] < threshold) & (anomaly_df['True_Label'] == 0.0)\n",
    "normal_missed_count = cond_normal_missed.sum()\n",
    "\n",
    "# ì¶œë ¥\n",
    "print(f\"ì´ìƒ íƒì§€ ì„±ê³µ (TP): {anomaly_detected_count}ê±´\")\n",
    "print(f\"ì´ìƒ íƒì§€ ì‹¤íŒ¨ (FN): {anomaly_missed_count}ê±´\")\n",
    "print(f\"ì •ìƒ íƒì§€ ì„±ê³µ (TN): {normal_detected_count}ê±´\")\n",
    "print(f\"ì •ìƒ íƒì§€ ì‹¤íŒ¨ (FP): {normal_missed_count}ê±´\")\n",
    "\n",
    "\n",
    "# ì´ì§„ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì˜ˆì¸¡ ë¼ë²¨ ìƒì„± (ì´ìƒ: 1, ì •ìƒ: 0)\n",
    "# Reconstruction_Errorê°€ thresholdë³´ë‹¤ ìž‘ìœ¼ë©´ ì´ìƒìœ¼ë¡œ ì˜ˆì¸¡ (ì´ìƒì€ ìž¬êµ¬ì„± ì˜¤ë¥˜ê°€ ë‚®ë‹¤ ê°€ì •)\n",
    "pred_labels = (anomaly_df['Reconstruction_Error'] < threshold).astype(int)\n",
    "true_labels = anomaly_df['True_Label'].apply(lambda x: 1 if x in [1.0, 2.0] else 0)\n",
    "\n",
    "# ì •ë°€ë„, ìž¬í˜„ìœ¨, F1 ì ìˆ˜\n",
    "precision = precision_score(true_labels, pred_labels)\n",
    "recall = recall_score(true_labels, pred_labels)\n",
    "f1 = f1_score(true_labels, pred_labels)\n",
    "roc_auc = roc_auc_score(true_labels, -anomaly_df['Reconstruction_Error'])  # ë‚®ì„ìˆ˜ë¡ ì´ìƒìœ¼ë¡œ ê°„ì£¼\n",
    "\n",
    "print(f\"\\nðŸ“Š Precision: {precision:.4f}\")\n",
    "print(f\"ðŸ“Š Recall:    {recall:.4f}\")\n",
    "print(f\"ðŸ“Š F1-score:  {f1:.4f}\")\n",
    "print(f\"ðŸ“ˆ AUC:       {roc_auc:.4f}\")\n",
    "\n",
    "\n",
    "# ROC Curve ì‹œê°í™”\n",
    "fpr, tpr, thresholds = roc_curve(true_labels, -anomaly_df['Reconstruction_Error'])  # ì´ìƒì¼ìˆ˜ë¡ ê°’ ìž‘ìŒ\n",
    "roc_auc_val = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc_val:.4f})\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Confusion Matrix ê³„ì‚°\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "# ì‹œê°í™” ê°ì²´ ìƒì„± ë° ì¶œë ¥\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Normal\", \"Anomaly\"])\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, colorbar=False)  # colorbar ìƒëžµ ê°€ëŠ¥\n",
    "\n",
    "# ì œëª© ë° ê·¸ë¦¬ë“œ ì„¤ì •\n",
    "ax.set_title(\"Confusion Matrix\")\n",
    "ax.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568bc248-3fa8-4e35-84bb-4991d0c0394a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e42df52-b739-4450-86ce-1cf79266b8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6125949-f030-4de8-b39c-270704ea0c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8b072c-1974-4311-933d-437716af4760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61c121f-a6ad-4903-a75e-e8d48c413704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baea1d6-f6f5-4329-968a-57bb5055b9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11e9a9b-2c84-4621-82f7-3c4704bca410",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
