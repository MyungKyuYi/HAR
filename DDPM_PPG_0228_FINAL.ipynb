{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "62f4bb50-9c28-4ff9-b980-24b6a43bc91b",
      "metadata": {
        "id": "62f4bb50-9c28-4ff9-b980-24b6a43bc91b"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from abc import abstractmethod\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import repeat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "360b9f1e-b576-4d64-99be-9124f4ed5056",
      "metadata": {
        "id": "360b9f1e-b576-4d64-99be-9124f4ed5056"
      },
      "outputs": [],
      "source": [
        "def timestep_embedding(timesteps, dim, max_period=10000, repeat_only=False):\n",
        "    \"\"\"\n",
        "    Create sinusoidal timestep embeddings.\n",
        "    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n",
        "                      These may be fractional.\n",
        "    :param dim: the dimension of the output.\n",
        "    :param max_period: controls the minimum frequency of the embeddings.\n",
        "    :return: an [N x dim] Tensor of positional embeddings.\n",
        "    \"\"\"\n",
        "    if not repeat_only:\n",
        "        half = dim // 2\n",
        "        freqs = torch.exp(\n",
        "            -math.log(max_period)\n",
        "            * torch.arange(start=0, end=half, dtype=torch.float32)\n",
        "            / half\n",
        "        ).to(device=timesteps.device)\n",
        "        args = timesteps[:, None].float() * freqs[None]\n",
        "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
        "        if dim % 2:\n",
        "            embedding = torch.cat(\n",
        "                [embedding, torch.zeros_like(embedding[:, :1])], dim=-1\n",
        "            )\n",
        "    else:\n",
        "        embedding = repeat(timesteps, \"b -> b d\", d=dim)\n",
        "    return embedding\n",
        "\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"\n",
        "    Zero out the parameters of a module and return it.\n",
        "    \"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "\n",
        "class TimestepBlock(nn.Module):\n",
        "    @abstractmethod\n",
        "    def forward(self, x, emb):\n",
        "        \"\"\"\n",
        "        Apply the module to `x` given `emb` timestep embeddings.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n",
        "    \"\"\"\n",
        "    A sequential module that passes timestep embeddings to the children that\n",
        "    support it as an extra input.\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, x, emb, context=None):\n",
        "        for layer in self:\n",
        "            if isinstance(layer, TimestepBlock):\n",
        "                x = layer(x, emb)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def Normalize(in_channels):\n",
        "    return nn.GroupNorm(\n",
        "        num_groups=32, num_channels=in_channels, eps=1e-6, affine=True\n",
        "    )\n",
        "\n",
        "\n",
        "def count_flops_attn(model, _x, y):\n",
        "    \"\"\"\n",
        "    A counter for the `thop` package to count the operations in an\n",
        "    attention operation.\n",
        "    Meant to be used like:\n",
        "        macs, params = thop.profile(\n",
        "            model,\n",
        "            inputs=(inputs, timestamps),\n",
        "            custom_ops={QKVAttention: QKVAttention.count_flops},\n",
        "        )\n",
        "    \"\"\"\n",
        "    b, c, *spatial = y[0].shape\n",
        "    num_spatial = int(np.prod(spatial))\n",
        "    # We perform two matmuls with the same number of ops.\n",
        "    # The first computes the weight matrix, the second computes\n",
        "    # the combination of the value vectors.\n",
        "    matmul_ops = 2 * b * (num_spatial**2) * c\n",
        "    model.total_ops += th.DoubleTensor([matmul_ops])\n",
        "\n",
        "\n",
        "class QKVAttentionLegacy(nn.Module):\n",
        "    \"\"\"\n",
        "    A module which performs QKV attention.\n",
        "    Matches legacy QKVAttention + input/ouput heads shaping\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_heads):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, qkv):\n",
        "        \"\"\"\n",
        "        Apply QKV attention.\n",
        "        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n",
        "        :return: an [N x (H * C) x T] tensor after attention.\n",
        "        \"\"\"\n",
        "        bs, width, length = qkv.shape\n",
        "        assert width % (3 * self.n_heads) == 0\n",
        "        ch = width // (3 * self.n_heads)\n",
        "        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(\n",
        "            ch, dim=1\n",
        "        )\n",
        "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
        "        weight = th.einsum(\n",
        "            \"bct,bcs->bts\", q * scale, k * scale\n",
        "        )  # More stable with f16 than dividing afterwards\n",
        "        weight = th.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
        "        a = th.einsum(\"bts,bcs->bct\", weight, v)\n",
        "        return a.reshape(bs, -1, length)\n",
        "\n",
        "    @staticmethod\n",
        "    def count_flops(model, _x, y):\n",
        "        return count_flops_attn(model, _x, y)\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    An attention block that allows spatial positions to attend to each other.\n",
        "    Originally ported from here, but adapted to the N-d case.\n",
        "    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels,\n",
        "        num_heads=1,\n",
        "        num_head_channels=-1,\n",
        "        use_checkpoint=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        if num_head_channels == -1:\n",
        "            self.num_heads = num_heads\n",
        "        else:\n",
        "            assert channels % num_head_channels == 0, (\n",
        "                f\"q,k,v channels {channels} is \"\n",
        "                f\"not divisible by num_head_channels {num_head_channels}\"\n",
        "            )\n",
        "            self.num_heads = channels // num_head_channels\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.norm = Normalize(channels)\n",
        "        self.qkv = nn.Conv1d(channels, channels * 3, 1)\n",
        "        self.attention = QKVAttentionLegacy(self.num_heads)\n",
        "\n",
        "        self.proj_out = zero_module(nn.Conv1d(channels, channels, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._forward(\n",
        "            x,\n",
        "        )\n",
        "\n",
        "    def _forward(self, x):\n",
        "        b, c, *spatial = x.shape\n",
        "        x = x.reshape(b, c, -1)\n",
        "        qkv = self.qkv(self.norm(x))\n",
        "        h = self.attention(qkv)\n",
        "        h = self.proj_out(h)\n",
        "        return (x + h).reshape(b, c, *spatial)\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    \"\"\"\n",
        "    A downsampling layer with an optional convolution.\n",
        "    :param channels: channels in the inputs and outputs.\n",
        "    :param use_conv: a bool determining if a convolution is applied.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, use_conv, out_channels=None, padding=1):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        if use_conv:\n",
        "            self.op = nn.Conv1d(\n",
        "                self.channels, self.out_channels, 3, stride=2, padding=padding\n",
        "            )#TODO:Mudar\n",
        "        else:\n",
        "            assert self.channels == self.out_channels\n",
        "            self.op = nn.AvgPool1d(kernel_size=2, stride=2)#TODO: Mudar\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.shape[1] == self.channels\n",
        "        return self.op(x)\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    \"\"\"\n",
        "    An upsampling layer with an optional convolution.\n",
        "    :param channels: channels in the inputs and outputs.\n",
        "    :param use_conv: a bool determining if a convolution is applied.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, use_conv, out_channels=None, padding=1):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        if use_conv:\n",
        "            self.conv = nn.Conv1d(\n",
        "                self.channels, self.out_channels, 3, padding=padding\n",
        "            )#TODO:Mudar\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.shape[1] == self.channels\n",
        "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
        "        if self.use_conv:\n",
        "            x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResBlock(TimestepBlock):\n",
        "    \"\"\"\n",
        "    A residual block that can optionally change the number of channels.\n",
        "    :param channels: the number of input channels.\n",
        "    :param emb_channels: the number of timestep embedding channels.\n",
        "    :param dropout: the rate of dropout.\n",
        "    :param out_channels: if specified, the number of out channels.\n",
        "    :param use_conv: if True and out_channels is specified, use a spatial\n",
        "        convolution instead of a smaller 1x1 convolution to change the\n",
        "        channels in the skip connection.\n",
        "    :param up: if True, use this block for upsampling.\n",
        "    :param down: if True, use this block for downsampling.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels,\n",
        "        emb_channels,\n",
        "        dropout,\n",
        "        out_channels=None,\n",
        "        use_conv=False,\n",
        "        use_scale_shift_norm=False,\n",
        "        up=False,\n",
        "        down=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.emb_channels = emb_channels\n",
        "        self.dropout = dropout\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.use_scale_shift_norm = use_scale_shift_norm\n",
        "\n",
        "        self.in_layers = nn.Sequential(\n",
        "            Normalize(channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv1d(channels, self.out_channels, 3, padding=1),\n",
        "        )\n",
        "\n",
        "        self.updown = up or down\n",
        "\n",
        "        if up:\n",
        "            self.h_upd = Upsample(channels, False)\n",
        "            self.x_upd = Upsample(channels, False)\n",
        "        elif down:\n",
        "            self.h_upd = Downsample(channels, False)\n",
        "            self.x_upd = Downsample(channels, False)\n",
        "        else:\n",
        "            self.h_upd = self.x_upd = nn.Identity()\n",
        "\n",
        "        self.emb_layers = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(\n",
        "                emb_channels,\n",
        "                2 * self.out_channels\n",
        "                if use_scale_shift_norm\n",
        "                else self.out_channels,\n",
        "            ),\n",
        "        )\n",
        "        self.out_layers = nn.Sequential(\n",
        "            Normalize(self.out_channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            zero_module(\n",
        "                nn.Conv1d(self.out_channels, self.out_channels, 3, padding=1)\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        if self.out_channels == channels:\n",
        "            self.skip_connection = nn.Identity()\n",
        "        elif use_conv:\n",
        "            self.skip_connection = nn.Conv1d(\n",
        "                channels, self.out_channels, 3, padding=1\n",
        "            )#TODO:Mudar\n",
        "        else:\n",
        "            self.skip_connection = nn.Conv1d(channels, self.out_channels, 1)#TODO:Mudar\n",
        "\n",
        "    def forward(self, x, emb):\n",
        "        return self._forward(x, emb)\n",
        "\n",
        "    def _forward(self, x, emb):\n",
        "        if self.updown:\n",
        "            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n",
        "            h = in_rest(x)\n",
        "            h = self.h_upd(h)\n",
        "            x = self.x_upd(x)\n",
        "            h = in_conv(h)\n",
        "        else:\n",
        "            h = self.in_layers(x)\n",
        "        emb_out = self.emb_layers(emb).type(h.dtype)\n",
        "        while len(emb_out.shape) < len(h.shape):\n",
        "            emb_out = emb_out[..., None]\n",
        "        if self.use_scale_shift_norm:\n",
        "            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n",
        "            scale, shift = th.chunk(emb_out, 2, dim=1)\n",
        "            h = out_norm(h) * (1 + scale) + shift\n",
        "            h = out_rest(h)\n",
        "        else:\n",
        "            h = h + emb_out\n",
        "            h = self.out_layers(h)\n",
        "        return self.skip_connection(x) + h\n",
        "\n",
        "class UNetModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size=32,\n",
        "        in_channels=1,\n",
        "        model_channels=32,\n",
        "        out_channels=1,\n",
        "        num_res_blocks=2,\n",
        "        attention_resolutions=[16, 8],\n",
        "        dropout=0.1,\n",
        "        channel_mult=(2, 4, 8),\n",
        "        num_heads=4,\n",
        "        use_scale_shift_norm=False,\n",
        "        resblock_updown=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.image_size = image_size\n",
        "        self.in_channels = in_channels\n",
        "        self.model_channels = model_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.attention_resolutions = attention_resolutions\n",
        "        self.dropout = dropout\n",
        "        self.channel_mult = channel_mult\n",
        "        self.num_heads = num_heads\n",
        "        self.use_scale_shift_norm = use_scale_shift_norm\n",
        "        self.resblock_updown = resblock_updown\n",
        "\n",
        "        time_embed_dim = model_channels * 4\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Linear(model_channels, time_embed_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_embed_dim, time_embed_dim),\n",
        "        )\n",
        "\n",
        "        self.input_blocks = nn.ModuleList([\n",
        "            TimestepEmbedSequential(nn.Conv1d(in_channels, model_channels, 3, padding=1))\n",
        "        ])\n",
        "        input_block_chans = [model_channels]\n",
        "        ch = model_channels\n",
        "        ds = 1\n",
        "\n",
        "        for level, mult in enumerate(channel_mult):\n",
        "            for _ in range(num_res_blocks):\n",
        "                layers = [ResBlock(ch, time_embed_dim, dropout, out_channels=mult * model_channels, use_scale_shift_norm=use_scale_shift_norm)]\n",
        "                ch = mult * model_channels\n",
        "                if ds in attention_resolutions:\n",
        "                    layers.append(AttentionBlock(ch, num_heads=num_heads))\n",
        "                self.input_blocks.append(TimestepEmbedSequential(*layers))\n",
        "                input_block_chans.append(ch)\n",
        "            if level != len(channel_mult) - 1:\n",
        "                out_ch = ch\n",
        "                self.input_blocks.append(TimestepEmbedSequential(Downsample(ch, True, out_channels=out_ch)))\n",
        "                ch = out_ch\n",
        "                input_block_chans.append(ch)\n",
        "                ds *= 2\n",
        "\n",
        "        self.middle_block = TimestepEmbedSequential(\n",
        "            ResBlock(ch, time_embed_dim, dropout, use_scale_shift_norm=use_scale_shift_norm),\n",
        "            AttentionBlock(ch, num_heads=num_heads),\n",
        "            ResBlock(ch, time_embed_dim, dropout, use_scale_shift_norm=use_scale_shift_norm),\n",
        "        )\n",
        "\n",
        "        self.output_blocks = nn.ModuleList([])\n",
        "        for level, mult in list(enumerate(channel_mult))[::-1]:\n",
        "            for i in range(num_res_blocks + 1):\n",
        "                ich = input_block_chans.pop()\n",
        "                layers = [ResBlock(ch + ich, time_embed_dim, dropout, out_channels=model_channels * mult, use_scale_shift_norm=use_scale_shift_norm)]\n",
        "                ch = model_channels * mult\n",
        "                if ds in attention_resolutions:\n",
        "                    layers.append(AttentionBlock(ch, num_heads=num_heads))\n",
        "                if level and i == num_res_blocks:\n",
        "                    out_ch = ch\n",
        "                    layers.append(Upsample(ch, True, out_channels=out_ch))\n",
        "                    ds //= 2\n",
        "                self.output_blocks.append(TimestepEmbedSequential(*layers))\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            Normalize(ch),\n",
        "            nn.SiLU(),\n",
        "            zero_module(nn.Conv1d(ch, out_channels, 3, padding=1)),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, timesteps=None, context=None, y=None):\n",
        "        assert timesteps is not None, \"timesteps must be provided\"\n",
        "        hs = []\n",
        "        t_emb = timestep_embedding(timesteps, self.model_channels, repeat_only=False)\n",
        "        emb = self.time_embed(t_emb)\n",
        "\n",
        "        h = x\n",
        "        for module in self.input_blocks:\n",
        "            h = module(h, emb, context)\n",
        "            hs.append(h)\n",
        "        h = self.middle_block(h, emb, context)\n",
        "\n",
        "        for module in self.output_blocks:\n",
        "            h_pop = hs.pop()\n",
        "            if h.shape[2] != h_pop.shape[2]:\n",
        "                h_pop = F.interpolate(h_pop, size=h.shape[2], mode='nearest')\n",
        "            h = torch.cat([h, h_pop], dim=1)\n",
        "            h = module(h, emb, context)\n",
        "\n",
        "        return self.out(h)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Latent Diffusion V3 from Improved Anomaly Detection\"\"\"\n",
        "from functools import partial\n",
        "from inspect import isfunction\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "def exists(x):\n",
        "    return x is not None\n",
        "\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if isfunction(d) else d\n",
        "\n",
        "\n",
        "def noise_like(shape, device, repeat=False):\n",
        "    repeat_noise = lambda: torch.randn((1, *shape[1:]), device=device).repeat(\n",
        "        shape[0], *((1,) * (len(shape) - 1))\n",
        "    )\n",
        "    noise = lambda: torch.randn(shape, device=device)\n",
        "    return repeat_noise() if repeat else noise()\n",
        "\n",
        "\n",
        "def extract(a, t, x_shape):\n",
        "    b, *_ = t.shape\n",
        "    out = a.gather(-1, t)\n",
        "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
        "\n",
        "\n",
        "def make_beta_schedule(\n",
        "    schedule, n_timestep, linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3\n",
        "):\n",
        "    if schedule == \"linear\":\n",
        "        betas = (\n",
        "            torch.linspace(\n",
        "                linear_start**0.5,\n",
        "                linear_end**0.5,\n",
        "                n_timestep,\n",
        "                dtype=torch.float64,\n",
        "            )\n",
        "            ** 2\n",
        "        )\n",
        "\n",
        "    elif schedule == \"cosine\":\n",
        "        timesteps = (\n",
        "            torch.arange(n_timestep + 1, dtype=torch.float64) / n_timestep\n",
        "            + cosine_s\n",
        "        )\n",
        "        alphas = timesteps / (1 + cosine_s) * np.pi / 2\n",
        "        alphas = torch.cos(alphas).pow(2)\n",
        "        alphas = alphas / alphas[0]\n",
        "        betas = 1 - alphas[1:] / alphas[:-1]\n",
        "        betas = np.clip(betas, a_min=0, a_max=0.999)\n",
        "\n",
        "    elif schedule == \"sqrt_linear\":\n",
        "        betas = torch.linspace(\n",
        "            linear_start, linear_end, n_timestep, dtype=torch.float64\n",
        "        )\n",
        "    elif schedule == \"sqrt\":\n",
        "        betas = (\n",
        "            torch.linspace(\n",
        "                linear_start, linear_end, n_timestep, dtype=torch.float64\n",
        "            )\n",
        "            ** 0.5\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"schedule '{schedule}' unknown.\")\n",
        "    return betas.numpy()\n",
        "\n",
        "\n",
        "class DDPM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        unet_config,\n",
        "        timesteps: int = 1000,\n",
        "        beta_schedule=\"linear\",\n",
        "        loss_type=\"l2\",\n",
        "        log_every_t=100,\n",
        "        clip_denoised=False,\n",
        "        linear_start=1e-4,\n",
        "        linear_end=2e-2,\n",
        "        cosine_s=8e-3,\n",
        "        original_elbo_weight=0.0,\n",
        "        v_posterior=0.0,\n",
        "        # weight for choosing posterior\n",
        "        # variance as sigma = (1-v) * beta_tilde + v * beta\n",
        "        l_simple_weight=1.0,\n",
        "        parameterization=\"eps\",  # all assuming fixed variance schedules\n",
        "        learn_logvar=False,\n",
        "        logvar_init=0.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert parameterization in [\n",
        "            \"eps\",\n",
        "            \"x0\",\n",
        "        ], 'currently only supporting \"eps\" and \"x0\"'\n",
        "        self.parameterization = parameterization\n",
        "\n",
        "        self.model = UNetModel(**unet_config.get(\"params\", dict()))\n",
        "\n",
        "        self.clip_denoised = clip_denoised\n",
        "        self.log_every_t = log_every_t\n",
        "\n",
        "        self.v_posterior = v_posterior\n",
        "        self.original_elbo_weight = original_elbo_weight\n",
        "        self.l_simple_weight = l_simple_weight\n",
        "\n",
        "        self.loss_type = loss_type\n",
        "\n",
        "        self.register_schedule(\n",
        "            beta_schedule=beta_schedule,\n",
        "            timesteps=timesteps,\n",
        "            linear_start=linear_start,\n",
        "            linear_end=linear_end,\n",
        "            cosine_s=cosine_s,\n",
        "        )\n",
        "\n",
        "        self.learn_logvar = learn_logvar\n",
        "        self.logvar = torch.full(\n",
        "            fill_value=logvar_init, size=(self.num_timesteps,)\n",
        "        )\n",
        "        if self.learn_logvar:\n",
        "            self.logvar = nn.Parameter(self.logvar, requires_grad=True)\n",
        "\n",
        "    def register_schedule(\n",
        "        self,\n",
        "        beta_schedule=\"linear\",\n",
        "        timesteps=1000,\n",
        "        linear_start=1e-4,\n",
        "        linear_end=2e-2,\n",
        "        cosine_s=8e-3,\n",
        "    ):\n",
        "        betas = make_beta_schedule(\n",
        "            beta_schedule,\n",
        "            timesteps,\n",
        "            linear_start=linear_start,\n",
        "            linear_end=linear_end,\n",
        "            cosine_s=cosine_s,\n",
        "        )\n",
        "        alphas = 1.0 - betas\n",
        "        alphas_cumprod = np.cumprod(alphas, axis=0)\n",
        "        alphas_cumprod_prev = np.append(1.0, alphas_cumprod[:-1])\n",
        "\n",
        "        (timesteps,) = betas.shape\n",
        "        self.num_timesteps = int(timesteps)\n",
        "        self.linear_start = linear_start\n",
        "        self.linear_end = linear_end\n",
        "\n",
        "        to_torch = partial(torch.tensor, dtype=torch.float32)\n",
        "\n",
        "        self.register_buffer(\"betas\", to_torch(betas))\n",
        "        self.register_buffer(\"alphas_cumprod\", to_torch(alphas_cumprod))\n",
        "        self.register_buffer(\n",
        "            \"alphas_cumprod_prev\", to_torch(alphas_cumprod_prev)\n",
        "        )\n",
        "\n",
        "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
        "        self.register_buffer(\n",
        "            \"sqrt_alphas_cumprod\", to_torch(np.sqrt(alphas_cumprod))\n",
        "        )\n",
        "        self.register_buffer(\n",
        "            \"sqrt_one_minus_alphas_cumprod\",\n",
        "            to_torch(np.sqrt(1.0 - alphas_cumprod)),\n",
        "        )\n",
        "        self.register_buffer(\n",
        "            \"log_one_minus_alphas_cumprod\",\n",
        "            to_torch(np.log(1.0 - alphas_cumprod)),\n",
        "        )\n",
        "        self.register_buffer(\n",
        "            \"sqrt_recip_alphas_cumprod\",\n",
        "            to_torch(np.sqrt(1.0 / alphas_cumprod)),\n",
        "        )\n",
        "        self.register_buffer(\n",
        "            \"sqrt_recipm1_alphas_cumprod\",\n",
        "            to_torch(np.sqrt(1.0 / alphas_cumprod - 1)),\n",
        "        )\n",
        "\n",
        "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
        "        posterior_variance = (1 - self.v_posterior) * betas * (\n",
        "            1.0 - alphas_cumprod_prev\n",
        "        ) / (1.0 - alphas_cumprod) + self.v_posterior * betas\n",
        "        # above: equal to 1. / (1. /\n",
        "        # (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n",
        "        self.register_buffer(\n",
        "            \"posterior_variance\", to_torch(posterior_variance)\n",
        "        )\n",
        "        # below: log calculation clipped because the\n",
        "        # posterior variance is 0 at the beginning of the diffusion chain\n",
        "        self.register_buffer(\n",
        "            \"posterior_log_variance_clipped\",\n",
        "            to_torch(np.log(np.maximum(posterior_variance, 1e-20))),\n",
        "        )\n",
        "        self.register_buffer(\n",
        "            \"posterior_mean_coef1\",\n",
        "            to_torch(\n",
        "                betas * np.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
        "            ),\n",
        "        )\n",
        "        self.register_buffer(\n",
        "            \"posterior_mean_coef2\",\n",
        "            to_torch(\n",
        "                (1.0 - alphas_cumprod_prev)\n",
        "                * np.sqrt(alphas)\n",
        "                / (1.0 - alphas_cumprod)\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        if self.parameterization == \"eps\":\n",
        "            lvlb_weights = self.betas**2 / (\n",
        "                2\n",
        "                * self.posterior_variance\n",
        "                * to_torch(alphas)\n",
        "                * (1 - self.alphas_cumprod)\n",
        "            )\n",
        "        elif self.parameterization == \"x0\":\n",
        "            lvlb_weights = (\n",
        "                0.5\n",
        "                * np.sqrt(torch.Tensor(alphas_cumprod))\n",
        "                / (2.0 * 1 - torch.Tensor(alphas_cumprod))\n",
        "            )\n",
        "        else:\n",
        "            raise NotImplementedError(\"mu not supported\")\n",
        "        # TODO how to choose this term\n",
        "        lvlb_weights[0] = lvlb_weights[1]\n",
        "        self.register_buffer(\"lvlb_weights\", lvlb_weights, persistent=False)\n",
        "        assert not torch.isnan(self.lvlb_weights).all()\n",
        "\n",
        "    def q_mean_variance(self, x_start, t):\n",
        "        \"\"\"\n",
        "        Get the distribution q(x_t | x_0).\n",
        "        :param x_start: the [N x C x ...] tensor of\n",
        "        noiseless inputs.\n",
        "        :param t: the number of diffusion steps (minus 1).\n",
        "        Here, 0 means one step.\n",
        "        :return: A tuple (mean, variance, log_variance),\n",
        "        all of x_start's shape.\n",
        "        \"\"\"\n",
        "        mean = extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
        "        variance = extract(1.0 - self.alphas_cumprod, t, x_start.shape)\n",
        "        log_variance = extract(\n",
        "            self.log_one_minus_alphas_cumprod, t, x_start.shape\n",
        "        )\n",
        "        return mean, variance, log_variance\n",
        "\n",
        "    def predict_start_from_noise(self, x_t, t, noise):\n",
        "        return (\n",
        "            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n",
        "            - extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
        "        )\n",
        "\n",
        "    def q_posterior(self, x_start, x_t, t):\n",
        "        \"\"\"\n",
        "        Compute the mean and variance of the diffusion posterior:\n",
        "            q(x_{t-1} | x_t, x_0)\n",
        "        \"\"\"\n",
        "        posterior_mean = (\n",
        "            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start\n",
        "            + extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
        "        )\n",
        "        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n",
        "        posterior_log_variance_clipped = extract(\n",
        "            self.posterior_log_variance_clipped, t, x_t.shape\n",
        "        )\n",
        "        return (\n",
        "            posterior_mean,\n",
        "            posterior_variance,\n",
        "            posterior_log_variance_clipped,\n",
        "        )\n",
        "\n",
        "    def p_mean_variance(self, x, t, clip_denoised: bool, return_x0=False):\n",
        "        \"\"\"\n",
        "        Apply the model to get p(x_{t-1} | x_t)\n",
        "        :param model: the model, which takes a signal and a batch of timesteps\n",
        "                      as input.\n",
        "        :param x: the [N x C x ...] tensor at time t.\n",
        "        :param t: a 1-D Tensor of timesteps.\n",
        "        :param clip_denoised: if True, clip the denoised signal into [-1, 1].\n",
        "        \"\"\"\n",
        "        model_out = self.model(x, t)\n",
        "        if self.parameterization == \"eps\":\n",
        "            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n",
        "        elif self.parameterization == \"x0\":\n",
        "            x_recon = model_out\n",
        "\n",
        "        if clip_denoised:\n",
        "            x_recon.clamp_(-1.0, 1.0)\n",
        "\n",
        "        (\n",
        "            model_mean,\n",
        "            posterior_variance,\n",
        "            posterior_log_variance,\n",
        "        ) = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n",
        "        if return_x0:\n",
        "            return (\n",
        "                model_mean,\n",
        "                posterior_variance,\n",
        "                posterior_log_variance,\n",
        "                x_recon,\n",
        "            )\n",
        "        else:\n",
        "            return model_mean, posterior_variance, posterior_log_variance\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def p_sample(\n",
        "        self,\n",
        "        x,\n",
        "        t,\n",
        "        clip_denoised=True,\n",
        "        repeat_noise=False,\n",
        "        return_x0=False,\n",
        "        temperature=1.0,\n",
        "        noise_dropout=0.0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Sample x_{t-1} from the model at the given timestep.\n",
        "        :param x: the current tensor at x_{t-1}.\n",
        "        :param t: the value of t, starting at 0 for the first diffusion step.\n",
        "        :param clip_denoised: if True, clip the x_start prediction to [-1, 1].\n",
        "        \"\"\"\n",
        "\n",
        "        b, *_, device = *x.shape, x.device\n",
        "        outputs = self.p_mean_variance(\n",
        "            x=x,\n",
        "            t=t,\n",
        "            clip_denoised=clip_denoised,\n",
        "            return_x0=return_x0,\n",
        "        )\n",
        "        if return_x0:\n",
        "            model_mean, _, model_log_variance, x0 = outputs\n",
        "        else:\n",
        "            model_mean, _, model_log_variance = outputs\n",
        "\n",
        "        noise = noise_like(x.shape, device, repeat_noise) * temperature\n",
        "        if noise_dropout > 0.0:\n",
        "            noise = torch.nn.functional.dropout(noise, p=noise_dropout)\n",
        "        # no noise when t == 0\n",
        "        nonzero_mask = (1 - (t == 0).float()).reshape(\n",
        "            b, *((1,) * (len(x.shape) - 1))\n",
        "        )\n",
        "        if return_x0:\n",
        "            return (\n",
        "                model_mean\n",
        "                + nonzero_mask * (0.5 * model_log_variance).exp() * noise,\n",
        "                x0,\n",
        "            )\n",
        "        else:\n",
        "            return (\n",
        "                model_mean\n",
        "                + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n",
        "            )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def p_sample_loop(self, shape, return_intermediates=False):\n",
        "        device = self.betas.device\n",
        "\n",
        "        b = shape[0]\n",
        "        img = torch.randn(shape, device=device)\n",
        "        intermediates = [img]\n",
        "\n",
        "        for i in tqdm(\n",
        "            reversed(range(0, self.num_timesteps)),\n",
        "            desc=\"sampling loop time step\",\n",
        "            total=self.num_timesteps,\n",
        "        ):\n",
        "            img = self.p_sample(\n",
        "                img,\n",
        "                torch.full((b,), i, device=device, dtype=torch.long),\n",
        "                clip_denoised=self.clip_denoised,\n",
        "            )\n",
        "            if i % self.log_every_t == 0 or i == self.num_timesteps - 1:\n",
        "                intermediates.append(img)\n",
        "        if return_intermediates:\n",
        "            return img, intermediates\n",
        "        return img\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, batch_size=16, return_intermediates=False):\n",
        "        image_size = self.image_size\n",
        "        channels = self.channels\n",
        "        return self.p_sample_loop(\n",
        "            (batch_size, channels, image_size, image_size),\n",
        "            return_intermediates=return_intermediates,\n",
        "        )\n",
        "\n",
        "    def q_sample(self, x_start, t, noise=None):\n",
        "        \"\"\"\n",
        "        Diffuse the data for a given number of diffusion steps.\n",
        "        In other words, sample from q(x_t | x_0).\n",
        "        :param x_start: the initial data batch.\n",
        "        :param t: the number of diffusion steps (minus 1). Here,\n",
        "        0 means one step.\n",
        "        :param noise: if specified, the split-out normal noise.\n",
        "        :return: A noisy version of x_start.\n",
        "        \"\"\"\n",
        "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
        "\n",
        "        return (\n",
        "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
        "            + extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n",
        "            * noise\n",
        "        )\n",
        "\n",
        "    def get_loss(self, pred, target, mean=True):\n",
        "        if self.loss_type == \"l1\":\n",
        "            loss = (target - pred).abs()\n",
        "            if mean:\n",
        "                loss = loss.mean()\n",
        "        elif self.loss_type == \"l2\":\n",
        "            if mean:\n",
        "                loss = torch.nn.functional.mse_loss(target, pred)\n",
        "            else:\n",
        "                loss = torch.nn.functional.mse_loss(\n",
        "                    target, pred, reduction=\"none\"\n",
        "                )\n",
        "        else:\n",
        "            raise NotImplementedError(\"unknown loss type '{loss_type}'\")\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def p_losses(self, x_start, t, noise=None):\n",
        "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
        "        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n",
        "        model_output = self.model(x_noisy, t)\n",
        "\n",
        "        loss_dict = {}\n",
        "        if self.parameterization == \"eps\":\n",
        "            target = noise\n",
        "        elif self.parameterization == \"x0\":\n",
        "            target = x_start\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                f\"Paramterization {self.parameterization} not yet supported\"\n",
        "            )\n",
        "\n",
        "        loss_simple = self.get_loss(model_output, target, mean=False).mean(\n",
        "            dim=[1, 2]\n",
        "        )\n",
        "        loss_dict.update({f\"loss_simple\": loss_simple.mean()})\n",
        "\n",
        "        logvar_t = self.logvar[t].to(x_start.device)\n",
        "        loss = loss_simple / torch.exp(logvar_t) + logvar_t\n",
        "        if self.learn_logvar:\n",
        "            loss_dict.update({f\"loss_gamma\": loss.mean()})\n",
        "            loss_dict.update({\"logvar\": self.logvar.data.mean()})\n",
        "\n",
        "        loss = self.l_simple_weight * loss.mean()\n",
        "\n",
        "        loss_vlb = self.get_loss(model_output, target, mean=False).mean(\n",
        "            dim=(1, 2)\n",
        "        )\n",
        "        loss_vlb = (self.lvlb_weights[t] * loss_vlb).mean()\n",
        "        loss_dict.update({f\"loss_vlb\": loss_vlb})\n",
        "        loss += self.original_elbo_weight * loss_vlb\n",
        "        loss_dict.update({f\"loss\": loss})\n",
        "\n",
        "        return loss, loss_dict\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        t = torch.randint(\n",
        "            0, self.num_timesteps, (x.shape[0],), device=x.device\n",
        "        ).long()\n",
        "        return self.p_losses(x, t, *args, **kwargs)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        self.learning_rate = 1e-4\n",
        "        lr = self.learning_rate\n",
        "        params = list(self.model.parameters())\n",
        "        if self.learn_logvar:\n",
        "            print(\"Diffusion model optimizing logvar\")\n",
        "            params.append(self.logvar)\n",
        "        opt = torch.optim.AdamW(params, lr=lr)\n",
        "        return opt"
      ],
      "metadata": {
        "id": "q6ZXC1uC7Fvf"
      },
      "id": "q6ZXC1uC7Fvf",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# 데이터 생성 함수\n",
        "def generate_data(num_normal=1000, num_anomalous=200):\n",
        "    normal_data = np.random.normal(loc=0, scale=1, size=(num_normal, 1, 32))\n",
        "    anomalous_data = np.random.uniform(low=-3, high=3, size=(num_anomalous, 1, 32))\n",
        "    normal_labels = np.zeros((num_normal,))\n",
        "    anomalous_labels = np.ones((num_anomalous,))\n",
        "\n",
        "    data = np.concatenate([normal_data, anomalous_data], axis=0)\n",
        "    labels = np.concatenate([normal_labels, anomalous_labels], axis=0)\n",
        "\n",
        "    return torch.tensor(data, dtype=torch.float32), torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "# 데이터 준비 (훈련 시 정상 데이터의 레이블 제거)\n",
        "train_data, _ = generate_data(num_normal=1000)\n",
        "test_data, test_labels = generate_data(num_normal=500, num_anomalous=100)\n",
        "\n",
        "# 데이터로더 생성\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(TensorDataset(train_data), batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(test_data, test_labels), batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "6G49dNg5_uKd"
      },
      "id": "6G49dNg5_uKd",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HvOmN170AUyk"
      },
      "id": "HvOmN170AUyk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DDPM 모델 초기화\n",
        "unet_config = {\n",
        "    \"params\": {\n",
        "        \"image_size\": 32,\n",
        "        \"in_channels\": 1,\n",
        "        \"model_channels\": 32,\n",
        "        \"out_channels\": 1,\n",
        "        \"num_res_blocks\": 2,\n",
        "        \"attention_resolutions\": [16, 8],\n",
        "        \"dropout\": 0.1,\n",
        "        \"channel_mult\": (2, 4, 8),\n",
        "        \"num_heads\": 4,\n",
        "    }\n",
        "}\n",
        "\n",
        "ddpm = DDPM(unet_config=unet_config)\n",
        "optimizer = ddpm.configure_optimizers()\n",
        "\n",
        "# DDPM 학습\n",
        "num_epochs = 10\n",
        "ddpm.train()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        x = batch[0].to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "        optimizer.zero_grad()\n",
        "        loss, _ = ddpm(x)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDr09hdY_uO1",
        "outputId": "2a0de52d-c3fd-4c72-e66f-4ff337abe171"
      },
      "id": "dDr09hdY_uO1",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.9831\n",
            "Epoch 2/10, Loss: 0.8904\n",
            "Epoch 3/10, Loss: 0.8334\n",
            "Epoch 4/10, Loss: 0.7647\n",
            "Epoch 5/10, Loss: 0.7008\n",
            "Epoch 6/10, Loss: 0.6601\n",
            "Epoch 7/10, Loss: 0.6253\n",
            "Epoch 8/10, Loss: 0.6186\n",
            "Epoch 9/10, Loss: 0.6029\n",
            "Epoch 10/10, Loss: 0.5771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.5"
      ],
      "metadata": {
        "id": "UZfAmhriAggl"
      },
      "id": "UZfAmhriAggl",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# DDPM 평가 모드\n",
        "ddpm.eval()\n",
        "anomaly_scores = []\n",
        "y_true = []\n",
        "\n",
        "# 데이터셋에서 이상 탐지 수행\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        x, labels = batch\n",
        "        x = x.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "        labels = labels.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "        t = torch.randint(0, ddpm.num_timesteps, (x.shape[0],), device=x.device).long()\n",
        "        x_noisy = ddpm.q_sample(x, t)\n",
        "        x_reconstructed = ddpm.p_sample(x_noisy, t)\n",
        "\n",
        "        loss = torch.nn.functional.mse_loss(x, x_reconstructed, reduction='none').mean(dim=[1, 2])\n",
        "        anomaly_scores.extend(loss.cpu().numpy())\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "\n",
        "# 박스플롯을 이용한 시각화\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.boxplot([\n",
        "    [anomaly_scores[i] for i in range(len(y_true)) if y_true[i] == 0],  # 정상 데이터\n",
        "    [anomaly_scores[i] for i in range(len(y_true)) if y_true[i] == 1]   # 이상 데이터\n",
        "], labels=['Normal', 'Anomalous'])\n",
        "\n",
        "# 임계값 표시 (빨간색 수평선)\n",
        "plt.axhline(y=threshold, color='r', linestyle='--', label=f'Threshold ({threshold:.4f})')\n",
        "\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Reconstruction Error')\n",
        "plt.title('Anomaly Detection using DDPM')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 정밀도, 재현율, F1 점수 계산\n",
        "predictions = [1 if score > threshold else 0 for score in anomaly_scores]\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_true, predictions, average='binary')\n",
        "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "BRN3f9Go_uRl",
        "outputId": "9a75157c-c058-4429-d493-dadade10e41a"
      },
      "id": "BRN3f9Go_uRl",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-ec5b5792ebab>:27: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
            "  plt.boxplot([\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.1636, Recall: 0.6300, F1 Score: 0.2598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GucdLlPg_uUd"
      },
      "id": "GucdLlPg_uUd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aKBXWH8t_uXO"
      },
      "id": "aKBXWH8t_uXO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5zFBse2S_uaF"
      },
      "id": "5zFBse2S_uaF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zFnxtzh9_udY"
      },
      "id": "zFnxtzh9_udY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5dc38fe-4290-49b8-b0b2-443f8f5a23ad",
      "metadata": {
        "id": "a5dc38fe-4290-49b8-b0b2-443f8f5a23ad"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}