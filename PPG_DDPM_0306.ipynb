{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c959315-11ef-4c28-aabe-4ee52629dc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from abc import abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd79bc33-41a4-4000-8e54-31d93dcfce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f986da2-d421-461d-8a9e-e1866b8a0951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "08b16dcb-270d-4fc6-8c23-c85228f9daca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 데이터 로드\n",
    "file_path = \"E:/dataset//PPG-BP/combined_dataset.xlsx\"\n",
    "df = pd.read_excel(file_path)  # read_csv -> read_excel로 변경\n",
    "df\n",
    "\n",
    "df['Hypertension'].value_counts()\n",
    "\n",
    "# Normal을 8:2로 나눔 (훈련: 80%, 테스트: 20%)\n",
    "normal_train, normal_test = train_test_split(\n",
    "    df[df[\"Hypertension\"] == \"Normal\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 나머지 클래스는 테스트 데이터에만 포함\n",
    "test_data = normal_test.copy()\n",
    "for category in [\"Prehypertension\", \"Stage 1 hypertension\", \"Stage 2 hypertension\"]:\n",
    "    _, test = train_test_split(\n",
    "        df[df[\"Hypertension\"] == category], test_size=0.2, random_state=42\n",
    "    )\n",
    "    test_data = pd.concat([test_data, test])\n",
    "\n",
    "# 훈련 데이터는 Normal만 포함\n",
    "train_data = normal_train.copy()\n",
    "train_data[\"Hypertension\"].value_counts()\n",
    "test_data[\"Hypertension\"].value_counts()\n",
    "\n",
    "ppg_columns = [str(i) for i in range(2091, 2101)]\n",
    "\n",
    "# 'Hypertension'이 0 (Normal)인 데이터만 훈련 데이터로 선택\n",
    "train_data = train_data[train_data[\"Hypertension\"] == 'Normal'][ppg_columns]\n",
    "\n",
    "# 테스트 데이터는 기존과 동일하게 유지 (Hypertension 레이블 + PPG 데이터)\n",
    "test_data = test_data[[\"Hypertension\"] + ppg_columns]\n",
    "\n",
    "test_data['Hypertension'].value_counts()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Label Encoding을 위한 변환기 생성\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Hypertension 컬럼을 Label Encoding 적용\n",
    "test_data[\"Hypertension_Encoded\"] = label_encoder.fit_transform(test_data[\"Hypertension\"])\n",
    "\n",
    "# 기존 Hypertension 컬럼 제거 후 새로운 컬럼으로 대체\n",
    "test_data_encoded = test_data.drop(columns=[\"Hypertension\"]).rename(columns={\"Hypertension_Encoded\": \"Hypertension\"})\n",
    "test_data_encoded['Hypertension'].value_counts()\n",
    "\n",
    "\n",
    "batch_size= 32\n",
    "\n",
    "# TensorDataset 생성\n",
    "train_data_tensor = torch.tensor(train_data.values, dtype=torch.float32).unsqueeze(-1)\n",
    "train_dataset = TensorDataset(train_data_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data_tensor = torch.tensor(test_data_encoded.drop(columns=[\"Hypertension\"]).values, dtype=torch.float32).unsqueeze(-1)\n",
    "test_labels_tensor = torch.tensor(test_data_encoded[\"Hypertension\"].values, dtype=torch.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1def822a-6e89-4cd3-9337-4245c23aae38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((192, 10), (132, 11))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data_encoded.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58366dc4-50b7-4f0f-8106-d7c4c70cfe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestep_embedding(timesteps, dim, max_period=10000, repeat_only=False):\n",
    "    \"\"\"\n",
    "    Create sinusoidal timestep embeddings.\n",
    "    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n",
    "                      These may be fractional.\n",
    "    :param dim: the dimension of the output.\n",
    "    :param max_period: controls the minimum frequency of the embeddings.\n",
    "    :return: an [N x dim] Tensor of positional embeddings.\n",
    "    \"\"\"\n",
    "    if not repeat_only:\n",
    "        half = dim // 2\n",
    "        freqs = torch.exp(\n",
    "            -math.log(max_period)\n",
    "            * torch.arange(start=0, end=half, dtype=torch.float32)\n",
    "            / half\n",
    "        ).to(device=timesteps.device)\n",
    "        args = timesteps[:, None].float() * freqs[None]\n",
    "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "        if dim % 2:\n",
    "            embedding = torch.cat(\n",
    "                [embedding, torch.zeros_like(embedding[:, :1])], dim=-1\n",
    "            )\n",
    "    else:\n",
    "        embedding = repeat(timesteps, \"b -> b d\", d=dim)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def zero_module(module):\n",
    "    \"\"\"\n",
    "    Zero out the parameters of a module and return it.\n",
    "    \"\"\"\n",
    "    for p in module.parameters():\n",
    "        p.detach().zero_()\n",
    "    return module\n",
    "\n",
    "\n",
    "class TimestepBlock(nn.Module):\n",
    "    @abstractmethod\n",
    "    def forward(self, x, emb):\n",
    "        \"\"\"\n",
    "        Apply the module to `x` given `emb` timestep embeddings.\n",
    "        \"\"\"\n",
    "\n",
    "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n",
    "    \"\"\"\n",
    "    A sequential module that passes timestep embeddings to the children that\n",
    "    support it as an extra input.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x, emb, context=None):\n",
    "        for layer in self:\n",
    "            if isinstance(layer, TimestepBlock):\n",
    "                x = layer(x, emb)  # Pass emb to TimestepBlock layers\n",
    "            else:\n",
    "                x = layer(x)  # Regular layers do not receive emb\n",
    "        return x\n",
    "\n",
    "def Normalize(in_channels):\n",
    "    return nn.GroupNorm(\n",
    "        num_groups=32, num_channels=in_channels, eps=1e-6, affine=True\n",
    "    )\n",
    "\n",
    "\n",
    "def count_flops_attn(model, _x, y):\n",
    "    \"\"\"\n",
    "    A counter for the `thop` package to count the operations in an\n",
    "    attention operation.\n",
    "    Meant to be used like:\n",
    "        macs, params = thop.profile(\n",
    "            model,\n",
    "            inputs=(inputs, timestamps),\n",
    "            custom_ops={QKVAttention: QKVAttention.count_flops},\n",
    "        )\n",
    "    \"\"\"\n",
    "    b, c, *spatial = y[0].shape\n",
    "    num_spatial = int(np.prod(spatial))\n",
    "    # We perform two matmuls with the same number of ops.\n",
    "    # The first computes the weight matrix, the second computes\n",
    "    # the combination of the value vectors.\n",
    "    matmul_ops = 2 * b * (num_spatial**2) * c\n",
    "    model.total_ops += th.DoubleTensor([matmul_ops])\n",
    "\n",
    "\n",
    "class QKVAttentionLegacy(nn.Module):\n",
    "    \"\"\"\n",
    "    A module which performs QKV attention.\n",
    "    Matches legacy QKVAttention + input/ouput heads shaping\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        \"\"\"\n",
    "        Apply QKV attention.\n",
    "        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n",
    "        :return: an [N x (H * C) x T] tensor after attention.\n",
    "        \"\"\"\n",
    "        bs, width, length = qkv.shape\n",
    "        assert width % (3 * self.n_heads) == 0\n",
    "        ch = width // (3 * self.n_heads)\n",
    "        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(\n",
    "            ch, dim=1\n",
    "        )\n",
    "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
    "        weight = th.einsum(\n",
    "            \"bct,bcs->bts\", q * scale, k * scale\n",
    "        )  # More stable with f16 than dividing afterwards\n",
    "        weight = th.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
    "        a = th.einsum(\"bts,bcs->bct\", weight, v)\n",
    "        return a.reshape(bs, -1, length)\n",
    "\n",
    "    @staticmethod\n",
    "    def count_flops(model, _x, y):\n",
    "        return count_flops_attn(model, _x, y)\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    An attention block that allows spatial positions to attend to each other.\n",
    "    Originally ported from here, but adapted to the N-d case.\n",
    "    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        num_heads=1,\n",
    "        num_head_channels=-1,\n",
    "        use_checkpoint=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        if num_head_channels == -1:\n",
    "            self.num_heads = num_heads\n",
    "        else:\n",
    "            assert channels % num_head_channels == 0, (\n",
    "                f\"q,k,v channels {channels} is \"\n",
    "                f\"not divisible by num_head_channels {num_head_channels}\"\n",
    "            )\n",
    "            self.num_heads = channels // num_head_channels\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.norm = Normalize(channels)\n",
    "        self.qkv = nn.Conv1d(channels, channels * 3, 1)\n",
    "        self.attention = QKVAttentionLegacy(self.num_heads)\n",
    "\n",
    "        self.proj_out = zero_module(nn.Conv1d(channels, channels, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward(\n",
    "            x,\n",
    "        )\n",
    "\n",
    "    def _forward(self, x):\n",
    "        b, c, *spatial = x.shape\n",
    "        x = x.reshape(b, c, -1)\n",
    "        qkv = self.qkv(self.norm(x))\n",
    "        h = self.attention(qkv)\n",
    "        h = self.proj_out(h)\n",
    "        return (x + h).reshape(b, c, *spatial)\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    \"\"\"\n",
    "    A downsampling layer with an optional convolution.\n",
    "    :param channels: channels in the inputs and outputs.\n",
    "    :param use_conv: a bool determining if a convolution is applied.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, use_conv, out_channels=None, padding=1):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        if use_conv:\n",
    "            self.op = nn.Conv1d(\n",
    "                self.channels, self.out_channels, 3, stride=2, padding=padding\n",
    "            )#TODO:Mudar\n",
    "        else:\n",
    "            assert self.channels == self.out_channels\n",
    "            self.op = nn.AvgPool1d(kernel_size=2, stride=2)#TODO: Mudar\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        return self.op(x)\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    \"\"\"\n",
    "    An upsampling layer with an optional convolution.\n",
    "    :param channels: channels in the inputs and outputs.\n",
    "    :param use_conv: a bool determining if a convolution is applied.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, use_conv, out_channels=None, padding=1):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        if use_conv:\n",
    "            self.conv = nn.Conv1d(\n",
    "                self.channels, self.out_channels, 3, padding=padding\n",
    "            )#TODO:Mudar\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        if self.use_conv:\n",
    "            x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResBlock(TimestepBlock):  # Ensure ResBlock inherits from TimestepBlock\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        emb_channels,\n",
    "        dropout,\n",
    "        out_channels=None,\n",
    "        use_conv=False,\n",
    "        use_scale_shift_norm=False,\n",
    "        up=False,\n",
    "        down=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.emb_channels = emb_channels\n",
    "        self.dropout = dropout\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        self.use_scale_shift_norm = use_scale_shift_norm\n",
    "\n",
    "        self.in_layers = nn.Sequential(\n",
    "            Normalize(channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv1d(channels, self.out_channels, 3, padding=1),\n",
    "        )\n",
    "\n",
    "        self.updown = up or down\n",
    "\n",
    "        if up:\n",
    "            self.h_upd = Upsample(channels, False)\n",
    "            self.x_upd = Upsample(channels, False)\n",
    "        elif down:\n",
    "            self.h_upd = Downsample(channels, False)\n",
    "            self.x_upd = Downsample(channels, False)\n",
    "        else:\n",
    "            self.h_upd = self.x_upd = nn.Identity()\n",
    "\n",
    "        self.emb_layers = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(\n",
    "                emb_channels,\n",
    "                2 * self.out_channels\n",
    "                if use_scale_shift_norm\n",
    "                else self.out_channels,\n",
    "            ),\n",
    "        )\n",
    "        self.out_layers = nn.Sequential(\n",
    "            Normalize(self.out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            zero_module(\n",
    "                nn.Conv1d(self.out_channels, self.out_channels, 3, padding=1)\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if self.out_channels == channels:\n",
    "            self.skip_connection = nn.Identity()\n",
    "        elif use_conv:\n",
    "            self.skip_connection = nn.Conv1d(\n",
    "                channels, self.out_channels, kernel_size=1\n",
    "            )\n",
    "        else:\n",
    "            self.skip_connection = nn.Conv1d(channels, self.out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        \"\"\"\n",
    "        Apply the ResBlock to `x` with timestep embeddings `emb`.\n",
    "        \"\"\"\n",
    "        if self.updown:\n",
    "            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n",
    "            h = in_rest(x)\n",
    "            h = self.h_upd(h)\n",
    "            x = self.x_upd(x)\n",
    "            h = in_conv(h)\n",
    "        else:\n",
    "            h = self.in_layers(x)\n",
    "\n",
    "        emb_out = self.emb_layers(emb).type(h.dtype)\n",
    "        while len(emb_out.shape) < len(h.shape):\n",
    "            emb_out = emb_out[..., None]\n",
    "\n",
    "        if self.use_scale_shift_norm:\n",
    "            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n",
    "            scale, shift = torch.chunk(emb_out, 2, dim=1)\n",
    "            h = out_norm(h) * (1 + scale) + shift\n",
    "            h = out_rest(h)\n",
    "        else:\n",
    "            h = h + emb_out\n",
    "            h = self.out_layers(h)\n",
    "\n",
    "        return self.skip_connection(x) + h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78ddf1cd-2926-43db-a641-5e65f43c2b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def timestep_embedding(timesteps, dim, max_period=10000, repeat_only=False):\n",
    "    \"\"\"\n",
    "    Create sinusoidal timestep embeddings.\n",
    "    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n",
    "                      These may be fractional.\n",
    "    :param dim: the dimension of the output.\n",
    "    :param max_period: controls the minimum frequency of the embeddings.\n",
    "    :return: an [N x dim] Tensor of positional embeddings.\n",
    "    \"\"\"\n",
    "    if not repeat_only:\n",
    "        half = dim // 2\n",
    "        freqs = torch.exp(\n",
    "            -math.log(max_period)\n",
    "            * torch.arange(start=0, end=half, dtype=torch.float32)\n",
    "            / half\n",
    "        ).to(device=timesteps.device)\n",
    "        args = timesteps[:, None].float() * freqs[None]\n",
    "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "        if dim % 2:\n",
    "            embedding = torch.cat(\n",
    "                [embedding, torch.zeros_like(embedding[:, :1])], dim=-1\n",
    "            )\n",
    "    else:\n",
    "        embedding = repeat(timesteps, \"b -> b d\", d=dim)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def zero_module(module):\n",
    "    \"\"\"\n",
    "    Zero out the parameters of a module and return it.\n",
    "    \"\"\"\n",
    "    for p in module.parameters():\n",
    "        p.detach().zero_()\n",
    "    return module\n",
    "\n",
    "\n",
    "class TimestepBlock(nn.Module):\n",
    "    @abstractmethod\n",
    "    def forward(self, x, emb):\n",
    "        \"\"\"\n",
    "        Apply the module to `x` given `emb` timestep embeddings.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n",
    "    \"\"\"\n",
    "    A sequential module that passes timestep embeddings to the children that\n",
    "    support it as an extra input.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x, emb, context=None):\n",
    "        for layer in self:\n",
    "            if isinstance(layer, TimestepBlock):\n",
    "                x = layer(x, emb)  # Pass emb to TimestepBlock layers\n",
    "            else:\n",
    "                x = layer(x)  # Regular layers do not receive emb\n",
    "        return x\n",
    "\n",
    "def Normalize(in_channels):\n",
    "    return nn.GroupNorm(\n",
    "        num_groups=32, num_channels=in_channels, eps=1e-6, affine=True\n",
    "    )\n",
    "\n",
    "\n",
    "def count_flops_attn(model, _x, y):\n",
    "    \"\"\"\n",
    "    A counter for the `thop` package to count the operations in an\n",
    "    attention operation.\n",
    "    Meant to be used like:\n",
    "        macs, params = thop.profile(\n",
    "            model,\n",
    "            inputs=(inputs, timestamps),\n",
    "            custom_ops={QKVAttention: QKVAttention.count_flops},\n",
    "        )\n",
    "    \"\"\"\n",
    "    b, c, *spatial = y[0].shape\n",
    "    num_spatial = int(np.prod(spatial))\n",
    "    # We perform two matmuls with the same number of ops.\n",
    "    # The first computes the weight matrix, the second computes\n",
    "    # the combination of the value vectors.\n",
    "    matmul_ops = 2 * b * (num_spatial**2) * c\n",
    "    model.total_ops += th.DoubleTensor([matmul_ops])\n",
    "\n",
    "\n",
    "class QKVAttentionLegacy(nn.Module):\n",
    "    \"\"\"\n",
    "    A module which performs QKV attention.\n",
    "    Matches legacy QKVAttention + input/ouput heads shaping\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        \"\"\"\n",
    "        Apply QKV attention.\n",
    "        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n",
    "        :return: an [N x (H * C) x T] tensor after attention.\n",
    "        \"\"\"\n",
    "        bs, width, length = qkv.shape\n",
    "        assert width % (3 * self.n_heads) == 0\n",
    "        ch = width // (3 * self.n_heads)\n",
    "        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(\n",
    "            ch, dim=1\n",
    "        )\n",
    "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
    "        weight = th.einsum(\n",
    "            \"bct,bcs->bts\", q * scale, k * scale\n",
    "        )  # More stable with f16 than dividing afterwards\n",
    "        weight = th.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
    "        a = th.einsum(\"bts,bcs->bct\", weight, v)\n",
    "        return a.reshape(bs, -1, length)\n",
    "\n",
    "    @staticmethod\n",
    "    def count_flops(model, _x, y):\n",
    "        return count_flops_attn(model, _x, y)\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    An attention block that allows spatial positions to attend to each other.\n",
    "    Originally ported from here, but adapted to the N-d case.\n",
    "    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        num_heads=1,\n",
    "        num_head_channels=-1,\n",
    "        use_checkpoint=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        if num_head_channels == -1:\n",
    "            self.num_heads = num_heads\n",
    "        else:\n",
    "            assert channels % num_head_channels == 0, (\n",
    "                f\"q,k,v channels {channels} is \"\n",
    "                f\"not divisible by num_head_channels {num_head_channels}\"\n",
    "            )\n",
    "            self.num_heads = channels // num_head_channels\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.norm = Normalize(channels)\n",
    "        self.qkv = nn.Conv1d(channels, channels * 3, 1)\n",
    "        self.attention = QKVAttentionLegacy(self.num_heads)\n",
    "\n",
    "        self.proj_out = zero_module(nn.Conv1d(channels, channels, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward(\n",
    "            x,\n",
    "        )\n",
    "\n",
    "    def _forward(self, x):\n",
    "        b, c, *spatial = x.shape\n",
    "        x = x.reshape(b, c, -1)\n",
    "        qkv = self.qkv(self.norm(x))\n",
    "        h = self.attention(qkv)\n",
    "        h = self.proj_out(h)\n",
    "        return (x + h).reshape(b, c, *spatial)\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    \"\"\"\n",
    "    A downsampling layer with an optional convolution.\n",
    "    :param channels: channels in the inputs and outputs.\n",
    "    :param use_conv: a bool determining if a convolution is applied.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, use_conv, out_channels=None, padding=1):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        if use_conv:\n",
    "            self.op = nn.Conv1d(\n",
    "                self.channels, self.out_channels, 3, stride=2, padding=padding\n",
    "            )#TODO:Mudar\n",
    "        else:\n",
    "            assert self.channels == self.out_channels\n",
    "            self.op = nn.AvgPool1d(kernel_size=2, stride=2)#TODO: Mudar\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        return self.op(x)\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    \"\"\"\n",
    "    An upsampling layer with an optional convolution.\n",
    "    :param channels: channels in the inputs and outputs.\n",
    "    :param use_conv: a bool determining if a convolution is applied.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, use_conv, out_channels=None, padding=1):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        if use_conv:\n",
    "            self.conv = nn.Conv1d(\n",
    "                self.channels, self.out_channels, 3, padding=padding\n",
    "            )#TODO:Mudar\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        if self.use_conv:\n",
    "            x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResBlock(TimestepBlock):  # Ensure ResBlock inherits from TimestepBlock\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        emb_channels,\n",
    "        dropout,\n",
    "        out_channels=None,\n",
    "        use_conv=False,\n",
    "        use_scale_shift_norm=False,\n",
    "        up=False,\n",
    "        down=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.emb_channels = emb_channels\n",
    "        self.dropout = dropout\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        self.use_scale_shift_norm = use_scale_shift_norm\n",
    "\n",
    "        self.in_layers = nn.Sequential(\n",
    "            Normalize(channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv1d(channels, self.out_channels, 3, padding=1),\n",
    "        )\n",
    "\n",
    "        self.updown = up or down\n",
    "\n",
    "        if up:\n",
    "            self.h_upd = Upsample(channels, False)\n",
    "            self.x_upd = Upsample(channels, False)\n",
    "        elif down:\n",
    "            self.h_upd = Downsample(channels, False)\n",
    "            self.x_upd = Downsample(channels, False)\n",
    "        else:\n",
    "            self.h_upd = self.x_upd = nn.Identity()\n",
    "\n",
    "        self.emb_layers = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(\n",
    "                emb_channels,\n",
    "                2 * self.out_channels\n",
    "                if use_scale_shift_norm\n",
    "                else self.out_channels,\n",
    "            ),\n",
    "        )\n",
    "        self.out_layers = nn.Sequential(\n",
    "            Normalize(self.out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            zero_module(\n",
    "                nn.Conv1d(self.out_channels, self.out_channels, 3, padding=1)\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if self.out_channels == channels:\n",
    "            self.skip_connection = nn.Identity()\n",
    "        elif use_conv:\n",
    "            self.skip_connection = nn.Conv1d(\n",
    "                channels, self.out_channels, kernel_size=1\n",
    "            )\n",
    "        else:\n",
    "            self.skip_connection = nn.Conv1d(channels, self.out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        \"\"\"\n",
    "        Apply the ResBlock to `x` with timestep embeddings `emb`.\n",
    "        \"\"\"\n",
    "        if self.updown:\n",
    "            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n",
    "            h = in_rest(x)\n",
    "            h = self.h_upd(h)\n",
    "            x = self.x_upd(x)\n",
    "            h = in_conv(h)\n",
    "        else:\n",
    "            h = self.in_layers(x)\n",
    "\n",
    "        emb_out = self.emb_layers(emb).type(h.dtype)\n",
    "        while len(emb_out.shape) < len(h.shape):\n",
    "            emb_out = emb_out[..., None]\n",
    "\n",
    "        if self.use_scale_shift_norm:\n",
    "            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n",
    "            scale, shift = torch.chunk(emb_out, 2, dim=1)\n",
    "            h = out_norm(h) * (1 + scale) + shift\n",
    "            h = out_rest(h)\n",
    "        else:\n",
    "            h = h + emb_out\n",
    "            h = self.out_layers(h)\n",
    "\n",
    "        return self.skip_connection(x) + h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "965e46dd-c1d4-47ef-8e08-b71b5e6e5915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from inspect import isfunction\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "\n",
    "def noise_like(shape, device, repeat=False):\n",
    "    repeat_noise = lambda: torch.randn((1, *shape[1:]), device=device).repeat(\n",
    "        shape[0], *((1,) * (len(shape) - 1))\n",
    "    )\n",
    "    noise = lambda: torch.randn(shape, device=device)\n",
    "    return repeat_noise() if repeat else noise()\n",
    "\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "\n",
    "def make_beta_schedule(\n",
    "    schedule, n_timestep, linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3\n",
    "):\n",
    "    if schedule == \"linear\":\n",
    "        betas = (\n",
    "            torch.linspace(\n",
    "                linear_start**0.5,\n",
    "                linear_end**0.5,\n",
    "                n_timestep,\n",
    "                dtype=torch.float64,\n",
    "            )\n",
    "            ** 2\n",
    "        )\n",
    "\n",
    "    elif schedule == \"cosine\":\n",
    "        timesteps = (\n",
    "            torch.arange(n_timestep + 1, dtype=torch.float64) / n_timestep\n",
    "            + cosine_s\n",
    "        )\n",
    "        alphas = timesteps / (1 + cosine_s) * np.pi / 2\n",
    "        alphas = torch.cos(alphas).pow(2)\n",
    "        alphas = alphas / alphas[0]\n",
    "        betas = 1 - alphas[1:] / alphas[:-1]\n",
    "        betas = np.clip(betas, a_min=0, a_max=0.999)\n",
    "\n",
    "    elif schedule == \"sqrt_linear\":\n",
    "        betas = torch.linspace(\n",
    "            linear_start, linear_end, n_timestep, dtype=torch.float64\n",
    "        )\n",
    "    elif schedule == \"sqrt\":\n",
    "        betas = (\n",
    "            torch.linspace(\n",
    "                linear_start, linear_end, n_timestep, dtype=torch.float64\n",
    "            )\n",
    "            ** 0.5\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"schedule '{schedule}' unknown.\")\n",
    "    return betas.numpy()\n",
    "\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        unet_config,\n",
    "        timesteps: int = 1000,\n",
    "        beta_schedule=\"linear\",\n",
    "        loss_type=\"l2\",\n",
    "        log_every_t=100,\n",
    "        clip_denoised=False,\n",
    "        linear_start=1e-4,\n",
    "        linear_end=2e-2,\n",
    "        cosine_s=8e-3,\n",
    "        original_elbo_weight=0.0,\n",
    "        v_posterior=0.0,\n",
    "        # weight for choosing posterior\n",
    "        # variance as sigma = (1-v) * beta_tilde + v * beta\n",
    "        l_simple_weight=1.0,\n",
    "        parameterization=\"eps\",  # all assuming fixed variance schedules\n",
    "        learn_logvar=False,\n",
    "        logvar_init=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert parameterization in [\n",
    "            \"eps\",\n",
    "            \"x0\",\n",
    "        ], 'currently only supporting \"eps\" and \"x0\"'\n",
    "        self.parameterization = parameterization\n",
    "\n",
    "        self.model = UNetModel(**unet_config.get(\"params\", dict()))\n",
    "\n",
    "        self.clip_denoised = clip_denoised\n",
    "        self.log_every_t = log_every_t\n",
    "\n",
    "        self.v_posterior = v_posterior\n",
    "        self.original_elbo_weight = original_elbo_weight\n",
    "        self.l_simple_weight = l_simple_weight\n",
    "\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "        self.register_schedule(\n",
    "            beta_schedule=beta_schedule,\n",
    "            timesteps=timesteps,\n",
    "            linear_start=linear_start,\n",
    "            linear_end=linear_end,\n",
    "            cosine_s=cosine_s,\n",
    "        )\n",
    "\n",
    "        self.learn_logvar = learn_logvar\n",
    "        self.logvar = torch.full(\n",
    "            fill_value=logvar_init, size=(self.num_timesteps,)\n",
    "        )\n",
    "        if self.learn_logvar:\n",
    "            self.logvar = nn.Parameter(self.logvar, requires_grad=True)\n",
    "\n",
    "    def register_schedule(\n",
    "        self,\n",
    "        beta_schedule=\"linear\",\n",
    "        timesteps=1000,\n",
    "        linear_start=1e-4,\n",
    "        linear_end=2e-2,\n",
    "        cosine_s=8e-3,\n",
    "    ):\n",
    "        betas = make_beta_schedule(\n",
    "            beta_schedule,\n",
    "            timesteps,\n",
    "            linear_start=linear_start,\n",
    "            linear_end=linear_end,\n",
    "            cosine_s=cosine_s,\n",
    "        )\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = np.cumprod(alphas, axis=0)\n",
    "        alphas_cumprod_prev = np.append(1.0, alphas_cumprod[:-1])\n",
    "\n",
    "        (timesteps,) = betas.shape\n",
    "        self.num_timesteps = int(timesteps)\n",
    "        self.linear_start = linear_start\n",
    "        self.linear_end = linear_end\n",
    "\n",
    "        to_torch = partial(torch.tensor, dtype=torch.float32)\n",
    "\n",
    "        self.register_buffer(\"betas\", to_torch(betas))\n",
    "        self.register_buffer(\"alphas_cumprod\", to_torch(alphas_cumprod))\n",
    "        self.register_buffer(\n",
    "            \"alphas_cumprod_prev\", to_torch(alphas_cumprod_prev)\n",
    "        )\n",
    "\n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "        self.register_buffer(\n",
    "            \"sqrt_alphas_cumprod\", to_torch(np.sqrt(alphas_cumprod))\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"sqrt_one_minus_alphas_cumprod\",\n",
    "            to_torch(np.sqrt(1.0 - alphas_cumprod)),\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"log_one_minus_alphas_cumprod\",\n",
    "            to_torch(np.log(1.0 - alphas_cumprod)),\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"sqrt_recip_alphas_cumprod\",\n",
    "            to_torch(np.sqrt(1.0 / alphas_cumprod)),\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"sqrt_recipm1_alphas_cumprod\",\n",
    "            to_torch(np.sqrt(1.0 / alphas_cumprod - 1)),\n",
    "        )\n",
    "\n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        posterior_variance = (1 - self.v_posterior) * betas * (\n",
    "            1.0 - alphas_cumprod_prev\n",
    "        ) / (1.0 - alphas_cumprod) + self.v_posterior * betas\n",
    "        # above: equal to 1. / (1. /\n",
    "        # (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n",
    "        self.register_buffer(\n",
    "            \"posterior_variance\", to_torch(posterior_variance)\n",
    "        )\n",
    "        # below: log calculation clipped because the\n",
    "        # posterior variance is 0 at the beginning of the diffusion chain\n",
    "        self.register_buffer(\n",
    "            \"posterior_log_variance_clipped\",\n",
    "            to_torch(np.log(np.maximum(posterior_variance, 1e-20))),\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"posterior_mean_coef1\",\n",
    "            to_torch(\n",
    "                betas * np.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
    "            ),\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"posterior_mean_coef2\",\n",
    "            to_torch(\n",
    "                (1.0 - alphas_cumprod_prev)\n",
    "                * np.sqrt(alphas)\n",
    "                / (1.0 - alphas_cumprod)\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if self.parameterization == \"eps\":\n",
    "            lvlb_weights = self.betas**2 / (\n",
    "                2\n",
    "                * self.posterior_variance\n",
    "                * to_torch(alphas)\n",
    "                * (1 - self.alphas_cumprod)\n",
    "            )\n",
    "        elif self.parameterization == \"x0\":\n",
    "            lvlb_weights = (\n",
    "                0.5\n",
    "                * np.sqrt(torch.Tensor(alphas_cumprod))\n",
    "                / (2.0 * 1 - torch.Tensor(alphas_cumprod))\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\"mu not supported\")\n",
    "        # TODO how to choose this term\n",
    "        lvlb_weights[0] = lvlb_weights[1]\n",
    "        self.register_buffer(\"lvlb_weights\", lvlb_weights, persistent=False)\n",
    "        assert not torch.isnan(self.lvlb_weights).all()\n",
    "\n",
    "    def q_mean_variance(self, x_start, t):\n",
    "        \"\"\"\n",
    "        Get the distribution q(x_t | x_0).\n",
    "        :param x_start: the [N x C x ...] tensor of\n",
    "        noiseless inputs.\n",
    "        :param t: the number of diffusion steps (minus 1).\n",
    "        Here, 0 means one step.\n",
    "        :return: A tuple (mean, variance, log_variance),\n",
    "        all of x_start's shape.\n",
    "        \"\"\"\n",
    "        mean = extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
    "        variance = extract(1.0 - self.alphas_cumprod, t, x_start.shape)\n",
    "        log_variance = extract(\n",
    "            self.log_one_minus_alphas_cumprod, t, x_start.shape\n",
    "        )\n",
    "        return mean, variance, log_variance\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        return (\n",
    "            extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n",
    "            - extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
    "        )\n",
    "\n",
    "    def q_posterior(self, x_start, x_t, t):\n",
    "        \"\"\"\n",
    "        Compute the mean and variance of the diffusion posterior:\n",
    "            q(x_{t-1} | x_t, x_0)\n",
    "        \"\"\"\n",
    "        posterior_mean = (\n",
    "            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start\n",
    "            + extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = extract(\n",
    "            self.posterior_log_variance_clipped, t, x_t.shape\n",
    "        )\n",
    "        return (\n",
    "            posterior_mean,\n",
    "            posterior_variance,\n",
    "            posterior_log_variance_clipped,\n",
    "        )\n",
    "\n",
    "    def p_mean_variance(self, x, t, clip_denoised: bool, return_x0=False):\n",
    "        \"\"\"\n",
    "        Apply the model to get p(x_{t-1} | x_t)\n",
    "        :param model: the model, which takes a signal and a batch of timesteps\n",
    "                      as input.\n",
    "        :param x: the [N x C x ...] tensor at time t.\n",
    "        :param t: a 1-D Tensor of timesteps.\n",
    "        :param clip_denoised: if True, clip the denoised signal into [-1, 1].\n",
    "        \"\"\"\n",
    "        model_out = self.model(x, t)\n",
    "        if self.parameterization == \"eps\":\n",
    "            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n",
    "        elif self.parameterization == \"x0\":\n",
    "            x_recon = model_out\n",
    "\n",
    "        if clip_denoised:\n",
    "            x_recon.clamp_(-1.0, 1.0)\n",
    "\n",
    "        (\n",
    "            model_mean,\n",
    "            posterior_variance,\n",
    "            posterior_log_variance,\n",
    "        ) = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n",
    "        if return_x0:\n",
    "            return (\n",
    "                model_mean,\n",
    "                posterior_variance,\n",
    "                posterior_log_variance,\n",
    "                x_recon,\n",
    "            )\n",
    "        else:\n",
    "            return model_mean, posterior_variance, posterior_log_variance\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(\n",
    "        self,\n",
    "        x,\n",
    "        t,\n",
    "        clip_denoised=True,\n",
    "        repeat_noise=False,\n",
    "        return_x0=False,\n",
    "        temperature=1.0,\n",
    "        noise_dropout=0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sample x_{t-1} from the model at the given timestep.\n",
    "        :param x: the current tensor at x_{t-1}.\n",
    "        :param t: the value of t, starting at 0 for the first diffusion step.\n",
    "        :param clip_denoised: if True, clip the x_start prediction to [-1, 1].\n",
    "        \"\"\"\n",
    "\n",
    "        b, *_, device = *x.shape, x.device\n",
    "        outputs = self.p_mean_variance(\n",
    "            x=x,\n",
    "            t=t,\n",
    "            clip_denoised=clip_denoised,\n",
    "            return_x0=return_x0,\n",
    "        )\n",
    "        if return_x0:\n",
    "            model_mean, _, model_log_variance, x0 = outputs\n",
    "        else:\n",
    "            model_mean, _, model_log_variance = outputs\n",
    "\n",
    "        noise = noise_like(x.shape, device, repeat_noise) * temperature\n",
    "        if noise_dropout > 0.0:\n",
    "            noise = torch.nn.functional.dropout(noise, p=noise_dropout)\n",
    "        # no noise when t == 0\n",
    "        nonzero_mask = (1 - (t == 0).float()).reshape(\n",
    "            b, *((1,) * (len(x.shape) - 1))\n",
    "        )\n",
    "        if return_x0:\n",
    "            return (\n",
    "                model_mean\n",
    "                + nonzero_mask * (0.5 * model_log_variance).exp() * noise,\n",
    "                x0,\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                model_mean\n",
    "                + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n",
    "            )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, shape, return_intermediates=False):\n",
    "        device = self.betas.device\n",
    "\n",
    "        b = shape[0]\n",
    "        img = torch.randn(shape, device=device)\n",
    "        intermediates = [img]\n",
    "\n",
    "        for i in tqdm(\n",
    "            reversed(range(0, self.num_timesteps)),\n",
    "            desc=\"sampling loop time step\",\n",
    "            total=self.num_timesteps,\n",
    "        ):\n",
    "            img = self.p_sample(\n",
    "                img,\n",
    "                torch.full((b,), i, device=device, dtype=torch.long),\n",
    "                clip_denoised=self.clip_denoised,\n",
    "            )\n",
    "            if i % self.log_every_t == 0 or i == self.num_timesteps - 1:\n",
    "                intermediates.append(img)\n",
    "        if return_intermediates:\n",
    "            return img, intermediates\n",
    "        return img\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size=16, return_intermediates=False):\n",
    "        image_size = self.image_size\n",
    "        channels = self.channels\n",
    "        return self.p_sample_loop(\n",
    "            (batch_size, channels, image_size, image_size),\n",
    "            return_intermediates=return_intermediates,\n",
    "        )\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        \"\"\"\n",
    "        Diffuse the data for a given number of diffusion steps.\n",
    "        In other words, sample from q(x_t | x_0).\n",
    "        :param x_start: the initial data batch.\n",
    "        :param t: the number of diffusion steps (minus 1). Here,\n",
    "        0 means one step.\n",
    "        :param noise: if specified, the split-out normal noise.\n",
    "        :return: A noisy version of x_start.\n",
    "        \"\"\"\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "\n",
    "        return (\n",
    "            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
    "            + extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "            * noise\n",
    "        )\n",
    "\n",
    "    def get_loss(self, pred, target, mean=True):\n",
    "        if self.loss_type == \"l1\":\n",
    "            loss = (target - pred).abs()\n",
    "            if mean:\n",
    "                loss = loss.mean()\n",
    "        elif self.loss_type == \"l2\":\n",
    "            if mean:\n",
    "                loss = torch.nn.functional.mse_loss(target, pred)\n",
    "            else:\n",
    "                loss = torch.nn.functional.mse_loss(\n",
    "                    target, pred, reduction=\"none\"\n",
    "                )\n",
    "        else:\n",
    "            raise NotImplementedError(\"unknown loss type '{loss_type}'\")\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def p_losses(self, x_start, t, noise=None):\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n",
    "        model_output = self.model(x_noisy, t)\n",
    "\n",
    "        loss_dict = {}\n",
    "        if self.parameterization == \"eps\":\n",
    "            target = noise\n",
    "        elif self.parameterization == \"x0\":\n",
    "            target = x_start\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                f\"Paramterization {self.parameterization} not yet supported\"\n",
    "            )\n",
    "\n",
    "        loss_simple = self.get_loss(model_output, target, mean=False).mean(\n",
    "            dim=[1, 2]\n",
    "        )\n",
    "        loss_dict.update({f\"loss_simple\": loss_simple.mean()})\n",
    "\n",
    "        logvar_t = self.logvar[t].to(x_start.device)\n",
    "        loss = loss_simple / torch.exp(logvar_t) + logvar_t\n",
    "        if self.learn_logvar:\n",
    "            loss_dict.update({f\"loss_gamma\": loss.mean()})\n",
    "            loss_dict.update({\"logvar\": self.logvar.data.mean()})\n",
    "\n",
    "        loss = self.l_simple_weight * loss.mean()\n",
    "\n",
    "        loss_vlb = self.get_loss(model_output, target, mean=False).mean(\n",
    "            dim=(1, 2)\n",
    "        )\n",
    "        loss_vlb = (self.lvlb_weights[t] * loss_vlb).mean()\n",
    "        loss_dict.update({f\"loss_vlb\": loss_vlb})\n",
    "        loss += self.original_elbo_weight * loss_vlb\n",
    "        loss_dict.update({f\"loss\": loss})\n",
    "\n",
    "        return loss, loss_dict\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        t = torch.randint(\n",
    "            0, self.num_timesteps, (x.shape[0],), device=x.device\n",
    "        ).long()\n",
    "        return self.p_losses(x, t, *args, **kwargs)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        self.learning_rate = 1e-4\n",
    "        lr = self.learning_rate\n",
    "        params = list(self.model.parameters())\n",
    "        if self.learn_logvar:\n",
    "            print(\"Diffusion model optimizing logvar\")\n",
    "            params.append(self.logvar)\n",
    "        opt = torch.optim.AdamW(params, lr=lr)\n",
    "        return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "992e1c2b-e48f-4e58-ba61-e0263a8ed99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_4276\\686431995.py:415: UserWarning: Using a target size (torch.Size([32, 10, 4])) that is different to the input size (torch.Size([32, 10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = torch.nn.functional.mse_loss(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.9452\n",
      "Epoch 2/30, Loss: 1.0197\n",
      "Epoch 3/30, Loss: 0.9883\n",
      "Epoch 4/30, Loss: 1.0195\n",
      "Epoch 5/30, Loss: 0.9645\n",
      "Epoch 6/30, Loss: 0.9644\n",
      "Epoch 7/30, Loss: 1.0080\n",
      "Epoch 8/30, Loss: 1.0560\n",
      "Epoch 9/30, Loss: 1.0230\n",
      "Epoch 10/30, Loss: 1.0039\n",
      "Epoch 11/30, Loss: 1.0229\n",
      "Epoch 12/30, Loss: 1.0023\n",
      "Epoch 13/30, Loss: 1.0011\n",
      "Epoch 14/30, Loss: 0.9700\n",
      "Epoch 15/30, Loss: 1.0436\n",
      "Epoch 16/30, Loss: 0.9984\n",
      "Epoch 17/30, Loss: 1.0547\n",
      "Epoch 18/30, Loss: 0.9945\n",
      "Epoch 19/30, Loss: 1.0267\n",
      "Epoch 20/30, Loss: 1.0004\n",
      "Epoch 21/30, Loss: 0.9634\n",
      "Epoch 22/30, Loss: 0.9507\n",
      "Epoch 23/30, Loss: 1.0166\n",
      "Epoch 24/30, Loss: 1.0152\n",
      "Epoch 25/30, Loss: 1.0145\n",
      "Epoch 26/30, Loss: 0.9741\n",
      "Epoch 27/30, Loss: 0.9468\n",
      "Epoch 28/30, Loss: 1.0517\n",
      "Epoch 29/30, Loss: 1.0212\n",
      "Epoch 30/30, Loss: 0.9899\n"
     ]
    }
   ],
   "source": [
    "# DDPM 모델 초기화\n",
    "unet_config = {\n",
    "    \"params\": {\n",
    "        \"in_channels\": 10,\n",
    "        \"model_channels\": 32,\n",
    "        \"out_channels\": 10,\n",
    "        \"num_res_blocks\": 2,\n",
    "        \"attention_resolutions\": [16, 8],\n",
    "        \"dropout\": 0.1,\n",
    "        \"channel_mult\": (2, 4, 8),\n",
    "        \"num_heads\": 4,\n",
    "    }\n",
    "}\n",
    "\n",
    "ddpm = DDPM(unet_config=unet_config)\n",
    "optimizer = ddpm.configure_optimizers()\n",
    "\n",
    "# DDPM 학습\n",
    "num_epochs = 30\n",
    "ddpm.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        x = batch[0].to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "        optimizer.zero_grad()\n",
    "        loss, _ = ddpm(x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "67cb44a8-e2f3-45ae-8c76-6b0084395315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch 데이터셋 생성\n",
    "test_dataset = TensorDataset(test_data_tensor, test_labels_tensor)\n",
    "\n",
    "# 데이터 로더 생성 (옵션)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5d71dffd-9272-4560-a46c-0f2ad5962d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([132, 10, 1])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e422a1e9-50b1-484e-9565-50c402aef7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3a1f3b05-edcc-42a3-9499-02db6d7089ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.0966\n",
      "Recall: 0.2500\n",
      "F1 Score: 0.1393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_4276\\686431995.py:415: UserWarning: Using a target size (torch.Size([32, 10, 4])) that is different to the input size (torch.Size([32, 10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = torch.nn.functional.mse_loss(\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_4276\\686431995.py:415: UserWarning: Using a target size (torch.Size([4, 10, 4])) that is different to the input size (torch.Size([4, 10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = torch.nn.functional.mse_loss(\n",
      "C:\\ProgramData\\anaconda33\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def evaluate_anomaly_detection(anomaly_df):\n",
    "    anomaly_scores = anomaly_df[\"Reconstruction_Error\"].values\n",
    "    true_labels = anomaly_df[\"True_Label\"].values\n",
    "    \n",
    "    # 클래스 수 확인\n",
    "    n_classes = len(np.unique(true_labels))\n",
    "    \n",
    "    # Precision, Recall, F1 Score 계산\n",
    "    # 이상치 탐지에서는 일반적으로 임계값을 설정하여 이진 분류로 변환\n",
    "    predicted_labels = (anomaly_scores < threshold).astype(int)\n",
    "    \n",
    "    # 다중 클래스 문제인 경우 average 매개변수 추가\n",
    "\n",
    "    precision = precision_score(true_labels, predicted_labels, average='macro')  # 또는 'micro', 'weighted'\n",
    "    recall = recall_score(true_labels, predicted_labels, average='macro')\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "    \n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# 테스트 및 평가 실행\n",
    "anomaly_df = detect_anomalies(ddpm, test_loader)\n",
    "evaluate_anomaly_detection(anomaly_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "350157d2-3793-4cb5-bc24-8a2d0a15acdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True_Label\n",
       "1.0    51\n",
       "0.0    48\n",
       "2.0    21\n",
       "3.0    12\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomaly_df['True_Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7577b3d3-744a-4763-8855-a3d4cb814830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_4276\\3894329864.py:9: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.boxplot(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_reconstruction_error_by_label(anomaly_df):\n",
    "    \"\"\"\n",
    "    True_Label에 따른 Reconstruction_Error의 박스플롯을 그립니다.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(\n",
    "        x=\"True_Label\",  # x축: True_Label\n",
    "        y=\"Reconstruction_Error\",  # y축: Reconstruction_Error\n",
    "        data=anomaly_df,  # 데이터프레임\n",
    "        palette=\"Set2\"  # 색상 팔레트\n",
    "    )\n",
    "    plt.title(\"Reconstruction Error by True Label\", fontsize=16)\n",
    "    plt.xlabel(\"True Label\", fontsize=14)\n",
    "    plt.ylabel(\"Reconstruction Error\", fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 박스플롯 그리기\n",
    "plot_reconstruction_error_by_label(anomaly_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884038fa-5c64-43cf-898c-55810ea78c94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
