{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3Kr_oY14WmWc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, feats_in: int, feats_model: int, embedding_type: str = 'fixed', frequency: str = 'h', dropout: float = 0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "        self.value_embedding = nn.Linear(feats_in, feats_model)  # [1] -> [d_model]\n",
    "        self.position_embedding = PositionalEmbedding(feats_model=feats_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(feats_model=feats_model, embedding_type=embedding_type, frequency=frequency)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, x_mark: torch.Tensor) -> torch.Tensor:\n",
    "        # x shape: [B, T, 1]\n",
    "        x_emb = self.value_embedding(x)  # [B, T, 1] -> [B, T, d_model]\n",
    "        x_emb = x_emb + self.position_embedding(x)  # [B, T, d_model]\n",
    "\n",
    "        if x_mark is not None:\n",
    "            x_emb += self.temporal_embedding(x_mark)\n",
    "\n",
    "        return self.dropout(x_emb)\n",
    "\n",
    "class TemporalEmbedding(nn.Module):\n",
    "    def __init__(self, feats_model: int, embedding_type: str = 'timeF', frequency: str = 'h'):\n",
    "        super(TemporalEmbedding, self).__init__()\n",
    "\n",
    "        # 시간 주기별 임베딩 차원\n",
    "        Embed = nn.Embedding\n",
    "\n",
    "        # 시간 주기 구성: 시간 단위에 따라 다르게 처리\n",
    "        if frequency == 'h':\n",
    "            self.minute_embed = None\n",
    "            self.hour_embed = Embed(24, feats_model)       # 0~23\n",
    "            self.weekday_embed = Embed(7, feats_model)     # 0~6\n",
    "            self.day_embed = Embed(32, feats_model)        # 1~31\n",
    "            self.month_embed = Embed(13, feats_model)      # 1~12\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unsupported frequency: {frequency}\")\n",
    "\n",
    "    def forward(self, x_mark: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_mark : Tensor of shape [B, T, 4] → columns: hour, weekday, day, month\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor of shape [B, T, d_model]\n",
    "        \"\"\"\n",
    "        hour_x = self.hour_embed(x_mark[:, :, 0].long())\n",
    "        weekday_x = self.weekday_embed(x_mark[:, :, 1].long())\n",
    "        day_x = self.day_embed(x_mark[:, :, 2].long())\n",
    "        month_x = self.month_embed(x_mark[:, :, 3].long())\n",
    "\n",
    "        return hour_x + weekday_x + day_x + month_x\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, feats_model: int, max_len: int = 5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "\n",
    "        # 위치 인덱스 (0, 1, 2, ..., max_len - 1)\n",
    "        pe = torch.zeros(max_len, feats_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, feats_model, 2).float() * (-np.log(10000.0) / feats_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Tensor of shape [batch_size, seq_len, feats_model]\n",
    "        \"\"\"\n",
    "        return self.pe[:, :x.size(1), :]\n",
    "\n",
    "\n",
    "class Inception_Block_V1(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_kernels=6, init_weight=True):\n",
    "        super(Inception_Block_V1, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_kernels = num_kernels\n",
    "        kernels = []\n",
    "        for i in range(self.num_kernels):\n",
    "            kernels.append(nn.Conv2d(in_channels, out_channels, kernel_size=2 * i + 1, padding=i))\n",
    "        self.kernels = nn.ModuleList(kernels)\n",
    "        if init_weight:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res_list = []\n",
    "        for i in range(self.num_kernels):\n",
    "            res_list.append(self.kernels[i](x))\n",
    "        res = torch.stack(res_list, dim=-1).mean(-1)\n",
    "        return res\n",
    "\n",
    "\n",
    "class Inception_Block_V2(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_kernels=6, init_weight=True):\n",
    "        super(Inception_Block_V2, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_kernels = num_kernels\n",
    "        kernels = []\n",
    "        for i in range(self.num_kernels // 2):\n",
    "            kernels.append(nn.Conv2d(in_channels, out_channels, kernel_size=[1, 2 * i + 3], padding=[0, i + 1]))\n",
    "            kernels.append(nn.Conv2d(in_channels, out_channels, kernel_size=[2 * i + 3, 1], padding=[i + 1, 0]))\n",
    "        kernels.append(nn.Conv2d(in_channels, out_channels, kernel_size=1))\n",
    "        self.kernels = nn.ModuleList(kernels)\n",
    "        if init_weight:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res_list = []\n",
    "        for i in range(self.num_kernels + 1):\n",
    "            res_list.append(self.kernels[i](x))\n",
    "        res = torch.stack(res_list, dim=-1).mean(-1)\n",
    "        return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43642a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ShallowPPGEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=1, cnn_channels=64, kernel_size=15):\n",
    "        super().__init__()\n",
    "        self.temporal_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, cnn_channels, kernel_size, padding=kernel_size//2),\n",
    "            nn.BatchNorm1d(cnn_channels),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.temporal_conv(x)  # [B, C, T//2]\n",
    "        x = x.permute(0, 2, 1)     # [B, T//2, C]\n",
    "        return x\n",
    "\n",
    "class FineTemporalBranch(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(d_model, d_model, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(d_model),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # [B, C, T]\n",
    "        x = self.conv(x)\n",
    "        x = x.permute(0, 2, 1)  # [B, T', C]\n",
    "        return x\n",
    "\n",
    "class CoarseTransformerBranch(nn.Module):\n",
    "    def __init__(self, d_model, nhead):\n",
    "        super().__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class IPUnit(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        power = torch.mean(x ** 2, dim=1)\n",
    "        return torch.log1p(power)\n",
    "\n",
    "class PPGDeformer(nn.Module):\n",
    "    def __init__(self, in_channels=1, cnn_channels=64, d_model=64, nhead=4, output_dim=2):\n",
    "        super().__init__()\n",
    "        self.encoder = ShallowPPGEncoder(in_channels, cnn_channels)\n",
    "        self.fine_branch = FineTemporalBranch(d_model)\n",
    "        self.coarse_branch = CoarseTransformerBranch(d_model, nhead)\n",
    "        self.ip_unit = IPUnit()\n",
    "        self.regressor = nn.Linear(d_model + d_model, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tokens = self.encoder(x)        # [B, T', C]\n",
    "        fine_feat = self.fine_branch(tokens)        # [B, T'', C]\n",
    "        coarse_feat = self.coarse_branch(tokens)   # [B, T', C]\n",
    "\n",
    "        fine_summary = self.ip_unit(fine_feat)     # [B, C]\n",
    "        coarse_summary = torch.mean(coarse_feat, dim=1)  # [B, C]\n",
    "\n",
    "        combined = torch.cat([fine_summary, coarse_summary], dim=-1)\n",
    "        out = self.regressor(combined)  # [B, output_dim]\n",
    "        return out\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
