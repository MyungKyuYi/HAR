{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from abc import abstractmethod\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from einops import repeat\n",
        "from abc import ABC, abstractmethod\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "from abc import ABC, abstractmethod\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 데이터 로드\n",
        "file_path = \"/content/drive/MyDrive/Colab Notebooks/combined_dataset.xlsx\"\n",
        "df = pd.read_excel(file_path)  # read_csv -> read_excel로 변경\n",
        "df\n",
        "\n",
        "df['Hypertension'].value_counts()\n",
        "\n",
        "# Normal을 8:2로 나눔 (훈련: 80%, 테스트: 20%)\n",
        "normal_train, normal_test = train_test_split(\n",
        "    df[df[\"Hypertension\"] == \"Normal\"], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 나머지 클래스는 테스트 데이터에만 포함\n",
        "test_data = normal_test.copy()\n",
        "for category in [\"Prehypertension\", \"Stage 1 hypertension\", \"Stage 2 hypertension\"]:\n",
        "    _, test = train_test_split(\n",
        "        df[df[\"Hypertension\"] == category], test_size=0.2, random_state=42\n",
        "    )\n",
        "    test_data = pd.concat([test_data, test])\n",
        "\n",
        "# 훈련 데이터는 Normal만 포함\n",
        "train_data = normal_train.copy()\n",
        "train_data[\"Hypertension\"].value_counts()\n",
        "test_data[\"Hypertension\"].value_counts()\n",
        "\n",
        "ppg_columns = [str(i) for i in range(2091, 2101)]\n",
        "\n",
        "# 'Hypertension'이 0 (Normal)인 데이터만 훈련 데이터로 선택\n",
        "train_data = train_data[train_data[\"Hypertension\"] == 'Normal'][ppg_columns]\n",
        "\n",
        "\n",
        "# 테스트 데이터는 기존과 동일하게 유지 (Hypertension 레이블 + PPG 데이터)\n",
        "test_data = test_data[[\"Hypertension\"] + ppg_columns]\n",
        "\n",
        "test_data['Hypertension'].value_counts()\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Label Encoding을 위한 변환기 생성\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Hypertension 컬럼을 Label Encoding 적용\n",
        "test_data[\"Hypertension_Encoded\"] = label_encoder.fit_transform(test_data[\"Hypertension\"])\n",
        "\n",
        "# 기존 Hypertension 컬럼 제거 후 새로운 컬럼으로 대체\n",
        "test_data_encoded = test_data.drop(columns=[\"Hypertension\"]).rename(columns={\"Hypertension_Encoded\": \"Hypertension\"})\n",
        "test_data_encoded['Hypertension'].value_counts()\n",
        "\n",
        "\n",
        "batch_size= 32\n",
        "\n",
        "# TensorDataset 생성\n",
        "train_data_tensor = torch.tensor(train_data.values, dtype=torch.float32).unsqueeze(-1)\n",
        "train_dataset = TensorDataset(train_data_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "test_data_tensor = torch.tensor(test_data_encoded.drop(columns=[\"Hypertension\"]).values, dtype=torch.float32).unsqueeze(-1)\n",
        "\n",
        "# 다시 PyTorch Tensor로 변환 및 차원 추가\n",
        "test_data_tensor = torch.tensor(test_data_tensor, dtype=torch.float32).unsqueeze(-1)\n",
        "test_labels_tensor = torch.tensor(test_data_encoded[\"Hypertension\"].values, dtype=torch.float32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smBL6yzA2b7v",
        "outputId": "b8733065-e7e4-46d0-c520-63c12d822d7a"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-67-dfffdc033ab6>:123: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_data_tensor = torch.tensor(test_data_tensor, dtype=torch.float32).unsqueeze(-1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "QhSP-j7tko9g"
      },
      "outputs": [],
      "source": [
        "def timestep_embedding(timesteps, dim, max_period=10000, repeat_only=False):\n",
        "    \"\"\"\n",
        "    Create sinusoidal timestep embeddings.\n",
        "    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n",
        "                      These may be fractional.\n",
        "    :param dim: the dimension of the output.\n",
        "    :param max_period: controls the minimum frequency of the embeddings.\n",
        "    :return: an [N x dim] Tensor of positional embeddings.\n",
        "    \"\"\"\n",
        "    if not repeat_only:\n",
        "        half = dim // 2\n",
        "        freqs = torch.exp(\n",
        "            -math.log(max_period)\n",
        "            * torch.arange(start=0, end=half, dtype=torch.float32)\n",
        "            / half\n",
        "        ).to(device=timesteps.device)\n",
        "        args = timesteps[:, None].float() * freqs[None]\n",
        "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
        "        if dim % 2:\n",
        "            embedding = torch.cat(\n",
        "                [embedding, torch.zeros_like(embedding[:, :1])], dim=-1\n",
        "            )\n",
        "    else:\n",
        "        embedding = repeat(timesteps, \"b -> b d\", d=dim)\n",
        "    return embedding\n",
        "\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"\n",
        "    Zero out the parameters of a module and return it.\n",
        "    \"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "\n",
        "class TimestepBlock(nn.Module):\n",
        "    @abstractmethod\n",
        "    def forward(self, x, emb):\n",
        "        \"\"\"\n",
        "        Apply the module to `x` given `emb` timestep embeddings.\n",
        "        \"\"\"\n",
        "\n",
        "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n",
        "    \"\"\"\n",
        "    A sequential module that passes timestep embeddings to the children that\n",
        "    support it as an extra input.\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, x, emb, context=None):\n",
        "        for layer in self:\n",
        "            if isinstance(layer, TimestepBlock):\n",
        "                x = layer(x, emb)  # Pass emb to TimestepBlock layers\n",
        "            else:\n",
        "                x = layer(x)  # Regular layers do not receive emb\n",
        "        return x\n",
        "\n",
        "def Normalize(in_channels):\n",
        "    return nn.GroupNorm(\n",
        "        num_groups=32, num_channels=in_channels, eps=1e-6, affine=True\n",
        "    )\n",
        "\n",
        "\n",
        "def count_flops_attn(model, _x, y):\n",
        "    \"\"\"\n",
        "    A counter for the `thop` package to count the operations in an\n",
        "    attention operation.\n",
        "    Meant to be used like:\n",
        "        macs, params = thop.profile(\n",
        "            model,\n",
        "            inputs=(inputs, timestamps),\n",
        "            custom_ops={QKVAttention: QKVAttention.count_flops},\n",
        "        )\n",
        "    \"\"\"\n",
        "    b, c, *spatial = y[0].shape\n",
        "    num_spatial = int(np.prod(spatial))\n",
        "    # We perform two matmuls with the same number of ops.\n",
        "    # The first computes the weight matrix, the second computes\n",
        "    # the combination of the value vectors.\n",
        "    matmul_ops = 2 * b * (num_spatial**2) * c\n",
        "    model.total_ops += th.DoubleTensor([matmul_ops])\n",
        "\n",
        "\n",
        "class QKVAttentionLegacy(nn.Module):\n",
        "    \"\"\"\n",
        "    A module which performs QKV attention.\n",
        "    Matches legacy QKVAttention + input/ouput heads shaping\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_heads):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, qkv):\n",
        "        \"\"\"\n",
        "        Apply QKV attention.\n",
        "        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n",
        "        :return: an [N x (H * C) x T] tensor after attention.\n",
        "        \"\"\"\n",
        "        bs, width, length = qkv.shape\n",
        "        assert width % (3 * self.n_heads) == 0\n",
        "        ch = width // (3 * self.n_heads)\n",
        "        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(\n",
        "            ch, dim=1\n",
        "        )\n",
        "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
        "        weight = th.einsum(\n",
        "            \"bct,bcs->bts\", q * scale, k * scale\n",
        "        )  # More stable with f16 than dividing afterwards\n",
        "        weight = th.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
        "        a = th.einsum(\"bts,bcs->bct\", weight, v)\n",
        "        return a.reshape(bs, -1, length)\n",
        "\n",
        "    @staticmethod\n",
        "    def count_flops(model, _x, y):\n",
        "        return count_flops_attn(model, _x, y)\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    An attention block that allows spatial positions to attend to each other.\n",
        "    Originally ported from here, but adapted to the N-d case.\n",
        "    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels,\n",
        "        num_heads=1,\n",
        "        num_head_channels=-1,\n",
        "        use_checkpoint=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        if num_head_channels == -1:\n",
        "            self.num_heads = num_heads\n",
        "        else:\n",
        "            assert channels % num_head_channels == 0, (\n",
        "                f\"q,k,v channels {channels} is \"\n",
        "                f\"not divisible by num_head_channels {num_head_channels}\"\n",
        "            )\n",
        "            self.num_heads = channels // num_head_channels\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.norm = Normalize(channels)\n",
        "        self.qkv = nn.Conv1d(channels, channels * 3, 1)\n",
        "        self.attention = QKVAttentionLegacy(self.num_heads)\n",
        "\n",
        "        self.proj_out = zero_module(nn.Conv1d(channels, channels, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._forward(\n",
        "            x,\n",
        "        )\n",
        "\n",
        "    def _forward(self, x):\n",
        "        b, c, *spatial = x.shape\n",
        "        x = x.reshape(b, c, -1)\n",
        "        qkv = self.qkv(self.norm(x))\n",
        "        h = self.attention(qkv)\n",
        "        h = self.proj_out(h)\n",
        "        return (x + h).reshape(b, c, *spatial)\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    \"\"\"\n",
        "    A downsampling layer with an optional convolution.\n",
        "    :param channels: channels in the inputs and outputs.\n",
        "    :param use_conv: a bool determining if a convolution is applied.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, use_conv, out_channels=None, padding=1):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        if use_conv:\n",
        "            self.op = nn.Conv1d(\n",
        "                self.channels, self.out_channels, 3, stride=2, padding=padding\n",
        "            )#TODO:Mudar\n",
        "        else:\n",
        "            assert self.channels == self.out_channels\n",
        "            self.op = nn.AvgPool1d(kernel_size=2, stride=2)#TODO: Mudar\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.shape[1] == self.channels\n",
        "        return self.op(x)\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    \"\"\"\n",
        "    An upsampling layer with an optional convolution.\n",
        "    :param channels: channels in the inputs and outputs.\n",
        "    :param use_conv: a bool determining if a convolution is applied.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, use_conv, out_channels=None, padding=1):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        if use_conv:\n",
        "            self.conv = nn.Conv1d(\n",
        "                self.channels, self.out_channels, 3, padding=padding\n",
        "            )#TODO:Mudar\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.shape[1] == self.channels\n",
        "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
        "        if self.use_conv:\n",
        "            x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResBlock(TimestepBlock):  # Ensure ResBlock inherits from TimestepBlock\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels,\n",
        "        emb_channels,\n",
        "        dropout,\n",
        "        out_channels=None,\n",
        "        use_conv=False,\n",
        "        use_scale_shift_norm=False,\n",
        "        up=False,\n",
        "        down=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.emb_channels = emb_channels\n",
        "        self.dropout = dropout\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.use_scale_shift_norm = use_scale_shift_norm\n",
        "\n",
        "        self.in_layers = nn.Sequential(\n",
        "            Normalize(channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv1d(channels, self.out_channels, 3, padding=1),\n",
        "        )\n",
        "\n",
        "        self.updown = up or down\n",
        "\n",
        "        if up:\n",
        "            self.h_upd = Upsample(channels, False)\n",
        "            self.x_upd = Upsample(channels, False)\n",
        "        elif down:\n",
        "            self.h_upd = Downsample(channels, False)\n",
        "            self.x_upd = Downsample(channels, False)\n",
        "        else:\n",
        "            self.h_upd = self.x_upd = nn.Identity()\n",
        "\n",
        "        self.emb_layers = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(\n",
        "                emb_channels,\n",
        "                2 * self.out_channels\n",
        "                if use_scale_shift_norm\n",
        "                else self.out_channels,\n",
        "            ),\n",
        "        )\n",
        "        self.out_layers = nn.Sequential(\n",
        "            Normalize(self.out_channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            zero_module(\n",
        "                nn.Conv1d(self.out_channels, self.out_channels, 3, padding=1)\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        if self.out_channels == channels:\n",
        "            self.skip_connection = nn.Identity()\n",
        "        elif use_conv:\n",
        "            self.skip_connection = nn.Conv1d(\n",
        "                channels, self.out_channels, kernel_size=1\n",
        "            )\n",
        "        else:\n",
        "            self.skip_connection = nn.Conv1d(channels, self.out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x, emb):\n",
        "        \"\"\"\n",
        "        Apply the ResBlock to `x` with timestep embeddings `emb`.\n",
        "        \"\"\"\n",
        "        if self.updown:\n",
        "            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n",
        "            h = in_rest(x)\n",
        "            h = self.h_upd(h)\n",
        "            x = self.x_upd(x)\n",
        "            h = in_conv(h)\n",
        "        else:\n",
        "            h = self.in_layers(x)\n",
        "\n",
        "        emb_out = self.emb_layers(emb).type(h.dtype)\n",
        "        while len(emb_out.shape) < len(h.shape):\n",
        "            emb_out = emb_out[..., None]\n",
        "\n",
        "        if self.use_scale_shift_norm:\n",
        "            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n",
        "            scale, shift = torch.chunk(emb_out, 2, dim=1)\n",
        "            h = out_norm(h) * (1 + scale) + shift\n",
        "            h = out_rest(h)\n",
        "        else:\n",
        "            h = h + emb_out\n",
        "            h = self.out_layers(h)\n",
        "\n",
        "        return self.skip_connection(x) + h\n",
        "class UNetModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size=32,\n",
        "        in_channels=1,\n",
        "        model_channels=32,\n",
        "        out_channels=1,\n",
        "        num_res_blocks=2,\n",
        "        attention_resolutions=[16, 8],\n",
        "        dropout=0.1,\n",
        "        channel_mult=(2, 4, 8),\n",
        "        num_heads=4,\n",
        "        use_scale_shift_norm=False,\n",
        "        resblock_updown=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.image_size = image_size\n",
        "        self.in_channels = in_channels\n",
        "        self.model_channels = model_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.attention_resolutions = attention_resolutions\n",
        "        self.dropout = dropout\n",
        "        self.channel_mult = channel_mult\n",
        "        self.num_heads = num_heads\n",
        "        self.use_scale_shift_norm = use_scale_shift_norm\n",
        "        self.resblock_updown = resblock_updown\n",
        "\n",
        "        time_embed_dim = model_channels * 4\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Linear(model_channels, time_embed_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_embed_dim, time_embed_dim),\n",
        "        )\n",
        "\n",
        "        self.input_blocks = nn.ModuleList([\n",
        "            TimestepEmbedSequential(nn.Conv1d(in_channels, model_channels, 3, padding=1))\n",
        "        ])\n",
        "        input_block_chans = [model_channels]\n",
        "        ch = model_channels\n",
        "        ds = 1\n",
        "\n",
        "        for level, mult in enumerate(channel_mult):\n",
        "            for _ in range(num_res_blocks):\n",
        "                layers = [ResBlock(ch, time_embed_dim, dropout, out_channels=mult * model_channels, use_scale_shift_norm=use_scale_shift_norm)]\n",
        "                ch = mult * model_channels\n",
        "                if ds in attention_resolutions:\n",
        "                    layers.append(AttentionBlock(ch, num_heads=num_heads))\n",
        "                self.input_blocks.append(TimestepEmbedSequential(*layers))\n",
        "                input_block_chans.append(ch)\n",
        "            if level != len(channel_mult) - 1:\n",
        "                out_ch = ch\n",
        "                self.input_blocks.append(TimestepEmbedSequential(Downsample(ch, True, out_channels=out_ch)))\n",
        "                ch = out_ch\n",
        "                input_block_chans.append(ch)\n",
        "                ds *= 2\n",
        "\n",
        "        self.middle_block = TimestepEmbedSequential(\n",
        "            ResBlock(ch, time_embed_dim, dropout, use_scale_shift_norm=use_scale_shift_norm),\n",
        "            AttentionBlock(ch, num_heads=num_heads),\n",
        "            ResBlock(ch, time_embed_dim, dropout, use_scale_shift_norm=use_scale_shift_norm),\n",
        "        )\n",
        "\n",
        "        self.output_blocks = nn.ModuleList([])\n",
        "        for level, mult in list(enumerate(channel_mult))[::-1]:\n",
        "            for i in range(num_res_blocks + 1):\n",
        "                ich = input_block_chans.pop()\n",
        "                layers = [ResBlock(ch + ich, time_embed_dim, dropout, out_channels=model_channels * mult, use_scale_shift_norm=use_scale_shift_norm)]\n",
        "                ch = model_channels * mult\n",
        "                if ds in attention_resolutions:\n",
        "                    layers.append(AttentionBlock(ch, num_heads=num_heads))\n",
        "                if level and i == num_res_blocks:\n",
        "                    out_ch = ch\n",
        "                    layers.append(Upsample(ch, True, out_channels=out_ch))\n",
        "                    ds //= 2\n",
        "                self.output_blocks.append(TimestepEmbedSequential(*layers))\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            Normalize(ch),\n",
        "            nn.SiLU(),\n",
        "            zero_module(nn.Conv1d(ch, out_channels, 3, padding=1)),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, timesteps=None, context=None, y=None):\n",
        "        assert timesteps is not None, \"timesteps must be provided\"\n",
        "        hs = []\n",
        "        t_emb = timestep_embedding(timesteps, self.model_channels, repeat_only=False)\n",
        "        emb = self.time_embed(t_emb)\n",
        "\n",
        "        h = x\n",
        "        for module in self.input_blocks:\n",
        "            h = module(h, emb, context)\n",
        "            hs.append(h)\n",
        "        h = self.middle_block(h, emb, context)\n",
        "\n",
        "        for module in self.output_blocks:\n",
        "            h_pop = hs.pop()\n",
        "            if h.shape[2] != h_pop.shape[2]:\n",
        "                h_pop = F.interpolate(h_pop, size=h.shape[2], mode='nearest')\n",
        "            h = torch.cat([h, h_pop], dim=1)\n",
        "            h = module(h, emb, context)\n",
        "\n",
        "        return self.out(h)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "def timestep_embedding(timesteps, dim, max_period=10000, repeat_only=False):\n",
        "    \"\"\"\n",
        "    Create sinusoidal timestep embeddings.\n",
        "    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n",
        "                      These may be fractional.\n",
        "    :param dim: the dimension of the output.\n",
        "    :param max_period: controls the minimum frequency of the embeddings.\n",
        "    :return: an [N x dim] Tensor of positional embeddings.\n",
        "    \"\"\"\n",
        "    if not repeat_only:\n",
        "        half = dim // 2\n",
        "        freqs = torch.exp(\n",
        "            -math.log(max_period)\n",
        "            * torch.arange(start=0, end=half, dtype=torch.float32)\n",
        "            / half\n",
        "        ).to(device=timesteps.device)\n",
        "        args = timesteps[:, None].float() * freqs[None]\n",
        "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
        "        if dim % 2:\n",
        "            embedding = torch.cat(\n",
        "                [embedding, torch.zeros_like(embedding[:, :1])], dim=-1\n",
        "            )\n",
        "    else:\n",
        "        embedding = repeat(timesteps, \"b -> b d\", d=dim)\n",
        "    return embedding\n",
        "\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"\n",
        "    Zero out the parameters of a module and return it.\n",
        "    \"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "\n",
        "class TimestepBlock(nn.Module):\n",
        "    @abstractmethod\n",
        "    def forward(self, x, emb):\n",
        "        \"\"\"\n",
        "        Apply the module to `x` given `emb` timestep embeddings.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n",
        "    \"\"\"\n",
        "    A sequential module that passes timestep embeddings to the children that\n",
        "    support it as an extra input.\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, x, emb, context=None):\n",
        "        for layer in self:\n",
        "            if isinstance(layer, TimestepBlock):\n",
        "                x = layer(x, emb)  # Pass emb to TimestepBlock layers\n",
        "            else:\n",
        "                x = layer(x)  # Regular layers do not receive emb\n",
        "        return x\n",
        "\n",
        "def Normalize(in_channels):\n",
        "    return nn.GroupNorm(\n",
        "        num_groups=32, num_channels=in_channels, eps=1e-6, affine=True\n",
        "    )\n",
        "\n",
        "\n",
        "def count_flops_attn(model, _x, y):\n",
        "    \"\"\"\n",
        "    A counter for the `thop` package to count the operations in an\n",
        "    attention operation.\n",
        "    Meant to be used like:\n",
        "        macs, params = thop.profile(\n",
        "            model,\n",
        "            inputs=(inputs, timestamps),\n",
        "            custom_ops={QKVAttention: QKVAttention.count_flops},\n",
        "        )\n",
        "    \"\"\"\n",
        "    b, c, *spatial = y[0].shape\n",
        "    num_spatial = int(np.prod(spatial))\n",
        "    # We perform two matmuls with the same number of ops.\n",
        "    # The first computes the weight matrix, the second computes\n",
        "    # the combination of the value vectors.\n",
        "    matmul_ops = 2 * b * (num_spatial**2) * c\n",
        "    model.total_ops += th.DoubleTensor([matmul_ops])\n",
        "\n",
        "\n",
        "class QKVAttentionLegacy(nn.Module):\n",
        "    \"\"\"\n",
        "    A module which performs QKV attention.\n",
        "    Matches legacy QKVAttention + input/ouput heads shaping\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_heads):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def forward(self, qkv):\n",
        "        \"\"\"\n",
        "        Apply QKV attention.\n",
        "        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n",
        "        :return: an [N x (H * C) x T] tensor after attention.\n",
        "        \"\"\"\n",
        "        bs, width, length = qkv.shape\n",
        "        assert width % (3 * self.n_heads) == 0\n",
        "        ch = width // (3 * self.n_heads)\n",
        "        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(\n",
        "            ch, dim=1\n",
        "        )\n",
        "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
        "        weight = th.einsum(\n",
        "            \"bct,bcs->bts\", q * scale, k * scale\n",
        "        )  # More stable with f16 than dividing afterwards\n",
        "        weight = th.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
        "        a = th.einsum(\"bts,bcs->bct\", weight, v)\n",
        "        return a.reshape(bs, -1, length)\n",
        "\n",
        "    @staticmethod\n",
        "    def count_flops(model, _x, y):\n",
        "        return count_flops_attn(model, _x, y)\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    An attention block that allows spatial positions to attend to each other.\n",
        "    Originally ported from here, but adapted to the N-d case.\n",
        "    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels,\n",
        "        num_heads=1,\n",
        "        num_head_channels=-1,\n",
        "        use_checkpoint=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        if num_head_channels == -1:\n",
        "            self.num_heads = num_heads\n",
        "        else:\n",
        "            assert channels % num_head_channels == 0, (\n",
        "                f\"q,k,v channels {channels} is \"\n",
        "                f\"not divisible by num_head_channels {num_head_channels}\"\n",
        "            )\n",
        "            self.num_heads = channels // num_head_channels\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "        self.norm = Normalize(channels)\n",
        "        self.qkv = nn.Conv1d(channels, channels * 3, 1)\n",
        "        self.attention = QKVAttentionLegacy(self.num_heads)\n",
        "\n",
        "        self.proj_out = zero_module(nn.Conv1d(channels, channels, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._forward(\n",
        "            x,\n",
        "        )\n",
        "\n",
        "    def _forward(self, x):\n",
        "        b, c, *spatial = x.shape\n",
        "        x = x.reshape(b, c, -1)\n",
        "        qkv = self.qkv(self.norm(x))\n",
        "        h = self.attention(qkv)\n",
        "        h = self.proj_out(h)\n",
        "        return (x + h).reshape(b, c, *spatial)\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    \"\"\"\n",
        "    A downsampling layer with an optional convolution.\n",
        "    :param channels: channels in the inputs and outputs.\n",
        "    :param use_conv: a bool determining if a convolution is applied.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, use_conv, out_channels=None, padding=1):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        if use_conv:\n",
        "            self.op = nn.Conv1d(\n",
        "                self.channels, self.out_channels, 3, stride=2, padding=padding\n",
        "            )#TODO:Mudar\n",
        "        else:\n",
        "            assert self.channels == self.out_channels\n",
        "            self.op = nn.AvgPool1d(kernel_size=2, stride=2)#TODO: Mudar\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.shape[1] == self.channels\n",
        "        return self.op(x)\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    \"\"\"\n",
        "    An upsampling layer with an optional convolution.\n",
        "    :param channels: channels in the inputs and outputs.\n",
        "    :param use_conv: a bool determining if a convolution is applied.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels, use_conv, out_channels=None, padding=1):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        if use_conv:\n",
        "            self.conv = nn.Conv1d(\n",
        "                self.channels, self.out_channels, 3, padding=padding\n",
        "            )#TODO:Mudar\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert x.shape[1] == self.channels\n",
        "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
        "        if self.use_conv:\n",
        "            x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResBlock(TimestepBlock):  # Ensure ResBlock inherits from TimestepBlock\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels,\n",
        "        emb_channels,\n",
        "        dropout,\n",
        "        out_channels=None,\n",
        "        use_conv=False,\n",
        "        use_scale_shift_norm=False,\n",
        "        up=False,\n",
        "        down=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.emb_channels = emb_channels\n",
        "        self.dropout = dropout\n",
        "        self.out_channels = out_channels or channels\n",
        "        self.use_conv = use_conv\n",
        "        self.use_scale_shift_norm = use_scale_shift_norm\n",
        "\n",
        "        self.in_layers = nn.Sequential(\n",
        "            Normalize(channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv1d(channels, self.out_channels, 3, padding=1),\n",
        "        )\n",
        "\n",
        "        self.updown = up or down\n",
        "\n",
        "        if up:\n",
        "            self.h_upd = Upsample(channels, False)\n",
        "            self.x_upd = Upsample(channels, False)\n",
        "        elif down:\n",
        "            self.h_upd = Downsample(channels, False)\n",
        "            self.x_upd = Downsample(channels, False)\n",
        "        else:\n",
        "            self.h_upd = self.x_upd = nn.Identity()\n",
        "\n",
        "        self.emb_layers = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(\n",
        "                emb_channels,\n",
        "                2 * self.out_channels\n",
        "                if use_scale_shift_norm\n",
        "                else self.out_channels,\n",
        "            ),\n",
        "        )\n",
        "        self.out_layers = nn.Sequential(\n",
        "            Normalize(self.out_channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(p=dropout),\n",
        "            zero_module(\n",
        "                nn.Conv1d(self.out_channels, self.out_channels, 3, padding=1)\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        if self.out_channels == channels:\n",
        "            self.skip_connection = nn.Identity()\n",
        "        elif use_conv:\n",
        "            self.skip_connection = nn.Conv1d(\n",
        "                channels, self.out_channels, kernel_size=1\n",
        "            )\n",
        "        else:\n",
        "            self.skip_connection = nn.Conv1d(channels, self.out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x, emb):\n",
        "        \"\"\"\n",
        "        Apply the ResBlock to `x` with timestep embeddings `emb`.\n",
        "        \"\"\"\n",
        "        if self.updown:\n",
        "            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n",
        "            h = in_rest(x)\n",
        "            h = self.h_upd(h)\n",
        "            x = self.x_upd(x)\n",
        "            h = in_conv(h)\n",
        "        else:\n",
        "            h = self.in_layers(x)\n",
        "\n",
        "        emb_out = self.emb_layers(emb).type(h.dtype)\n",
        "        while len(emb_out.shape) < len(h.shape):\n",
        "            emb_out = emb_out[..., None]\n",
        "\n",
        "        if self.use_scale_shift_norm:\n",
        "            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n",
        "            scale, shift = torch.chunk(emb_out, 2, dim=1)\n",
        "            h = out_norm(h) * (1 + scale) + shift\n",
        "            h = out_rest(h)\n",
        "        else:\n",
        "            h = h + emb_out\n",
        "            h = self.out_layers(h)\n",
        "\n",
        "        return self.skip_connection(x) + h\n",
        "\n",
        "from functools import partial\n",
        "from inspect import isfunction\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "def exists(x):\n",
        "    return x is not None\n",
        "\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if isfunction(d) else d\n",
        "\n",
        "\n",
        "def noise_like(shape, device, repeat=False):\n",
        "    repeat_noise = lambda: torch.randn((1, *shape[1:]), device=device).repeat(\n",
        "        shape[0], *((1,) * (len(shape) - 1))\n",
        "    )\n",
        "    noise = lambda: torch.randn(shape, device=device)\n",
        "    return repeat_noise() if repeat else noise()\n",
        "\n",
        "\n",
        "def extract(a, t, x_shape):\n",
        "    b, *_ = t.shape\n",
        "    out = a.gather(-1, t)\n",
        "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
        "\n",
        "\n",
        "def make_beta_schedule(\n",
        "    schedule, n_timestep, linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3\n",
        "):\n",
        "    if schedule == \"linear\":\n",
        "        betas = (\n",
        "            torch.linspace(\n",
        "                linear_start**0.5,\n",
        "                linear_end**0.5,\n",
        "                n_timestep,\n",
        "                dtype=torch.float64,\n",
        "            )\n",
        "            ** 2\n",
        "        )\n",
        "\n",
        "    elif schedule == \"cosine\":\n",
        "        timesteps = (\n",
        "            torch.arange(n_timestep + 1, dtype=torch.float64) / n_timestep\n",
        "            + cosine_s\n",
        "        )\n",
        "        alphas = timesteps / (1 + cosine_s) * np.pi / 2\n",
        "        alphas = torch.cos(alphas).pow(2)\n",
        "        alphas = alphas / alphas[0]\n",
        "        betas = 1 - alphas[1:] / alphas[:-1]\n",
        "        betas = np.clip(betas, a_min=0, a_max=0.999)\n",
        "\n",
        "    elif schedule == \"sqrt_linear\":\n",
        "        betas = torch.linspace(\n",
        "            linear_start, linear_end, n_timestep, dtype=torch.float64\n",
        "        )\n",
        "    elif schedule == \"sqrt\":\n",
        "        betas = (\n",
        "            torch.linspace(\n",
        "                linear_start, linear_end, n_timestep, dtype=torch.float64\n",
        "            )\n",
        "            ** 0.5\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"schedule '{schedule}' unknown.\")\n",
        "    return betas.numpy()\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from functools import partial\n",
        "\n",
        "def extract(a, t, x_shape):\n",
        "    b, *_ = t.shape\n",
        "    out = a.gather(-1, t)\n",
        "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
        "\n",
        "class DDIM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        unet_config,\n",
        "        timesteps=1000,\n",
        "        ddim_steps=50,\n",
        "        beta_schedule=\"linear\",\n",
        "        clip_denoised=False,\n",
        "        linear_start=1e-4,\n",
        "        linear_end=2e-2,\n",
        "        original_elbo_weight=0.0,\n",
        "        parameterization=\"eps\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert parameterization in [\"eps\", \"x0\"], 'Only \"eps\" and \"x0\" are supported.'\n",
        "        self.parameterization = parameterization\n",
        "        self.model = UNetModel(**unet_config.get(\"params\", {}))\n",
        "        self.clip_denoised = clip_denoised\n",
        "        self.original_elbo_weight = original_elbo_weight\n",
        "        self.ddim_steps = ddim_steps\n",
        "        self.register_schedule(beta_schedule, timesteps, linear_start, linear_end)\n",
        "\n",
        "    def register_schedule(self, beta_schedule, timesteps, linear_start, linear_end):\n",
        "        betas = np.linspace(linear_start, linear_end, timesteps, dtype=np.float64)\n",
        "        alphas = 1.0 - betas\n",
        "        alphas_cumprod = np.cumprod(alphas, axis=0)\n",
        "        alphas_cumprod_prev = np.append(1.0, alphas_cumprod[:-1])\n",
        "\n",
        "        self.num_timesteps = int(timesteps)\n",
        "        to_torch = partial(torch.tensor, dtype=torch.float32)\n",
        "\n",
        "        self.register_buffer(\"betas\", to_torch(betas))\n",
        "        self.register_buffer(\"alphas_cumprod\", to_torch(alphas_cumprod))\n",
        "        self.register_buffer(\"alphas_cumprod_prev\", to_torch(alphas_cumprod_prev))\n",
        "        self.register_buffer(\"sqrt_recip_alphas_cumprod\", to_torch(np.sqrt(1.0 / alphas_cumprod)))\n",
        "        self.register_buffer(\"sqrt_one_minus_alphas_cumprod\", to_torch(np.sqrt(1.0 - alphas_cumprod)))\n",
        "\n",
        "    def ddim_sample(self, x, t, eta=0.0):\n",
        "        model_output = self.model(x, t)\n",
        "\n",
        "        if self.parameterization == \"eps\":\n",
        "            pred_x0 = extract(self.sqrt_recip_alphas_cumprod, t, x.shape) * x - \\\n",
        "                      extract(self.sqrt_one_minus_alphas_cumprod, t, x.shape) * model_output\n",
        "        else:\n",
        "            pred_x0 = model_output\n",
        "\n",
        "        if self.clip_denoised:\n",
        "            pred_x0 = torch.clamp(pred_x0, -1.0, 1.0)\n",
        "\n",
        "        sigma = eta * (1 - extract(self.alphas_cumprod, t, x.shape)).sqrt()\n",
        "        noise = torch.randn_like(x)\n",
        "\n",
        "        return pred_x0 + sigma * noise\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def p_sample_loop_ddim(self, shape, eta=0.0):\n",
        "        device = self.betas.device\n",
        "        img = torch.randn(shape, device=device)\n",
        "\n",
        "        for i in tqdm(reversed(range(0, self.ddim_steps)), desc=\"DDIM sampling\"):\n",
        "            t = torch.full((shape[0],), i, device=device, dtype=torch.long)\n",
        "            img = self.ddim_sample(img, t, eta)\n",
        "\n",
        "        return img\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, batch_size=16, eta=0.0):\n",
        "        image_size = self.model.image_size\n",
        "        channels = self.model.in_channels\n",
        "        return self.p_sample_loop_ddim((batch_size, channels, image_size), eta=eta)\n",
        "\n",
        "    def forward(self, x):\n",
        "        t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=x.device).long()\n",
        "        return self.ddim_sample(x, t)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHES = 5"
      ],
      "metadata": {
        "id": "OIp0KxjRrzBy"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import optim # Import optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "# 데이터셋 로드\n",
        "batch_size = 32\n",
        "\n",
        "# train_data_tensor는 (192, 10) 크기의 입력 데이터\n",
        "train_data_tensor = torch.tensor(train_data.values, dtype=torch.float32)  # (192, 10)\n",
        "train_dataset = TensorDataset(train_data_tensor)  # 레이블 없음\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# test_data_tensor는 (132, 10) 크기의 입력 데이터, test_labels_tensor는 (132,) 크기의 라벨\n",
        "test_data_tensor = torch.tensor\n",
        "\n",
        "# 데이터셋 로드\n",
        "batch_size = 32\n",
        "\n",
        "# train_data_tensor는 (192, 10) 크기의 입력 데이터\n",
        "train_data_tensor = torch.tensor(train_data.values, dtype=torch.float32)  # (192, 10)\n",
        "train_dataset = TensorDataset(train_data_tensor)  # 레이블 없음\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# test_data_tensor는 (132, 10) 크기의 입력 데이터, test_labels_tensor는 (132,) 크기의 라벨\n",
        "test_data_tensor = torch.tensor(test_data_encoded.drop(columns=[\"Hypertension\"]).values, dtype=torch.float32)  # (132, 10)\n",
        "test_labels_tensor = torch.tensor(test_data_encoded[\"Hypertension\"].values, dtype=torch.float32).unsqueeze(-1)  # (132, 1)\n",
        "test_dataset = TensorDataset(test_data_tensor, test_labels_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# DDIM 모델 정의 (예제)\n",
        "class DDIM(nn.Module):\n",
        "    def __init__(self, unet_config, timesteps=1000, ddim_steps=50, parameterization='eps'):\n",
        "        super(DDIM, self).__init__()\n",
        "        self.num_timesteps = timesteps\n",
        "        self.ddim_steps = ddim_steps\n",
        "        self.parameterization = parameterization\n",
        "        self.unet = UNetModel(**unet_config[\"params\"])  # 가정: UNet 클래스 존재\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        return self.unet(x, t)\n",
        "\n",
        "\n",
        "# 학습 함수\n",
        "def train_ddim_model(model, train_loader, test_loader, num_epochs=20, learning_rate=1e-4, device='cuda'):\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            x = batch[0].to(device)  # 레이블이 없으므로 x만 가져옴\n",
        "            # x = x.unsqueeze(1)  # Remove this line - it's causing the error\n",
        "\n",
        "            # Reshape x to have the expected shape (batch_size, in_channels, image_size)\n",
        "            x = x.view(x.shape[0], unet_config[\"params\"][\"in_channels\"], -1)\n",
        "\n",
        "            t = torch.randint(0, model.num_timesteps, (x.shape[0],), device=device).long()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = loss_fn(model(x, t), x)  # 모델의 출력과 입력 비교 (재구성 손실)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}: Avg Train Loss = {avg_loss:.4f}\")\n",
        "\n",
        "# 모델 초기화\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "unet_config = {\n",
        "    \"params\": {\n",
        "        \"image_size\": 10,  # feature 개수와 맞춤\n",
        "        \"in_channels\": 10,\n",
        "        \"model_channels\": 32,\n",
        "        \"out_channels\": 10,  # feature 개수와 동일하게 설정\n",
        "        \"num_res_blocks\": 2,\n",
        "        \"attention_resolutions\": [5],\n",
        "        \"dropout\": 0.1,\n",
        "        \"channel_mult\": (2, 4, 8),\n",
        "        \"num_heads\": 4,\n",
        "        \"use_scale_shift_norm\": False,\n",
        "        \"resblock_updown\": True,\n",
        "    }\n",
        "}\n",
        "\n",
        "ddim_model = DDIM(unet_config=unet_config, timesteps=1000, ddim_steps=50, parameterization='eps').to(device)\n",
        "\n",
        "# 학습 시작\n",
        "train_ddim_model(ddim_model, train_loader, test_loader, num_epochs=EPOCHES, learning_rate=1e-4, device=device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rznWkAeIomfF",
        "outputId": "6f5b4acd-9e33-4f88-83fe-5a1621c2a9f4"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/5:   0%|          | 0/6 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([32, 10, 1])) that is different to the input size (torch.Size([32, 10, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "Epoch 1/5: 100%|██████████| 6/6 [00:02<00:00,  2.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Avg Train Loss = 4801429.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 6/6 [00:01<00:00,  3.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Avg Train Loss = 4801220.3333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 6/6 [00:01<00:00,  3.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Avg Train Loss = 4801009.6667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 6/6 [00:01<00:00,  3.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Avg Train Loss = 4800797.5833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 6/6 [00:01<00:00,  3.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Avg Train Loss = 4800580.1667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAPx9lHQwEt7",
        "outputId": "c7ed1abe-2b6f-48ef-d3ce-3e453c57d312"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([132, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 함수\n",
        "\n",
        "# 테스트 함수\n",
        "def test_ddim_model(model, test_loader, device='cuda'):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Testing Model\"):\n",
        "            x, y = batch  # x는 입력 데이터, y는 실제 라벨 (Hypertension 값)\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # Reshape x to have the expected shape (batch_size, in_channels, image_size)\n",
        "            x = x.view(x.shape[0], unet_config[\"params\"][\"in_channels\"], -1)  # Reshape to match in_channels of UNet\n",
        "\n",
        "            t = torch.randint(0, model.num_timesteps, (x.shape[0],), device=device).long()\n",
        "            pred = model(x, t)\n",
        "\n",
        "            loss = loss_fn(pred, x)  # Calculate loss against x instead of y\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    print(f\"Test Avg Loss = {avg_loss:.4f}\")\n",
        "\n",
        "# 학습 함수\n",
        "\n",
        "# 테스트 실행\n",
        "test_ddim_model(ddim_model, test_loader, device=device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZa14LFEq0yy",
        "outputId": "49bc4660-88c8-4e23-80d3-031dcf022693"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing Model:  80%|████████  | 4/5 [00:00<00:00, 13.54it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([4, 10, 1])) that is different to the input size (torch.Size([4, 10, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "Testing Model: 100%|██████████| 5/5 [00:00<00:00, 15.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Avg Loss = 4497591.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "def test_ddim_model(model, test_loader, device='cuda'):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    results = []  # 결과 저장을 위한 리스트\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Testing Model\"):\n",
        "            x, y = batch  # x는 입력 데이터, y는 실제 라벨 (Hypertension 값)\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # Reshape x to have the expected shape (batch_size, in_channels, image_size)\n",
        "            x = x.view(x.shape[0], unet_config[\"params\"][\"in_channels\"], -1)  # Reshape to match in_channels of UNet\n",
        "\n",
        "            t = torch.randint(0, model.num_timesteps, (x.shape[0],), device=device).long()\n",
        "            pred = model(x, t)\n",
        "\n",
        "            loss = loss_fn(pred, x)  # Calculate loss against x instead of y\n",
        "\n",
        "            # 결과 저장\n",
        "            for i in range(x.shape[0]):\n",
        "                results.append({\n",
        "                    'Reconstruction_Loss': loss.item(),\n",
        "                    'Label': y[i].item()\n",
        "                })\n",
        "\n",
        "    # 데이터프레임 생성\n",
        "    df_results = pd.DataFrame(results)\n",
        "\n",
        "    return df_results\n",
        "\n",
        "# 테스트 실행\n",
        "df_results = test_ddim_model(ddim_model, test_loader, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCEgDzWVv-xH",
        "outputId": "b6c103db-0a2e-46d3-8af0-6bc1968b6d9b"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing Model: 100%|██████████| 5/5 [00:00<00:00, 15.60it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Label별 최소값과 최대값 계산\n",
        "MinMax_result = df_results.groupby('Label')['Reconstruction_Loss'].agg(['min', 'max']).reset_index()\n",
        "MinMax_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "M7NcyGcF0dp3",
        "outputId": "947bb0b0-d558-44f5-ca8c-d525c28a56c9"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Label        min        max\n",
              "0    0.0  4662753.0  4708318.0\n",
              "1    1.0  4041274.5  4662753.0\n",
              "2    2.0  4041274.5  4041274.5\n",
              "3    3.0  4041274.5  4796478.5"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3e02dbaa-f046-4e34-854d-111f69ecaa06\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>min</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4662753.0</td>\n",
              "      <td>4708318.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>4041274.5</td>\n",
              "      <td>4662753.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.0</td>\n",
              "      <td>4041274.5</td>\n",
              "      <td>4041274.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.0</td>\n",
              "      <td>4041274.5</td>\n",
              "      <td>4796478.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3e02dbaa-f046-4e34-854d-111f69ecaa06')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3e02dbaa-f046-4e34-854d-111f69ecaa06 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3e02dbaa-f046-4e34-854d-111f69ecaa06');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ccc92d7e-3244-4780-8bdc-590ba109121f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ccc92d7e-3244-4780-8bdc-590ba109121f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ccc92d7e-3244-4780-8bdc-590ba109121f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_128781fd-cc88-4503-900d-af82eefdb423\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('MinMax_result')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_128781fd-cc88-4503-900d-af82eefdb423 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('MinMax_result');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "MinMax_result",
              "summary": "{\n  \"name\": \"MinMax_result\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.2909944487358056,\n        \"min\": 0.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1.0,\n          3.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"min\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 310739.25,\n        \"min\": 4041274.5,\n        \"max\": 4662753.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          4041274.5,\n          4662753.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"max\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 345114.30362586636,\n        \"min\": 4041274.5,\n        \"max\": 4796478.5,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          4662753.0,\n          4796478.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 4662753.0"
      ],
      "metadata": {
        "id": "-Gz5lRLD0yF4"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 박스플롯 생성\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x=df_results[\"Label\"], y=df_results[\"Reconstruction_Loss\"])\n",
        "\n",
        "plt.axhline(y=threshold, color='r', linestyle='--', linewidth=2, label=f'Threshold = {threshold}')\n",
        "\n",
        "# 그래프 제목 및 라벨 설정\n",
        "plt.title(\"Reconstruction Loss Distribution by Label\")\n",
        "plt.xlabel(\"Label\")\n",
        "plt.ylabel(\"Reconstruction Loss\")\n",
        "\n",
        "# 그래프 표시\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "T_z-GVbYwXt1",
        "outputId": "e8b1ed27-30ff-422a-a8b7-7eb8fd3fe040"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score, roc_auc_score\n",
        "\n",
        "# 이상치(Outlier) 판별 (임계값 이하인 경우 이상치로 간주)\n",
        "df_results['Predicted_Outlier'] = (df_results['Reconstruction_Loss'] <= threshold).astype(int)\n",
        "\n",
        "# 실제 이상치(1: 이상치, 0: 정상) 정의\n",
        "df_results['Actual_Outlier'] = df_results['Label'].isin([1, 2, 3]).astype(int)\n",
        "\n",
        "# 평가 지표 계산\n",
        "f1 = f1_score(df_results['Actual_Outlier'], df_results['Predicted_Outlier'])\n",
        "recall = recall_score(df_results['Actual_Outlier'], df_results['Predicted_Outlier'])\n",
        "precision = precision_score(df_results['Actual_Outlier'], df_results['Predicted_Outlier'])\n",
        "auc = roc_auc_score(df_results['Actual_Outlier'], df_results['Reconstruction_Loss'])  # AUC는 재구성 오류 값 사용\n",
        "\n",
        "# 결과 출력\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"AUC: {auc:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0oi8D0BzAFC",
        "outputId": "62674e24-920e-4788-8ff4-d3131c963341"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score: 0.8889\n",
            "Recall: 0.9524\n",
            "Precision: 0.8333\n",
            "AUC: 0.0794\n"
          ]
        }
      ]
    }
  ]
}