{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1B8j7ngOM-tsmqdnGQGqvmxWb4kSx08B8","timestamp":1738741717266}],"gpuType":"T4","authorship_tag":"ABX9TyNfaAWcrIP+q6mZeOy30oN9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","\n","# Define diffusion schedule\n","class DiffusionScheduler:\n","    def __init__(self, timesteps=1000, beta_start=0.0001, beta_end=0.02):\n","        self.timesteps = timesteps\n","        self.beta = np.linspace(beta_start, beta_end, timesteps, dtype=np.float32)\n","        self.alpha = 1.0 - self.beta\n","        self.alpha_bar = np.cumprod(self.alpha)\n","\n","    def get_noise_level(self, t):\n","        return self.beta[t], self.alpha_bar[t]\n","\n","diffusion_scheduler = DiffusionScheduler()\n","\n","# Forward diffusion function\n","def forward_diffusion(x_0, t, noise_level=1.0):\n","    noise = torch.randn_like(x_0) * noise_level\n","    beta_t, alpha_bar_t = diffusion_scheduler.get_noise_level(t)\n","    x_t = torch.sqrt(torch.tensor(alpha_bar_t)) * x_0 + torch.sqrt(torch.tensor(1 - alpha_bar_t)) * noise\n","    return x_t, noise\n","\n","# Define U-Net Model for noise prediction\n","class UNet(nn.Module):\n","    def __init__(self):\n","        super(UNet, self).__init__()\n","        self.enc1 = nn.Sequential(nn.Conv1d(9, 32, 3, padding=1), nn.ReLU(), nn.MaxPool1d(2))\n","        self.enc2 = nn.Sequential(nn.Conv1d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool1d(2))\n","        self.enc3 = nn.Sequential(nn.Conv1d(64, 128, 3, padding=1), nn.ReLU())\n","\n","        self.dec2 = nn.Sequential(nn.Upsample(scale_factor=2, mode='nearest'), nn.Conv1d(128, 64, 3, padding=1), nn.ReLU())\n","        self.dec1 = nn.Sequential(nn.Upsample(scale_factor=2, mode='nearest'), nn.Conv1d(64, 32, 3, padding=1), nn.ReLU())\n","\n","        self.final = nn.Conv1d(32, 9, 1, padding=0)\n","\n","    def forward(self, x):\n","        enc1_out = self.enc1(x)\n","        enc2_out = self.enc2(enc1_out)\n","        bottleneck = self.enc3(enc2_out)\n","\n","        dec2_out = self.dec2(bottleneck)\n","        dec1_out = self.dec1(dec2_out)\n","\n","        return self.final(dec1_out)\n","\n","# Create synthetic dataset\n","def create_dataset(num_samples=1000, seq_length=256, num_features=9):\n","    return torch.randn((num_samples, num_features, seq_length))\n","\n","dataset = create_dataset()\n","\n","# Model Initialization\n","unet_model = UNet()\n","optimizer = optim.Adam(unet_model.parameters(), lr=0.0001)\n","criterion = nn.MSELoss()\n","\n","# Training Loop\n","EPOCHS = 10\n","BATCH_SIZE = 32\n","TIMESTEPS = 1000\n","for epoch in range(EPOCHS):\n","    for i in range(0, dataset.shape[0], BATCH_SIZE):\n","        batch = dataset[i:i+BATCH_SIZE]\n","        t = np.random.randint(0, TIMESTEPS)\n","        noisy_x, noise = forward_diffusion(batch, t)\n","\n","        optimizer.zero_grad()\n","        predicted_noise = unet_model(noisy_x)\n","        loss = criterion(predicted_noise, noise)\n","        loss.backward()\n","        optimizer.step()\n","\n","    print(f\"Epoch {epoch + 1}/{EPOCHS}, Loss: {loss.item():.4f}\")\n","\n","# Sampling function (Reverse Process)\n","def sample(num_samples=1, timesteps=1000):\n","    x_t = torch.randn((num_samples, 9, 256))  # Random noise\n","    for t in reversed(range(timesteps)):\n","        predicted_noise = unet_model(x_t)\n","        beta_t, alpha_bar_t = diffusion_scheduler.get_noise_level(t)\n","        x_t = (x_t - torch.sqrt(torch.tensor(1 - alpha_bar_t)) * predicted_noise) / torch.sqrt(torch.tensor(alpha_bar_t))\n","        x_t += torch.sqrt(torch.tensor(beta_t)) * torch.randn_like(x_t)  # Add small noise\n","    return x_t\n","\n","# Generate new samples\n","generated_samples = sample(num_samples=5)\n","print(\"Generated Samples Shape:\", generated_samples.shape)\n","\n","# Anomaly Detection Function\n","def anomaly_score(real, recon):\n","    return torch.mean(torch.abs(real - recon), dim=1)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M_HxD5AzEWZs","executionInfo":{"status":"ok","timestamp":1738741814939,"user_tz":-540,"elapsed":35085,"user":{"displayName":"­이명규 | 의공학연구소 | 연구교수 | 한양대(서울)","userId":"13620601812072270669"}},"outputId":"9f23e8fe-0e4e-4f1e-bc59-95049670193d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10, Loss: 1.0031\n","Epoch 2/10, Loss: 0.9839\n","Epoch 3/10, Loss: 1.0007\n","Epoch 4/10, Loss: 0.9522\n","Epoch 5/10, Loss: 0.9386\n","Epoch 6/10, Loss: 0.9194\n","Epoch 7/10, Loss: 0.9130\n","Epoch 8/10, Loss: 0.8777\n","Epoch 9/10, Loss: 1.0318\n","Epoch 10/10, Loss: 0.8703\n","Generated Samples Shape: torch.Size([5, 9, 256])\n"]}]},{"cell_type":"code","source":["# Generate Test Data\n","num_test_samples = 100\n","test_data = torch.randn((num_test_samples, 9, 256))\n","noisy_test, _ = forward_diffusion(test_data, np.random.randint(0, TIMESTEPS))"],"metadata":{"id":"wZeq3frnE4ED","executionInfo":{"status":"ok","timestamp":1738741835521,"user_tz":-540,"elapsed":378,"user":{"displayName":"­이명규 | 의공학연구소 | 연구교수 | 한양대(서울)","userId":"13620601812072270669"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Reconstruct using trained model\n","reconstructed_test = sample(num_samples=num_test_samples)\n","\n","# Compute anomaly scores\n","anomaly_scores = anomaly_score(test_data, reconstructed_test)\n","\n","# Identify anomalies (threshold can be tuned)\n","anomaly_threshold = torch.mean(anomaly_scores) + 2 * torch.std(anomaly_scores)\n","anomalies = anomaly_scores > anomaly_threshold\n","\n","print(f\"Number of detected anomalies: {torch.sum(anomalies).item()}\")"],"metadata":{"id":"Y4FFUpRrFODs"},"execution_count":null,"outputs":[]}]}