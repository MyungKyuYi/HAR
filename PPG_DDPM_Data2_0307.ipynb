{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "0c959315-11ef-4c28-aabe-4ee52629dc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from abc import abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "fd79bc33-41a4-4000-8e54-31d93dcfce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "4f986da2-d421-461d-8a9e-e1866b8a0951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "31fbc844-d1fc-4afb-8e8e-df1f82e68685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 이름에서 숫자를 추출할 수 없습니다: merged.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:40: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:40: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_13900\\3832726767.py:40: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  path = \"E:\\dataset\\PPG_BP_assessment\\Dataset\\PPG_csv\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PPG</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.493164</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.473633</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.454102</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.439453</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.429688</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707925</th>\n",
       "      <td>1.240234</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707926</th>\n",
       "      <td>1.269531</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707927</th>\n",
       "      <td>1.298828</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707928</th>\n",
       "      <td>1.328125</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707929</th>\n",
       "      <td>1.347656</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>707930 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             PPG  subject\n",
       "0       0.493164        1\n",
       "1       0.473633        1\n",
       "2       0.454102        1\n",
       "3       0.439453        1\n",
       "4       0.429688        1\n",
       "...          ...      ...\n",
       "707925  1.240234       56\n",
       "707926  1.269531       56\n",
       "707927  1.298828       56\n",
       "707928  1.328125       56\n",
       "707929  1.347656       56\n",
       "\n",
       "[707930 rows x 2 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_ppg_data(path):\n",
    "    data_list = []\n",
    "\n",
    "    # 디렉토리 내 모든 파일을 순회\n",
    "    for file in os.listdir(path):\n",
    "        file_path = os.path.join(path, file)\n",
    "        \n",
    "        # 파일인지 확인\n",
    "        if os.path.isfile(file_path):\n",
    "            try:\n",
    "                # 파일 이름에서 끝자리 숫자 추출 (정수 변환)\n",
    "                subject_number = int(''.join(filter(str.isdigit, file.split('.')[-2])))\n",
    "                \n",
    "                # 파일 데이터 읽기 (여기서는 CSV 형식 가정)\n",
    "                df = pd.read_csv(file_path, header=None, names=['PPG'])\n",
    "                \n",
    "                # subject 컬럼 추가\n",
    "                df['subject'] = subject_number\n",
    "                \n",
    "                # 리스트에 추가\n",
    "                data_list.append(df)\n",
    "\n",
    "            except ValueError:\n",
    "                print(f\"파일 이름에서 숫자를 추출할 수 없습니다: {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"파일 {file} 처리 중 오류 발생: {e}\")\n",
    "\n",
    "    # 데이터프레임 병합\n",
    "    if data_list:\n",
    "        final_df = pd.concat(data_list, ignore_index=True)\n",
    "    else:\n",
    "        final_df = pd.DataFrame(columns=['PPG', 'subject'])\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# 사용 예시\n",
    "path = \"E:\\dataset\\PPG_BP_assessment\\Dataset\\PPG_csv\"\n",
    "df = load_ppg_data(path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e183e2b6-0acd-4605-912d-54199f86da43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject\n",
       "29    36522\n",
       "12    12321\n",
       "53    12321\n",
       "8     12321\n",
       "6     12318\n",
       "9     12305\n",
       "17    12292\n",
       "54    12287\n",
       "28    12287\n",
       "1     12287\n",
       "13    12285\n",
       "43    12285\n",
       "33    12280\n",
       "22    12279\n",
       "46    12278\n",
       "37    12278\n",
       "39    12278\n",
       "27    12278\n",
       "41    12278\n",
       "44    12278\n",
       "34    12278\n",
       "20    12278\n",
       "19    12278\n",
       "51    12278\n",
       "32    12278\n",
       "50    12277\n",
       "40    12277\n",
       "36    12277\n",
       "18    12275\n",
       "42    12268\n",
       "7     12268\n",
       "3     12259\n",
       "56    12242\n",
       "24    12242\n",
       "2     12239\n",
       "14    12238\n",
       "30    12193\n",
       "31    12172\n",
       "38    12168\n",
       "4     12151\n",
       "45    12132\n",
       "26    12132\n",
       "52    12119\n",
       "10    12119\n",
       "47    12115\n",
       "5     12115\n",
       "11    12086\n",
       "48    12085\n",
       "15    12085\n",
       "49    12041\n",
       "25    12041\n",
       "21    12025\n",
       "16    12001\n",
       "23    11952\n",
       "55    11952\n",
       "35    11936\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subject'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0e02e946-f1c8-4fb1-a73f-5c6d04840b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             PPG  subject\n",
      "0       0.493164        1\n",
      "1       0.473633        1\n",
      "2       0.454102        1\n",
      "3       0.439453        1\n",
      "4       0.429688        1\n",
      "...          ...      ...\n",
      "706683  1.069336       56\n",
      "706684  1.059570       56\n",
      "706685  1.049805       56\n",
      "706686  1.040039       56\n",
      "706687  1.035156       56\n",
      "\n",
      "[616000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 예시 데이터프레임 df\n",
    "# df = pd.read_csv(\"your_file.csv\")\n",
    "\n",
    "# subject별로 인덱스를 부여하고 11000개 초과하는 항목 제거\n",
    "df['seq'] = df.groupby('subject').cumcount()\n",
    "df_filtered = df[df['seq'] < 11000].drop(columns='seq')\n",
    "\n",
    "# 결과 확인\n",
    "print(df_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "9de4ba33-e151-492f-a8a4-4aef71555fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subject\n",
       "1     11000\n",
       "2     11000\n",
       "31    11000\n",
       "32    11000\n",
       "33    11000\n",
       "34    11000\n",
       "35    11000\n",
       "36    11000\n",
       "37    11000\n",
       "38    11000\n",
       "39    11000\n",
       "40    11000\n",
       "41    11000\n",
       "42    11000\n",
       "43    11000\n",
       "44    11000\n",
       "45    11000\n",
       "46    11000\n",
       "47    11000\n",
       "48    11000\n",
       "49    11000\n",
       "50    11000\n",
       "51    11000\n",
       "52    11000\n",
       "53    11000\n",
       "54    11000\n",
       "55    11000\n",
       "30    11000\n",
       "29    11000\n",
       "28    11000\n",
       "14    11000\n",
       "3     11000\n",
       "4     11000\n",
       "5     11000\n",
       "6     11000\n",
       "7     11000\n",
       "8     11000\n",
       "9     11000\n",
       "10    11000\n",
       "11    11000\n",
       "12    11000\n",
       "13    11000\n",
       "15    11000\n",
       "27    11000\n",
       "16    11000\n",
       "17    11000\n",
       "18    11000\n",
       "19    11000\n",
       "20    11000\n",
       "21    11000\n",
       "22    11000\n",
       "23    11000\n",
       "24    11000\n",
       "25    11000\n",
       "26    11000\n",
       "56    11000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered['subject'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "51aca187-b8ec-4936-9264-60ef91fca6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5, 6, 10, 11, 12, 15, 19, 22, 24, 25, 28, 30, 40, 47, 48, 49, 50, 51)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N-0 : \n",
    "\n",
    "2, 5, 6, 10, 11, 12, 15, 19, 22, 24, 25, 28, 30, 40, 47, 48, 49, 50, 51\n",
    "\n",
    "\n",
    "# H-1 : 7, 8, 13, 14, 16, 18, 20, 26, 27, 35, 41, 42, 43, 44, 45, 46\n",
    "# E-2: 3, 4, 9, 32, 33, 36, 52, 53\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e8d67c41-f016-46a1-8ddb-c78486fbb288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(707930, 3)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "95aeabde-3a62-4801-bd23-31aa0c3aedec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def reshape_ppg_dataframe(df, window_size=50):\n",
    "    # 결과 저장 리스트\n",
    "    result = []\n",
    "\n",
    "    # subject별로 처리\n",
    "    for subject_id, group in df.groupby('subject'):\n",
    "        ppg_values = group['PPG'].values\n",
    "        total_len = len(ppg_values)\n",
    "\n",
    "        # window_size보다 작으면 건너뜀\n",
    "        if total_len < window_size:\n",
    "            continue\n",
    "\n",
    "        # window_size로 나눠떨어지는 최대 길이까지 처리\n",
    "        max_valid_length = (total_len // window_size) * window_size\n",
    "        for i in range(0, max_valid_length, window_size):\n",
    "            window = ppg_values[i:i+window_size]\n",
    "            row = {f'PPG{j+1}': window[j] for j in range(window_size)}\n",
    "            row['subject'] = subject_id\n",
    "            result.append(row)\n",
    "\n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "# 사용 예시: 윈도우 크기를 64로 지정\n",
    "new_df = reshape_ppg_dataframe(df_filtered, window_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "6ab7370f-7905-47df-957a-5162166a9f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12320, 51)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d2692874-7e15-4319-900c-6ad136661372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hypertension\n",
       "1.0    4620\n",
       "0.0    4180\n",
       "2.0    3520\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 조건에 따라 Hypertension 값 할당\n",
    "hypertension_0 = {2, 5, 6, 10, 11, 12, 15, 19, 22, 24, 25, 28, 30, 40, 47, 48, 49, 50, 51}\n",
    "hypertension_2 = {7, 8, 13, 14, 16, 18, 20, 26, 27, 35, 41, 42, 43, 44, 45, 46}\n",
    "hypertension_1 = {1, 3, 4, 9, 17, 21, 23, 29, 31, 32, 33, 34, 36, 37, 38, 39, 52, 53, 54, 55, 56}\n",
    "\n",
    "new_df.loc[new_df['subject'].isin(hypertension_0), 'Hypertension'] = 0\n",
    "new_df.loc[new_df['subject'].isin(hypertension_2), 'Hypertension'] = 2\n",
    "new_df.loc[new_df['subject'].isin(hypertension_1), 'Hypertension'] = 1\n",
    "\n",
    "# 결과 출력\n",
    "new_df['Hypertension'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "1c390b35-9dcd-4132-8bde-6275c70b19cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train으로 사용할 subject 리스트\n",
    "train_subjects = {2, 5, 6, 10, 11, 12, 15, 19, 22, 24, 25, 28, 30, 40}\n",
    "\n",
    "# 조건에 따라 데이터 분할\n",
    "train_df = new_df[new_df['subject'].isin(train_subjects)]\n",
    "test_df = new_df[~new_df['subject'].isin(train_subjects)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "44c782ed-fa05-49c8-aadb-324181f7c19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PPG1</th>\n",
       "      <th>PPG2</th>\n",
       "      <th>PPG3</th>\n",
       "      <th>PPG4</th>\n",
       "      <th>PPG5</th>\n",
       "      <th>PPG6</th>\n",
       "      <th>PPG7</th>\n",
       "      <th>PPG8</th>\n",
       "      <th>PPG9</th>\n",
       "      <th>PPG10</th>\n",
       "      <th>...</th>\n",
       "      <th>PPG43</th>\n",
       "      <th>PPG44</th>\n",
       "      <th>PPG45</th>\n",
       "      <th>PPG46</th>\n",
       "      <th>PPG47</th>\n",
       "      <th>PPG48</th>\n",
       "      <th>PPG49</th>\n",
       "      <th>PPG50</th>\n",
       "      <th>subject</th>\n",
       "      <th>Hypertension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.375977</td>\n",
       "      <td>0.341797</td>\n",
       "      <td>0.307617</td>\n",
       "      <td>0.273438</td>\n",
       "      <td>0.249023</td>\n",
       "      <td>0.229492</td>\n",
       "      <td>0.205078</td>\n",
       "      <td>0.190430</td>\n",
       "      <td>0.170898</td>\n",
       "      <td>0.161133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>0.517578</td>\n",
       "      <td>0.522461</td>\n",
       "      <td>0.527344</td>\n",
       "      <td>0.532227</td>\n",
       "      <td>0.527344</td>\n",
       "      <td>0.527344</td>\n",
       "      <td>0.522461</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0.512695</td>\n",
       "      <td>0.502930</td>\n",
       "      <td>0.488281</td>\n",
       "      <td>0.473633</td>\n",
       "      <td>0.463867</td>\n",
       "      <td>0.449219</td>\n",
       "      <td>0.434570</td>\n",
       "      <td>0.424805</td>\n",
       "      <td>0.419922</td>\n",
       "      <td>0.410156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180664</td>\n",
       "      <td>0.175781</td>\n",
       "      <td>0.170898</td>\n",
       "      <td>0.170898</td>\n",
       "      <td>0.170898</td>\n",
       "      <td>0.166016</td>\n",
       "      <td>0.161133</td>\n",
       "      <td>0.161133</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>0.161133</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.166016</td>\n",
       "      <td>0.180664</td>\n",
       "      <td>0.200195</td>\n",
       "      <td>0.229492</td>\n",
       "      <td>0.258789</td>\n",
       "      <td>0.302734</td>\n",
       "      <td>0.341797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.385742</td>\n",
       "      <td>0.380859</td>\n",
       "      <td>0.375977</td>\n",
       "      <td>0.371094</td>\n",
       "      <td>0.366211</td>\n",
       "      <td>0.356445</td>\n",
       "      <td>0.351562</td>\n",
       "      <td>0.341797</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>0.332031</td>\n",
       "      <td>0.322266</td>\n",
       "      <td>0.307617</td>\n",
       "      <td>0.297852</td>\n",
       "      <td>0.288086</td>\n",
       "      <td>0.273438</td>\n",
       "      <td>0.263672</td>\n",
       "      <td>0.253906</td>\n",
       "      <td>0.244141</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.571289</td>\n",
       "      <td>0.581055</td>\n",
       "      <td>0.585938</td>\n",
       "      <td>0.585938</td>\n",
       "      <td>0.585938</td>\n",
       "      <td>0.585938</td>\n",
       "      <td>0.576172</td>\n",
       "      <td>0.571289</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0.561523</td>\n",
       "      <td>0.551758</td>\n",
       "      <td>0.537109</td>\n",
       "      <td>0.522461</td>\n",
       "      <td>0.512695</td>\n",
       "      <td>0.502930</td>\n",
       "      <td>0.488281</td>\n",
       "      <td>0.473633</td>\n",
       "      <td>0.463867</td>\n",
       "      <td>0.454102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.209961</td>\n",
       "      <td>0.200195</td>\n",
       "      <td>0.200195</td>\n",
       "      <td>0.195312</td>\n",
       "      <td>0.190430</td>\n",
       "      <td>0.185547</td>\n",
       "      <td>0.185547</td>\n",
       "      <td>0.180664</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8795</th>\n",
       "      <td>0.952148</td>\n",
       "      <td>0.947266</td>\n",
       "      <td>0.952148</td>\n",
       "      <td>0.971680</td>\n",
       "      <td>1.000977</td>\n",
       "      <td>1.049805</td>\n",
       "      <td>1.108398</td>\n",
       "      <td>1.166992</td>\n",
       "      <td>1.235352</td>\n",
       "      <td>1.293945</td>\n",
       "      <td>...</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.230469</td>\n",
       "      <td>1.206055</td>\n",
       "      <td>1.186523</td>\n",
       "      <td>1.166992</td>\n",
       "      <td>1.142578</td>\n",
       "      <td>1.123047</td>\n",
       "      <td>1.098633</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8796</th>\n",
       "      <td>1.083984</td>\n",
       "      <td>1.059570</td>\n",
       "      <td>1.035156</td>\n",
       "      <td>1.020508</td>\n",
       "      <td>1.000977</td>\n",
       "      <td>0.986328</td>\n",
       "      <td>0.976562</td>\n",
       "      <td>0.961914</td>\n",
       "      <td>0.976562</td>\n",
       "      <td>0.996094</td>\n",
       "      <td>...</td>\n",
       "      <td>1.381836</td>\n",
       "      <td>1.362305</td>\n",
       "      <td>1.342773</td>\n",
       "      <td>1.318359</td>\n",
       "      <td>1.298828</td>\n",
       "      <td>1.274414</td>\n",
       "      <td>1.259766</td>\n",
       "      <td>1.235352</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8797</th>\n",
       "      <td>1.215820</td>\n",
       "      <td>1.196289</td>\n",
       "      <td>1.171875</td>\n",
       "      <td>1.157227</td>\n",
       "      <td>1.132812</td>\n",
       "      <td>1.113281</td>\n",
       "      <td>1.093750</td>\n",
       "      <td>1.064453</td>\n",
       "      <td>1.044922</td>\n",
       "      <td>1.025391</td>\n",
       "      <td>...</td>\n",
       "      <td>1.489258</td>\n",
       "      <td>1.464844</td>\n",
       "      <td>1.440430</td>\n",
       "      <td>1.416016</td>\n",
       "      <td>1.401367</td>\n",
       "      <td>1.376953</td>\n",
       "      <td>1.357422</td>\n",
       "      <td>1.337891</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8798</th>\n",
       "      <td>1.313477</td>\n",
       "      <td>1.293945</td>\n",
       "      <td>1.274414</td>\n",
       "      <td>1.254883</td>\n",
       "      <td>1.235352</td>\n",
       "      <td>1.206055</td>\n",
       "      <td>1.191406</td>\n",
       "      <td>1.171875</td>\n",
       "      <td>1.147461</td>\n",
       "      <td>1.132812</td>\n",
       "      <td>...</td>\n",
       "      <td>1.586914</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>1.542969</td>\n",
       "      <td>1.518555</td>\n",
       "      <td>1.494141</td>\n",
       "      <td>1.469727</td>\n",
       "      <td>1.440430</td>\n",
       "      <td>1.425781</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8799</th>\n",
       "      <td>1.401367</td>\n",
       "      <td>1.381836</td>\n",
       "      <td>1.362305</td>\n",
       "      <td>1.342773</td>\n",
       "      <td>1.323242</td>\n",
       "      <td>1.303711</td>\n",
       "      <td>1.284180</td>\n",
       "      <td>1.269531</td>\n",
       "      <td>1.245117</td>\n",
       "      <td>1.230469</td>\n",
       "      <td>...</td>\n",
       "      <td>1.572266</td>\n",
       "      <td>1.586914</td>\n",
       "      <td>1.596680</td>\n",
       "      <td>1.601562</td>\n",
       "      <td>1.596680</td>\n",
       "      <td>1.591797</td>\n",
       "      <td>1.572266</td>\n",
       "      <td>1.557617</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3080 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          PPG1      PPG2      PPG3      PPG4      PPG5      PPG6      PPG7  \\\n",
       "220   0.375977  0.341797  0.307617  0.273438  0.249023  0.229492  0.205078   \n",
       "221   0.512695  0.502930  0.488281  0.473633  0.463867  0.449219  0.434570   \n",
       "222   0.161133  0.156250  0.156250  0.166016  0.180664  0.200195  0.229492   \n",
       "223   0.332031  0.322266  0.307617  0.297852  0.288086  0.273438  0.263672   \n",
       "224   0.561523  0.551758  0.537109  0.522461  0.512695  0.502930  0.488281   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "8795  0.952148  0.947266  0.952148  0.971680  1.000977  1.049805  1.108398   \n",
       "8796  1.083984  1.059570  1.035156  1.020508  1.000977  0.986328  0.976562   \n",
       "8797  1.215820  1.196289  1.171875  1.157227  1.132812  1.113281  1.093750   \n",
       "8798  1.313477  1.293945  1.274414  1.254883  1.235352  1.206055  1.191406   \n",
       "8799  1.401367  1.381836  1.362305  1.342773  1.323242  1.303711  1.284180   \n",
       "\n",
       "          PPG8      PPG9     PPG10  ...     PPG43     PPG44     PPG45  \\\n",
       "220   0.190430  0.170898  0.161133  ...  0.507812  0.517578  0.522461   \n",
       "221   0.424805  0.419922  0.410156  ...  0.180664  0.175781  0.170898   \n",
       "222   0.258789  0.302734  0.341797  ...  0.385742  0.380859  0.375977   \n",
       "223   0.253906  0.244141  0.234375  ...  0.571289  0.581055  0.585938   \n",
       "224   0.473633  0.463867  0.454102  ...  0.209961  0.200195  0.200195   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "8795  1.166992  1.235352  1.293945  ...  1.250000  1.230469  1.206055   \n",
       "8796  0.961914  0.976562  0.996094  ...  1.381836  1.362305  1.342773   \n",
       "8797  1.064453  1.044922  1.025391  ...  1.489258  1.464844  1.440430   \n",
       "8798  1.171875  1.147461  1.132812  ...  1.586914  1.562500  1.542969   \n",
       "8799  1.269531  1.245117  1.230469  ...  1.572266  1.586914  1.596680   \n",
       "\n",
       "         PPG46     PPG47     PPG48     PPG49     PPG50  subject  Hypertension  \n",
       "220   0.527344  0.532227  0.527344  0.527344  0.522461        2           0.0  \n",
       "221   0.170898  0.170898  0.166016  0.161133  0.161133        2           0.0  \n",
       "222   0.371094  0.366211  0.356445  0.351562  0.341797        2           0.0  \n",
       "223   0.585938  0.585938  0.585938  0.576172  0.571289        2           0.0  \n",
       "224   0.195312  0.190430  0.185547  0.185547  0.180664        2           0.0  \n",
       "...        ...       ...       ...       ...       ...      ...           ...  \n",
       "8795  1.186523  1.166992  1.142578  1.123047  1.098633       40           0.0  \n",
       "8796  1.318359  1.298828  1.274414  1.259766  1.235352       40           0.0  \n",
       "8797  1.416016  1.401367  1.376953  1.357422  1.337891       40           0.0  \n",
       "8798  1.518555  1.494141  1.469727  1.440430  1.425781       40           0.0  \n",
       "8799  1.601562  1.596680  1.591797  1.572266  1.557617       40           0.0  \n",
       "\n",
       "[3080 rows x 52 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "5de4b82c-e921-46a9-925d-3f276c384162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hypertension\n",
       "0.0    3080\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Hypertension'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "f6e975c0-7eef-4425-a6d2-6b105352c5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hypertension\n",
       "1.0    4620\n",
       "2.0    3520\n",
       "0.0    1100\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['Hypertension'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b105f08b-2dd3-403e-9384-f61fc61611fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def timestep_embedding(timesteps, dim, max_period=1000, repeat_only=False):\n",
    "    \"\"\"\n",
    "    Create sinusoidal timestep embeddings.\n",
    "    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n",
    "                        These may be fractional.\n",
    "    :param dim: the dimension of the output.\n",
    "    :param max_period: controls the minimum frequency of the embeddings.\n",
    "    :return: an [N x dim] Tensor of positional embeddings.\n",
    "    \"\"\"\n",
    "    if not repeat_only:\n",
    "        half = dim // 2\n",
    "        freqs = torch.exp(\n",
    "            -math.log(max_period)\n",
    "            * torch.arange(start=0, end=half, dtype=torch.float32)\n",
    "            / half\n",
    "        ).to(device=timesteps.device)\n",
    "        args = timesteps[:, None].float() * freqs[None]\n",
    "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "        if dim % 2:\n",
    "            embedding = torch.cat(\n",
    "                [embedding, torch.zeros_like(embedding[:, :1])], dim=-1\n",
    "            )\n",
    "    else:\n",
    "        embedding = repeat(timesteps, \"b -> b d\", d=dim)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def zero_module(module):\n",
    "    \"\"\"\n",
    "    Zero out the parameters of a module and return it.\n",
    "    \"\"\"\n",
    "    for p in module.parameters():\n",
    "        p.detach().zero_()\n",
    "    return module\n",
    "\n",
    "\n",
    "class TimestepBlock(nn.Module):\n",
    "    def forward(self, x, emb):\n",
    "        \"\"\"\n",
    "        Apply the module to `x` given `emb` timestep embeddings.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n",
    "    \"\"\"\n",
    "    A sequential module that passes timestep embeddings to the children that\n",
    "    support it as an extra input.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x, emb, context=None):\n",
    "        for layer in self:\n",
    "            if isinstance(layer, TimestepBlock):\n",
    "                x = layer(x, emb)  # Pass emb to TimestepBlock layers\n",
    "            else:\n",
    "                x = layer(x)  # Regular layers do not receive emb\n",
    "        return x\n",
    "\n",
    "def Normalize(in_channels):\n",
    "    return nn.GroupNorm(\n",
    "        num_groups=32, num_channels=in_channels, eps=1e-6, affine=True\n",
    "    )\n",
    "\n",
    "\n",
    "def count_flops_attn(model, _x, y):\n",
    "    \"\"\"\n",
    "    A counter for the `thop` package to count the operations in an\n",
    "    attention operation.\n",
    "    Meant to be used like:\n",
    "        macs, params = thop.profile(\n",
    "            model,\n",
    "            inputs=(inputs, timestamps),\n",
    "            custom_ops={QKVAttention: QKVAttention.count_flops},\n",
    "        )\n",
    "    \"\"\"\n",
    "    b, c, *spatial = y[0].shape\n",
    "    num_spatial = int(np.prod(spatial))\n",
    "    # We perform two matmuls with the same number of ops.\n",
    "    # The first computes the weight matrix, the second computes\n",
    "    # the combination of the value vectors.\n",
    "    matmul_ops = 2 * b * (num_spatial**2) * c\n",
    "    model.total_ops += th.DoubleTensor([matmul_ops])\n",
    "\n",
    "\n",
    "class QKVAttentionLegacy(nn.Module):\n",
    "    \"\"\"\n",
    "    A module which performs QKV attention.\n",
    "    Matches legacy QKVAttention + input/ouput heads shaping\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        \"\"\"\n",
    "        Apply QKV attention.\n",
    "        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n",
    "        :return: an [N x (H * C) x T] tensor after attention.\n",
    "        \"\"\"\n",
    "        bs, width, length = qkv.shape\n",
    "        assert width % (3 * self.n_heads) == 0\n",
    "        ch = width // (3 * self.n_heads)\n",
    "        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(\n",
    "            ch, dim=1\n",
    "        )\n",
    "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
    "        weight = torch.einsum(\n",
    "            \"bct,bcs->bts\", q * scale, k * scale\n",
    "        )  # More stable with f16 than dividing afterwards\n",
    "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
    "        a = torch.einsum(\"bts,bcs->bct\", weight, v)\n",
    "        return a.reshape(bs, -1, length)\n",
    "\n",
    "    @staticmethod\n",
    "    def count_flops(model, _x, y):\n",
    "        return count_flops_attn(model, _x, y)\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    An attention block that allows spatial positions to attend to each other.\n",
    "    Originally ported from here, but adapted to the N-d case.\n",
    "    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        num_heads=1,\n",
    "        num_head_channels=-1,\n",
    "        use_checkpoint=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        if num_head_channels == -1:\n",
    "            self.num_heads = num_heads\n",
    "        else:\n",
    "            assert channels % num_head_channels == 0, (\n",
    "                f\"q,k,v channels {channels} is \"\n",
    "                f\"not divisible by num_head_channels {num_head_channels}\"\n",
    "            )\n",
    "            self.num_heads = channels // num_head_channels\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.norm = Normalize(channels)\n",
    "        self.qkv = nn.Conv1d(channels, channels * 3, 1)\n",
    "        self.attention = QKVAttentionLegacy(self.num_heads)\n",
    "\n",
    "        self.proj_out = zero_module(nn.Conv1d(channels, channels, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward(\n",
    "            x,\n",
    "        )\n",
    "\n",
    "    def _forward(self, x):\n",
    "        b, c, *spatial = x.shape\n",
    "        x = x.reshape(b, c, -1)\n",
    "        qkv = self.qkv(self.norm(x))\n",
    "        h = self.attention(qkv)\n",
    "        h = self.proj_out(h)\n",
    "        return (x + h).reshape(b, c, *spatial)\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    \"\"\"\n",
    "    A downsampling layer with an optional convolution.\n",
    "    :param channels: channels in the inputs and outputs.\n",
    "    :param use_conv: a bool determining if a convolution is applied.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, use_conv, out_channels=None, padding=1):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        if use_conv:\n",
    "            self.op = nn.Conv1d(\n",
    "                self.channels, self.out_channels, 3, stride=2, padding=padding\n",
    "            )#TODO:Mudar\n",
    "        else:\n",
    "            assert self.channels == self.out_channels\n",
    "            self.op = nn.AvgPool1d(kernel_size=2, stride=2)#TODO: Mudar\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        return self.op(x)\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    \"\"\"\n",
    "    An upsampling layer with an optional convolution.\n",
    "    :param channels: channels in the inputs and outputs.\n",
    "    :param use_conv: a bool determining if a convolution is applied.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, use_conv, out_channels=None, padding=1):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        if use_conv:\n",
    "            self.conv = nn.Conv1d(\n",
    "                self.channels, self.out_channels, 3, padding=padding\n",
    "            )#TODO:Mudar\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        if self.use_conv:\n",
    "            x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResBlock(TimestepBlock):  # Ensure ResBlock inherits from TimestepBlock\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        emb_channels,\n",
    "        dropout,\n",
    "        out_channels=None,\n",
    "        use_conv=False,\n",
    "        use_scale_shift_norm=False,\n",
    "        up=False,\n",
    "        down=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.emb_channels = emb_channels\n",
    "        self.dropout = dropout\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        self.use_scale_shift_norm = use_scale_shift_norm\n",
    "\n",
    "        self.in_layers = nn.Sequential(\n",
    "            Normalize(channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv1d(channels, self.out_channels, 3, padding=1),\n",
    "        )\n",
    "\n",
    "        self.updown = up or down\n",
    "\n",
    "        if up:\n",
    "            self.h_upd = Upsample(channels, False)\n",
    "            self.x_upd = Upsample(channels, False)\n",
    "        elif down:\n",
    "            self.h_upd = Downsample(channels, False)\n",
    "            self.x_upd = Downsample(channels, False)\n",
    "        else:\n",
    "            self.h_upd = self.x_upd = nn.Identity()\n",
    "\n",
    "        self.emb_layers = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(\n",
    "                emb_channels,\n",
    "                2 * self.out_channels\n",
    "                if use_scale_shift_norm\n",
    "                else self.out_channels,\n",
    "            ),\n",
    "        )\n",
    "        self.out_layers = nn.Sequential(\n",
    "            Normalize(self.out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            zero_module(\n",
    "                nn.Conv1d(self.out_channels, self.out_channels, 3, padding=1)\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if self.out_channels == channels:\n",
    "            self.skip_connection = nn.Identity()\n",
    "        elif use_conv:\n",
    "            self.skip_connection = nn.Conv1d(\n",
    "                channels, self.out_channels, kernel_size=1\n",
    "            )\n",
    "        else:\n",
    "            self.skip_connection = nn.Conv1d(channels, self.out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        \"\"\"\n",
    "        Apply the ResBlock to `x` with timestep embeddings `emb`.\n",
    "        \"\"\"\n",
    "        if self.updown:\n",
    "            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n",
    "            h = in_rest(x)\n",
    "            h = self.h_upd(h)\n",
    "            x = self.x_upd(x)\n",
    "            h = in_conv(h)\n",
    "        else:\n",
    "            h = self.in_layers(x)\n",
    "\n",
    "        emb_out = self.emb_layers(emb).type(h.dtype)\n",
    "        while len(emb_out.shape) < len(h.shape):\n",
    "            emb_out = emb_out[..., None]\n",
    "\n",
    "        if self.use_scale_shift_norm:\n",
    "            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n",
    "            scale, shift = torch.chunk(emb_out, 2, dim=1)\n",
    "            h = out_norm(h) * (1 + scale) + shift\n",
    "            h = out_rest(h)\n",
    "        else:\n",
    "            h = h + emb_out\n",
    "            h = self.out_layers(h)\n",
    "\n",
    "        return self.skip_connection(x) + h\n",
    "\n",
    "class UNetModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size=32,\n",
    "        in_channels=1,\n",
    "        model_channels=32,\n",
    "        out_channels=1,\n",
    "        num_res_blocks=2,\n",
    "        attention_resolutions=[16, 8],\n",
    "        dropout=0.1,\n",
    "        channel_mult=(2, 4, 8),\n",
    "        num_heads=4,\n",
    "        use_scale_shift_norm=False,\n",
    "        resblock_updown=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.in_channels = in_channels\n",
    "        self.model_channels = model_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.attention_resolutions = attention_resolutions\n",
    "        self.dropout = dropout\n",
    "        self.channel_mult = channel_mult\n",
    "        self.num_heads = num_heads\n",
    "        self.use_scale_shift_norm = use_scale_shift_norm\n",
    "        self.resblock_updown = resblock_updown\n",
    "\n",
    "        time_embed_dim = model_channels * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(model_channels, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_embed_dim, time_embed_dim),\n",
    "        )\n",
    "\n",
    "        self.input_blocks = nn.ModuleList([\n",
    "            TimestepEmbedSequential(nn.Conv1d(in_channels, model_channels, 3, padding=1))\n",
    "        ])\n",
    "        input_block_chans = [model_channels]\n",
    "        ch = model_channels\n",
    "        ds = 1\n",
    "\n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            for _ in range(num_res_blocks):\n",
    "                layers = [ResBlock(ch, time_embed_dim, dropout, out_channels=mult * model_channels, use_scale_shift_norm=use_scale_shift_norm)]\n",
    "                ch = mult * model_channels\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(AttentionBlock(ch, num_heads=num_heads))\n",
    "                self.input_blocks.append(TimestepEmbedSequential(*layers))\n",
    "                input_block_chans.append(ch)\n",
    "            if level != len(channel_mult) - 1:\n",
    "                out_ch = ch\n",
    "                self.input_blocks.append(TimestepEmbedSequential(Downsample(ch, True, out_channels=out_ch)))\n",
    "                ch = out_ch\n",
    "                input_block_chans.append(ch)\n",
    "                ds *= 2\n",
    "\n",
    "        self.middle_block = TimestepEmbedSequential(\n",
    "            ResBlock(ch, time_embed_dim, dropout, use_scale_shift_norm=use_scale_shift_norm),\n",
    "            AttentionBlock(ch, num_heads=num_heads),\n",
    "            ResBlock(ch, time_embed_dim, dropout, use_scale_shift_norm=use_scale_shift_norm),\n",
    "        )\n",
    "\n",
    "        self.output_blocks = nn.ModuleList([])\n",
    "        for level, mult in list(enumerate(channel_mult))[::-1]:\n",
    "            for i in range(num_res_blocks + 1):\n",
    "                ich = input_block_chans.pop()\n",
    "                layers = [ResBlock(ch + ich, time_embed_dim, dropout, out_channels=model_channels * mult, use_scale_shift_norm=use_scale_shift_norm)]\n",
    "                ch = model_channels * mult\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(AttentionBlock(ch, num_heads=num_heads))\n",
    "                if level and i == num_res_blocks:\n",
    "                    out_ch = ch\n",
    "                    layers.append(Upsample(ch, True, out_channels=out_ch))\n",
    "                    ds //= 2\n",
    "                self.output_blocks.append(TimestepEmbedSequential(*layers))\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            Normalize(ch),\n",
    "            nn.SiLU(),\n",
    "            zero_module(nn.Conv1d(ch, out_channels, 3, padding=1)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, timesteps=None, context=None, y=None):\n",
    "        assert timesteps is not None, \"timesteps must be provided\"\n",
    "        hs = []\n",
    "        t_emb = timestep_embedding(timesteps, self.model_channels, repeat_only=False)\n",
    "        emb = self.time_embed(t_emb)\n",
    "\n",
    "        h = x\n",
    "        for module in self.input_blocks:\n",
    "            h = module(h, emb, context)\n",
    "            hs.append(h)\n",
    "        h = self.middle_block(h, emb, context)\n",
    "\n",
    "        for module in self.output_blocks:\n",
    "            h_pop = hs.pop()\n",
    "            if h.shape[2] != h_pop.shape[2]:\n",
    "                h_pop = F.interpolate(h_pop, size=h.shape[2], mode='nearest')\n",
    "            h = torch.cat([h, h_pop], dim=1)\n",
    "            h = module(h, emb, context)\n",
    "\n",
    "        return self.out(h)\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "from inspect import isfunction\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "\n",
    "def noise_like(shape, device, repeat=False):\n",
    "    repeat_noise = lambda: torch.randn((1, *shape[1:]), device=device).repeat(\n",
    "        shape[0], *((1,) * (len(shape) - 1))\n",
    "    )\n",
    "    noise = lambda: torch.randn(shape, device=device)\n",
    "    return repeat_noise() if repeat else noise()\n",
    "\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "\n",
    "def make_beta_schedule(\n",
    "    schedule, n_timestep, linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3\n",
    "):\n",
    "    if schedule == \"linear\":\n",
    "        betas = (\n",
    "            torch.linspace(\n",
    "                linear_start**0.5,\n",
    "                linear_end**0.5,\n",
    "                n_timestep,\n",
    "                dtype=torch.float64,\n",
    "            )\n",
    "            ** 2\n",
    "        )\n",
    "\n",
    "    elif schedule == \"cosine\":\n",
    "        timesteps = (\n",
    "            torch.arange(n_timestep + 1, dtype=torch.float64) / n_timestep\n",
    "            + cosine_s\n",
    "        )\n",
    "        alphas = timesteps / (1 + cosine_s) * np.pi / 2\n",
    "        alphas = torch.cos(alphas).pow(2)\n",
    "        alphas = alphas / alphas[0]\n",
    "        betas = 1 - alphas[1:] / alphas[:-1]\n",
    "        betas = np.clip(betas, a_min=0, a_max=0.999)\n",
    "\n",
    "    elif schedule == \"sqrt_linear\":\n",
    "        betas = torch.linspace(\n",
    "            linear_start, linear_end, n_timestep, dtype=torch.float64\n",
    "        )\n",
    "    elif schedule == \"sqrt\":\n",
    "        betas = (\n",
    "            torch.linspace(\n",
    "                linear_start, linear_end, n_timestep, dtype=torch.float64\n",
    "            )\n",
    "            ** 0.5\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"schedule '{schedule}' unknown.\")\n",
    "    return betas.numpy()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "class DDIM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        unet_config,\n",
    "        timesteps=1000,\n",
    "        ddim_steps=50,\n",
    "        beta_schedule=\"linear\",\n",
    "        clip_denoised=False,\n",
    "        linear_start=1e-4,\n",
    "        linear_end=2e-2,\n",
    "        original_elbo_weight=0.0,\n",
    "        parameterization=\"eps\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert parameterization in [\"eps\", \"x0\"], 'Only \"eps\" and \"x0\" are supported.'\n",
    "        self.parameterization = parameterization\n",
    "        self.model = UNetModel(**unet_config.get(\"params\", {}))\n",
    "        self.clip_denoised = clip_denoised\n",
    "        self.original_elbo_weight = original_elbo_weight\n",
    "        self.ddim_steps = ddim_steps\n",
    "        self.register_schedule(beta_schedule, timesteps, linear_start, linear_end)\n",
    "\n",
    "    def register_schedule(self, beta_schedule, timesteps, linear_start, linear_end):\n",
    "        betas = np.linspace(linear_start, linear_end, timesteps, dtype=np.float64)\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = np.cumprod(alphas, axis=0)\n",
    "        alphas_cumprod_prev = np.append(1.0, alphas_cumprod[:-1])\n",
    "\n",
    "        self.num_timesteps = int(timesteps)\n",
    "        to_torch = partial(torch.tensor, dtype=torch.float32)\n",
    "\n",
    "        self.register_buffer(\"betas\", to_torch(betas))\n",
    "        self.register_buffer(\"alphas_cumprod\", to_torch(alphas_cumprod))\n",
    "        self.register_buffer(\"alphas_cumprod_prev\", to_torch(alphas_cumprod_prev))\n",
    "        self.register_buffer(\"sqrt_recip_alphas_cumprod\", to_torch(np.sqrt(1.0 / alphas_cumprod)))\n",
    "        self.register_buffer(\"sqrt_one_minus_alphas_cumprod\", to_torch(np.sqrt(1.0 - alphas_cumprod)))\n",
    "\n",
    "    def ddim_sample(self, x, t, eta=0.0):\n",
    "        model_output = self.model(x, t)\n",
    "\n",
    "        if self.parameterization == \"eps\":\n",
    "            pred_x0 = extract(self.sqrt_recip_alphas_cumprod, t, x.shape) * x - \\\n",
    "                        extract(self.sqrt_one_minus_alphas_cumprod, t, x.shape) * model_output\n",
    "        else:\n",
    "            pred_x0 = model_output\n",
    "\n",
    "        if self.clip_denoised:\n",
    "            pred_x0 = torch.clamp(pred_x0, -1.0, 1.0)\n",
    "\n",
    "        sigma = eta * (1 - extract(self.alphas_cumprod, t, x.shape)).sqrt()\n",
    "        noise = torch.randn_like(x)\n",
    "\n",
    "        return pred_x0 + sigma * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop_ddim(self, shape, eta=0.0):\n",
    "        device = self.betas.device\n",
    "        img = torch.randn(shape, device=device)\n",
    "\n",
    "        for i in tqdm(reversed(range(0, self.ddim_steps)), desc=\"DDIM sampling\"):\n",
    "            t = torch.full((shape[0],), i, device=device, dtype=torch.long)\n",
    "            img = self.ddim_sample(img, t, eta)\n",
    "\n",
    "        return img\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size=16, eta=0.0):\n",
    "        image_size = self.model.image_size\n",
    "        channels = self.model.in_channels\n",
    "        return self.p_sample_loop_ddim((batch_size, channels, image_size), eta=eta)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        return self.model(x, t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "03202c6c-557a-4134-b444-6b6da3a04657",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|███████████████████████████████████████████████████████████████████████| 97/97 [00:17<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Avg Train Loss = 7.1732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|███████████████████████████████████████████████████████████████████████| 97/97 [00:16<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Avg Train Loss = 4.2406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|███████████████████████████████████████████████████████████████████████| 97/97 [00:16<00:00,  6.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Avg Train Loss = 2.6291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|███████████████████████████████████████████████████████████████████████| 97/97 [00:15<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Avg Train Loss = 1.6482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|███████████████████████████████████████████████████████████████████████| 97/97 [00:16<00:00,  6.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Avg Train Loss = 1.0335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import optim # Import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# 데이터셋 로드\n",
    "batch_size = 32\n",
    "\n",
    "train_data_tensor = torch.tensor(train_df.values, dtype=torch.float32)  # (192, 10)\n",
    "train_dataset = TensorDataset(train_data_tensor)  # 레이블 없음\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 학습 함수\n",
    "def train_ddim_model(model, train_loader, num_epochs=20, learning_rate=1e-4, device='cuda'):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            x = batch[0].to(device)  # 레이블이 없으므로 x만 가져옴\n",
    "            # x = x.unsqueeze(1)  # Remove this line - it's causing the error\n",
    "\n",
    "            # Reshape x to have the expected shape (batch_size, in_channels, image_size)\n",
    "            x = x.view(x.shape[0], unet_config[\"params\"][\"in_channels\"], -1)\n",
    "\n",
    "            t = torch.randint(0, model.num_timesteps, (x.shape[0],), device=device).long()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(model(x, t), x)  # 모델의 출력과 입력 비교 (재구성 손실)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}: Avg Train Loss = {avg_loss:.4f}\")\n",
    "\n",
    "# 모델 초기화\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 모델 설정 변경\n",
    "unet_config = {\n",
    "    \"params\": {\n",
    "        \"image_size\": 50,           # 입력 길이\n",
    "        \"in_channels\": 1,           # 채널 수\n",
    "        \"model_channels\": 32,\n",
    "        \"out_channels\": 1,\n",
    "        \"num_res_blocks\": 2,\n",
    "        \"attention_resolutions\": [5],\n",
    "        \"dropout\": 0.1,\n",
    "        \"channel_mult\": (2, 4, 8),\n",
    "        \"num_heads\": 4,\n",
    "        \"use_scale_shift_norm\": False,\n",
    "        \"resblock_updown\": True,\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "ddim_model = DDIM(unet_config=unet_config, timesteps=2000, ddim_steps=1000, parameterization='eps').to(device)\n",
    "\n",
    "# 학습 시작\n",
    "train_ddim_model(ddim_model, train_loader, num_epochs=5, learning_rate=1e-4, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "41872803-7f50-41a4-b3ff-35395cd9ced6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9240, 52)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d1bbdbcb-acd5-45d0-a3b2-13ea246a536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df에 subject 컬럼이 있다고 가정\n",
    "test_data_tensor = torch.tensor(test_df.drop(columns=[\"Hypertension\", \"subject\"]).values, dtype=torch.float32).unsqueeze(1)\n",
    "test_labels_tensor = torch.tensor(test_df[\"Hypertension\"].values, dtype=torch.float32).unsqueeze(-1)\n",
    "subject_tensor = torch.tensor(test_df[\"subject\"].values, dtype=torch.int32).unsqueeze(-1)\n",
    "\n",
    "# subject 추가 포함\n",
    "test_dataset = TensorDataset(test_data_tensor, test_labels_tensor, subject_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "cfcf85d6-c9e3-40ac-8639-82eb2f319b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "✅ 최종 테스트 손실: 1.2137\n",
      "\n",
      "📋 테스트 결과 상위 5개:\n",
      "   Subject  True_Label  Hypertension  Reconstruction_Error\n",
      "0        1         1.0           1.0              0.355291\n",
      "1        1         1.0           1.0              0.293586\n",
      "2        1         1.0           1.0              0.327268\n",
      "3        1         1.0           1.0              0.392839\n",
      "4        1         1.0           1.0              0.374394\n",
      "\n",
      "✅ Test Loss: 1.2137\n"
     ]
    }
   ],
   "source": [
    "def test_ddpm(ddpm, test_loader, device):\n",
    "    ddpm.eval()\n",
    "    total_loss = 0\n",
    "    num_samples = 0\n",
    "    all_reconstruction_errors = []\n",
    "    true_labels = []\n",
    "    subject_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x, labels, subjects = batch\n",
    "            x = x.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            t = torch.zeros(x.size(0), dtype=torch.long).to(device)  # time step 고정\n",
    "            reconstructed_x = ddpm(x, t)\n",
    "\n",
    "            # shape mismatch 방지\n",
    "            min_len = min(x.shape[2], reconstructed_x.shape[2])\n",
    "            x = x[:, :, :min_len]\n",
    "            reconstructed_x = reconstructed_x[:, :, :min_len]\n",
    "\n",
    "            reconstruction_error = F.mse_loss(reconstructed_x, x, reduction='none').mean(dim=[1, 2])\n",
    "            loss = reconstruction_error.mean()\n",
    "\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            num_samples += x.size(0)\n",
    "\n",
    "            all_reconstruction_errors.extend(reconstruction_error.cpu().numpy().tolist())\n",
    "            true_labels.extend(labels.cpu().numpy().flatten().tolist())\n",
    "            subject_ids.extend(subjects.cpu().numpy().flatten().tolist())\n",
    "\n",
    "    avg_loss = total_loss / num_samples\n",
    "    anomaly_df = pd.DataFrame({\n",
    "        \"Subject\": subject_ids,\n",
    "        \"True_Label\": true_labels,\n",
    "        \"Hypertension\": true_labels,\n",
    "        \"Reconstruction_Error\": all_reconstruction_errors\n",
    "    })\n",
    "\n",
    "    print(f\"✅ 최종 테스트 손실: {avg_loss:.4f}\")\n",
    "    return anomaly_df, avg_loss, None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 실행 코드 블록\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # 모델 초기화 (실제 모델 구조에 맞게 수정 필요)\n",
    "    ddim = DDIM(unet_config=unet_config).to(device)\n",
    "\n",
    "    # 테스트 실행\n",
    "    anomaly_df, test_loss, _ = test_ddpm(ddim, test_loader, device)\n",
    "\n",
    "    # 결과 출력\n",
    "    print(\"\\n📋 테스트 결과 상위 5개:\")\n",
    "    print(anomaly_df.head())\n",
    "\n",
    "\n",
    "    print(f\"\\n✅ Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "cf6f2519-129f-4fe9-a2d9-9abdf6cd31a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>True_Label</th>\n",
       "      <th>Hypertension</th>\n",
       "      <th>Reconstruction_Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.355291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.293586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.327268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.392839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.374394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9235</th>\n",
       "      <td>56</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.200369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9236</th>\n",
       "      <td>56</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.126819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9237</th>\n",
       "      <td>56</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.177299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9238</th>\n",
       "      <td>56</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.385038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9239</th>\n",
       "      <td>56</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.398050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9240 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Subject  True_Label  Hypertension  Reconstruction_Error\n",
       "0           1         1.0           1.0              0.355291\n",
       "1           1         1.0           1.0              0.293586\n",
       "2           1         1.0           1.0              0.327268\n",
       "3           1         1.0           1.0              0.392839\n",
       "4           1         1.0           1.0              0.374394\n",
       "...       ...         ...           ...                   ...\n",
       "9235       56         1.0           1.0              1.200369\n",
       "9236       56         1.0           1.0              1.126819\n",
       "9237       56         1.0           1.0              1.177299\n",
       "9238       56         1.0           1.0              1.385038\n",
       "9239       56         1.0           1.0              1.398050\n",
       "\n",
       "[9240 rows x 4 columns]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomaly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "38f4496b-1bb0-4674-b325-044d134dfbb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True_Label\n",
       "1.0    4620\n",
       "2.0    3520\n",
       "0.0    1100\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomaly_df['True_Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "9db5dc76-1da9-4ab5-8bd1-868de67f2b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 스타일 설정 (선택 사항)\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 박스플롯 그리기\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(data=anomaly_df, x=\"True_Label\", y=\"Reconstruction_Error\")\n",
    "\n",
    "# 라벨 및 타이틀 설정\n",
    "plt.title(\"Reconstruction Error by True Label\")\n",
    "plt.xlabel(\"True Label (0: Normal, 1: Hypertension)\")\n",
    "plt.ylabel(\"Reconstruction Error\")\n",
    "\n",
    "# y축 로그스케일이 필요한 경우\n",
    "# plt.yscale(\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568bc248-3fa8-4e35-84bb-4991d0c0394a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e42df52-b739-4450-86ce-1cf79266b8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6125949-f030-4de8-b39c-270704ea0c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8b072c-1974-4311-933d-437716af4760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61c121f-a6ad-4903-a75e-e8d48c413704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baea1d6-f6f5-4329-968a-57bb5055b9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11e9a9b-2c84-4621-82f7-3c4704bca410",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
