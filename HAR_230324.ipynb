{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c7dc2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from scipy import ndimage\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D,Conv2D\n",
    "from keras.layers.convolutional import MaxPooling1D,MaxPooling2D\n",
    "from keras.layers import BatchNormalization,ReLU,GlobalAveragePooling1D,MaxPooling1D,LSTM,TimeDistributed,GlobalAveragePooling2D\n",
    "#from keras.utils import to_categorical\n",
    "from tensorflow.keras.models import save_model,load_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model,save_model,load_model\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import expand_dims\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b555b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filepath):\n",
    "  dataframe=read_csv(filepath,header=None,delim_whitespace=True)\n",
    "  return dataframe.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d957d92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_group(filenames,prefix=''):\n",
    "  loaded=list()\n",
    "  for name in filenames:\n",
    "    data=load_file(prefix+name)\n",
    "    loaded.append(data)\n",
    "  loaded=dstack(loaded)\n",
    "  return loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba922457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_group(group,prefix=''):\n",
    "  filepath=prefix+group+'/Inertial Signals/'\n",
    "  filenames=list()\n",
    "#  filenames+=['total_acc_x_'+group+'.txt','total_acc_y_'+group+'.txt','total_acc_z_'+group+'.txt']\n",
    "  filenames+=['body_acc_x_'+group+'.txt','body_acc_y_'+group+'.txt','body_acc_z_'+group+'.txt']\n",
    "# filenames+=['body_gyro_x_'+group+'.txt','body_gyro_y_'+group+'.txt','body_gyro_z_'+group+'.txt']\n",
    "  X=load_group(filenames,filepath)\n",
    "  y=load_file(prefix+group+'/y_'+group+'.txt')\n",
    "  return (X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5f32667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(prefix=''):\n",
    "  trainX,trainy=load_dataset_group('train',prefix+'F:/HAR/UCI/')\n",
    "  print(trainX.shape,trainy.shape)\n",
    "  testX,testy=load_dataset_group('test',prefix+'F:/HAR/UCI/')\n",
    "  print(testX.shape,testy.shape)\n",
    "  trainy=trainy-1\n",
    "  testy=testy-1\n",
    "  #trainy=tf.compat.v1.keras.utils.to_categorical(trainy)\n",
    "  #testy=tf.compat.v1.keras.utils.to_categorical(testy)\n",
    "  print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "  return trainX, trainy, testX, testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "467a26ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale_data(trainX, testX):\n",
    "\t# remove overlap\n",
    "\tcut = int(trainX.shape[1] / 2)\n",
    "\tlongX = trainX[:, -cut:, :]\n",
    "\t# flatten windows\n",
    "\tlongX = longX.reshape((longX.shape[0] * longX.shape[1], longX.shape[2]))\n",
    "\t# flatten train and test\n",
    "\tflatTrainX = trainX.reshape((trainX.shape[0] * trainX.shape[1], trainX.shape[2]))\n",
    "\tflatTestX = testX.reshape((testX.shape[0] * testX.shape[1], testX.shape[2]))\n",
    "\t# standardize\n",
    "\n",
    "\ts = StandardScaler()\n",
    "\t\t# fit on training data\n",
    "\ts.fit(longX)\n",
    "\t\t# apply to training and test data\n",
    "\tlongX = s.transform(longX)\n",
    "\tflatTrainX = s.transform(flatTrainX)\n",
    "\tflatTestX = s.transform(flatTestX)\n",
    "\t# reshape\n",
    "\tflatTrainX = flatTrainX.reshape((trainX.shape))\n",
    "\tflatTestX = flatTestX.reshape((testX.shape))\n",
    "\treturn flatTrainX, flatTestX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cd49e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 128, 3) (7352, 1)\n",
      "(2947, 128, 3) (2947, 1)\n",
      "(7352, 128, 3) (7352, 1) (2947, 128, 3) (2947, 1)\n"
     ]
    }
   ],
   "source": [
    "trainX,trainy,testX,testy=load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd7aa31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
