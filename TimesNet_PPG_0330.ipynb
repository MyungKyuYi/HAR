{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class DataEmbedding(nn.Module):\n",
        "    def __init__(self, feats_in: int, feats_model: int, embedding_type: str = 'fixed', frequency: str = 'h', dropout: float = 0.1):\n",
        "        super(DataEmbedding, self).__init__()\n",
        "        self.value_embedding = nn.Linear(feats_in, feats_model)  # [1] -> [d_model]\n",
        "        self.position_embedding = PositionalEmbedding(feats_model=feats_model)\n",
        "        self.temporal_embedding = TemporalEmbedding(feats_model=feats_model, embedding_type=embedding_type, frequency=frequency)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, x_mark: torch.Tensor) -> torch.Tensor:\n",
        "        # x shape: [B, T, 1]\n",
        "        x_emb = self.value_embedding(x)  # [B, T, 1] -> [B, T, d_model]\n",
        "        x_emb = x_emb + self.position_embedding(x)  # [B, T, d_model]\n",
        "\n",
        "        if x_mark is not None:\n",
        "            x_emb += self.temporal_embedding(x_mark)\n",
        "\n",
        "        return self.dropout(x_emb)\n",
        "\n",
        "class TemporalEmbedding(nn.Module):\n",
        "    def __init__(self, feats_model: int, embedding_type: str = 'timeF', frequency: str = 'h'):\n",
        "        super(TemporalEmbedding, self).__init__()\n",
        "\n",
        "        # 시간 주기별 임베딩 차원\n",
        "        Embed = nn.Embedding\n",
        "\n",
        "        # 시간 주기 구성: 시간 단위에 따라 다르게 처리\n",
        "        if frequency == 'h':\n",
        "            self.minute_embed = None\n",
        "            self.hour_embed = Embed(24, feats_model)       # 0~23\n",
        "            self.weekday_embed = Embed(7, feats_model)     # 0~6\n",
        "            self.day_embed = Embed(32, feats_model)        # 1~31\n",
        "            self.month_embed = Embed(13, feats_model)      # 1~12\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Unsupported frequency: {frequency}\")\n",
        "\n",
        "    def forward(self, x_mark: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        x_mark : Tensor of shape [B, T, 4] → columns: hour, weekday, day, month\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Tensor of shape [B, T, d_model]\n",
        "        \"\"\"\n",
        "        hour_x = self.hour_embed(x_mark[:, :, 0].long())\n",
        "        weekday_x = self.weekday_embed(x_mark[:, :, 1].long())\n",
        "        day_x = self.day_embed(x_mark[:, :, 2].long())\n",
        "        month_x = self.month_embed(x_mark[:, :, 3].long())\n",
        "\n",
        "        return hour_x + weekday_x + day_x + month_x\n",
        "\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, feats_model: int, max_len: int = 5000):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "\n",
        "        # 위치 인덱스 (0, 1, 2, ..., max_len - 1)\n",
        "        pe = torch.zeros(max_len, feats_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, feats_model, 2).float() * (-np.log(10000.0) / feats_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : Tensor of shape [batch_size, seq_len, feats_model]\n",
        "        \"\"\"\n",
        "        return self.pe[:, :x.size(1), :]\n",
        "\n",
        "\n",
        "class Inception_Block_V1(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_kernels=6, init_weight=True):\n",
        "        super(Inception_Block_V1, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_kernels = num_kernels\n",
        "        kernels = []\n",
        "        for i in range(self.num_kernels):\n",
        "            kernels.append(nn.Conv2d(in_channels, out_channels, kernel_size=2 * i + 1, padding=i))\n",
        "        self.kernels = nn.ModuleList(kernels)\n",
        "        if init_weight:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res_list = []\n",
        "        for i in range(self.num_kernels):\n",
        "            res_list.append(self.kernels[i](x))\n",
        "        res = torch.stack(res_list, dim=-1).mean(-1)\n",
        "        return res\n",
        "\n",
        "\n",
        "class Inception_Block_V2(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_kernels=6, init_weight=True):\n",
        "        super(Inception_Block_V2, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_kernels = num_kernels\n",
        "        kernels = []\n",
        "        for i in range(self.num_kernels // 2):\n",
        "            kernels.append(nn.Conv2d(in_channels, out_channels, kernel_size=[1, 2 * i + 3], padding=[0, i + 1]))\n",
        "            kernels.append(nn.Conv2d(in_channels, out_channels, kernel_size=[2 * i + 3, 1], padding=[i + 1, 0]))\n",
        "        kernels.append(nn.Conv2d(in_channels, out_channels, kernel_size=1))\n",
        "        self.kernels = nn.ModuleList(kernels)\n",
        "        if init_weight:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res_list = []\n",
        "        for i in range(self.num_kernels + 1):\n",
        "            res_list.append(self.kernels[i](x))\n",
        "        res = torch.stack(res_list, dim=-1).mean(-1)\n",
        "        return res\n",
        "\n"
      ],
      "metadata": {
        "id": "3Kr_oY14WmWc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.fft\n",
        "\n",
        "def FFT_for_Period(x, k=2):\n",
        "    # [B, T, C]\n",
        "    xf = torch.fft.rfft(x, dim=1)\n",
        "    # find period by amplitudes\n",
        "    frequency_list = abs(xf).mean(0).mean(-1)\n",
        "    frequency_list[0] = 0\n",
        "    _, top_list = torch.topk(frequency_list, k)\n",
        "    top_list = top_list.detach().cpu().numpy()\n",
        "    period = x.shape[1] // top_list\n",
        "    return period, abs(xf).mean(-1)[:, top_list]\n",
        "\n",
        "\n",
        "class TimesBlock(nn.Module):\n",
        "    def __init__(self, configs):\n",
        "        super(TimesBlock, self).__init__()\n",
        "        self.seq_len = configs.seq_len\n",
        "        self.pred_len = configs.pred_len\n",
        "        self.k = configs.top_k\n",
        "        # parameter-efficient design\n",
        "        self.conv = nn.Sequential(\n",
        "            Inception_Block_V1(configs.d_model, configs.d_ff,\n",
        "                               num_kernels=configs.num_kernels),\n",
        "            nn.GELU(),\n",
        "            Inception_Block_V1(configs.d_ff, configs.d_model,\n",
        "                               num_kernels=configs.num_kernels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, N = x.size()\n",
        "        period_list, period_weight = FFT_for_Period(x, self.k)\n",
        "\n",
        "        res = []\n",
        "        for i in range(self.k):\n",
        "            period = period_list[i]\n",
        "            # padding\n",
        "            if (self.seq_len + self.pred_len) % period != 0:\n",
        "                length = (\n",
        "                                 ((self.seq_len + self.pred_len) // period) + 1) * period\n",
        "                padding = torch.zeros([x.shape[0], (length - (self.seq_len + self.pred_len)), x.shape[2]]).to(x.device)\n",
        "                out = torch.cat([x, padding], dim=1)\n",
        "            else:\n",
        "                length = (self.seq_len + self.pred_len)\n",
        "                out = x\n",
        "            # reshape\n",
        "            out = out.reshape(B, length // period, period,\n",
        "                              N).permute(0, 3, 1, 2).contiguous()\n",
        "            # 2D conv: from 1d Variation to 2d Variation\n",
        "            out = self.conv(out)\n",
        "            # reshape back\n",
        "            out = out.permute(0, 2, 3, 1).reshape(B, -1, N)\n",
        "            res.append(out[:, :(self.seq_len + self.pred_len), :])\n",
        "        res = torch.stack(res, dim=-1)\n",
        "        # adaptive aggregation\n",
        "        period_weight = F.softmax(period_weight, dim=1)\n",
        "        period_weight = period_weight.unsqueeze(\n",
        "            1).unsqueeze(1).repeat(1, T, N, 1)\n",
        "        res = torch.sum(res * period_weight, -1)\n",
        "        # residual connection\n",
        "        res = res + x\n",
        "        return res\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, configs):\n",
        "        super(Model, self).__init__()\n",
        "        self.configs = configs\n",
        "        self.task_name = configs.task_name\n",
        "        self.seq_len = configs.seq_len\n",
        "        self.label_len = configs.label_len\n",
        "        self.pred_len = configs.pred_len\n",
        "        self.model = nn.ModuleList([TimesBlock(configs)\n",
        "                                  for _ in range(configs.e_layers)])\n",
        "        self.enc_embedding = DataEmbedding(configs.enc_in, configs.d_model, configs.embed, configs.freq,\n",
        "                                         configs.dropout)\n",
        "        self.layer = configs.e_layers\n",
        "        self.layer_norm = nn.LayerNorm(configs.d_model)\n",
        "\n",
        "        # Always initialize projection layer for regression\n",
        "        self.projection = nn.Linear(configs.d_model, configs.c_out, bias=True)\n",
        "\n",
        "    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
        "        # Normalization\n",
        "        means = x_enc.mean(1, keepdim=True).detach()\n",
        "        x_enc = x_enc - means\n",
        "        stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
        "        x_enc /= stdev\n",
        "\n",
        "        # Embedding\n",
        "        enc_out = self.enc_embedding(x_enc, x_mark_enc)  # [B,T,C]\n",
        "\n",
        "        # TimesNet blocks\n",
        "        for i in range(self.layer):\n",
        "            enc_out = self.layer_norm(self.model[i](enc_out))\n",
        "\n",
        "        # Projection\n",
        "        dec_out = self.projection(enc_out)\n",
        "\n",
        "        # De-normalization\n",
        "        dec_out = dec_out * stdev[:, 0, :].unsqueeze(1).repeat(1, self.seq_len, 1)\n",
        "        dec_out = dec_out + means[:, 0, :].unsqueeze(1).repeat(1, self.seq_len, 1)\n",
        "\n",
        "        return dec_out\n",
        "\n",
        "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n",
        "        dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
        "        return dec_out.mean(dim=1)  # Average over time dimension to get [B, 2]\n"
      ],
      "metadata": {
        "id": "bW4h4ZZyWbOl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# TimesNet 설정 클래스\n",
        "\n",
        "def get_config(seq_len, pred_len, input_dim, output_dim):\n",
        "    class Config:\n",
        "        def __init__(self):\n",
        "            self.task_name = 'regression'\n",
        "            self.enc_in = input_dim  # Should be 1 for your case\n",
        "            self.c_out = output_dim  # Should be 2 (SBP and DBP)\n",
        "            self.d_model = 64\n",
        "            self.d_ff = 128\n",
        "            self.num_kernels = 6\n",
        "            self.top_k = 3\n",
        "            self.embed = 'timeF'\n",
        "            self.freq = 'h'\n",
        "            self.dropout = 0.1\n",
        "            self.seq_len = seq_len\n",
        "            self.label_len = 0\n",
        "            self.pred_len = pred_len\n",
        "            self.e_layers = 2\n",
        "\n",
        "    return Config()\n",
        "\n",
        "\n",
        "# 1. Load Data\n",
        "file_path = \"/content/drive/MyDrive/Colab Notebooks/combined_dataset.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# 2. Feature & Target Selection\n",
        "X_raw = df[[str(i) for i in range(1, 2101)]].dropna()\n",
        "y_sbp = df.loc[X_raw.index, 'Systolic Blood Pressure(mmHg)']\n",
        "y_dbp = df.loc[X_raw.index, 'Diastolic Blood Pressure(mmHg)']\n",
        "\n",
        "# 3. Sliding Window Transform\n",
        "def sliding_window_transform(X_raw, y_sbp, y_dbp, window_size, stride):\n",
        "    X_win, y_win = [], []\n",
        "    for i in range(len(X_raw)):\n",
        "        row = X_raw.iloc[i].values\n",
        "        for start in range(0, 2100 - window_size + 1, stride):\n",
        "            end = start + window_size\n",
        "            X_win.append(row[start:end])\n",
        "            y_win.append([y_sbp.iloc[i], y_dbp.iloc[i]])\n",
        "    return np.array(X_win), np.array(y_win)\n",
        "\n",
        "# Set window & stride\n",
        "window_size = 200\n",
        "stride = 200\n",
        "X_win, y_win = sliding_window_transform(X_raw, y_sbp, y_dbp, window_size, stride)\n",
        "\n",
        "# 4. Normalize\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_win.reshape(-1, window_size)).reshape(X_win.shape)\n",
        "\n",
        "# 5. Tensor Conversion (B, T, 1)\n",
        "# Convert the input data to float type before passing it to the model\n",
        "X_tensor = torch.tensor(X_scaled, dtype=torch.float32).unsqueeze(-1)  # (batch, seq_len, 1)\n",
        "y_tensor = torch.tensor(y_win, dtype=torch.float32)\n",
        "             # (batch, 2)\n",
        "# 6. Train/Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tensor, y_tensor, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 7. DataLoader 구성\n",
        "batch_size = 64\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "# Model 설정\n",
        "config = get_config(seq_len=window_size, pred_len=0, input_dim=1, output_dim=2)\n",
        "\n",
        "model = Model(config)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.L1Loss()\n",
        "\n",
        "# Train\n",
        "from tqdm import tqdm  # 진행률 표시를 위한 라이브러리\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "for epoch in range(20):\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/20 [TRAIN]', leave=False)\n",
        "\n",
        "    for xb, yb in train_bar:\n",
        "        pred = model(xb, None, None, None)\n",
        "        loss = loss_fn(pred, yb)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_sbp_mae = 0\n",
        "    val_dbp_mae = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        val_bar = tqdm(test_loader, desc=f'Epoch {epoch+1}/20 [VAL]', leave=False)\n",
        "        for xb, yb in val_bar:\n",
        "            pred = model(xb, None, None, None)\n",
        "            loss = loss_fn(pred, yb)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Calculate MAE for SBP and DBP separately\n",
        "            sbp_mae = mean_absolute_error(yb[:, 0].cpu().numpy(), pred[:, 0].cpu().numpy())\n",
        "            dbp_mae = mean_absolute_error(yb[:, 1].cpu().numpy(), pred[:, 1].cpu().numpy())\n",
        "            val_sbp_mae += sbp_mae\n",
        "            val_dbp_mae += dbp_mae\n",
        "\n",
        "    avg_val_loss = val_loss / len(test_loader)\n",
        "    avg_sbp_mae = val_sbp_mae / len(test_loader)\n",
        "    avg_dbp_mae = val_dbp_mae / len(test_loader)\n",
        "\n",
        "    # Print epoch summary\n",
        "    print(f'\\nEpoch {epoch+1}/20:')\n",
        "    print(f'Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}')\n",
        "    print(f'Val MAE - SBP: {avg_sbp_mae:.2f} mmHg | DBP: {avg_dbp_mae:.2f} mmHg')\n",
        "\n",
        "    # Save best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        print(f'Best model saved (Val Loss: {best_val_loss:.4f})')\n",
        "\n",
        "print('\\nTraining completed!')\n",
        "\n",
        "# Eval\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pred_test = model(X_test, None, None, None).mean(dim=1)\n",
        "    mae_sbp = mean_absolute_error(y_test[:, 0].numpy(), pred_test[:, 0].numpy())\n",
        "    mae_dbp = mean_absolute_error(y_test[:, 1].numpy(), pred_test[:, 1].numpy())\n",
        "\n",
        "print(f\"MAE (SBP): {mae_sbp:.2f} mmHg\")\n",
        "print(f\"MAE (DBP): {mae_dbp:.2f} mmHg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojf7cQkhWPH-",
        "outputId": "d5f05bc3-496f-4bb2-a145-7ed19adce676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 [TRAIN]:  25%|██▌       | 21/83 [11:49<35:43, 34.57s/it, loss=100.9807]"
          ]
        }
      ]
    }
  ]
}