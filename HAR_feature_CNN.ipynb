{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23f3f3b6-c019-4c17-998a-231913cc9a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_12768\\69108586.py:34: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  y_df = pd.read_csv(y_path, delim_whitespace=True, header=None, names=[\"Activity\"])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 2 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 95\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# 시퀀스 변환 적용\u001b[39;00m\n\u001b[0;32m     94\u001b[0m y_train_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_train)\n\u001b[1;32m---> 95\u001b[0m train_set \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mc_[X_train, y_train_array]\n\u001b[0;32m     96\u001b[0m y_test_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_test)\n\u001b[0;32m     97\u001b[0m test_set \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mc_[X_test, y_test_array]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\index_tricks.py:418\u001b[0m, in \u001b[0;36mAxisConcatenator.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;66;03m# concatenate could do cast, but that can be overriden:\u001b[39;00m\n\u001b[0;32m    415\u001b[0m     objs \u001b[38;5;241m=\u001b[39m [array(obj, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, subok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    416\u001b[0m                   ndmin\u001b[38;5;241m=\u001b[39mndmin, dtype\u001b[38;5;241m=\u001b[39mfinal_dtype) \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m objs]\n\u001b[1;32m--> 418\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcatenate(\u001b[38;5;28mtuple\u001b[39m(objs), axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m matrix:\n\u001b[0;32m    421\u001b[0m     oldndim \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mndim\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 2 dimension(s)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, Dropout, MaxPooling1D, Concatenate, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# 중력 가속도 (m/s²)\n",
    "GRAVITY = 9.81  \n",
    "\n",
    "# Inertial Signals 데이터 로드 함수\n",
    "def load_inertial_data():\n",
    "    \"\"\"\n",
    "    UCI HAR Inertial Signals 데이터를 로드하는 함수.\n",
    "    \n",
    "    출력:\n",
    "        X_data (numpy array): 가속도 원본 데이터 (128, 3)\n",
    "        feature_data (numpy array): 추가 피처 (128, 5)\n",
    "        y_df (DataFrame): 활동 라벨\n",
    "    \"\"\"\n",
    "    data_dir = \"E:/dataset/HAR/UCI-HAR/train/Inertial Signals/\"\n",
    "\n",
    "    # Inertial Signals 가속도 데이터 로드\n",
    "    acc_x = np.loadtxt(os.path.join(data_dir, \"body_acc_x_train.txt\"))\n",
    "    acc_y = np.loadtxt(os.path.join(data_dir, \"body_acc_y_train.txt\"))\n",
    "    acc_z = np.loadtxt(os.path.join(data_dir, \"body_acc_z_train.txt\"))\n",
    "\n",
    "    # 활동 라벨 로드\n",
    "    y_path = \"E:/dataset/HAR/UCI-HAR/train/y_train.txt\"\n",
    "    y_df = pd.read_csv(y_path, delim_whitespace=True, header=None, names=[\"Activity\"])\n",
    "\n",
    "    # 추가 피처 계산\n",
    "    Theta = np.degrees(np.arccos(acc_z / GRAVITY))\n",
    "    Theta_XY = np.degrees(np.arctan2(acc_y, acc_x))\n",
    "    Magnitude = np.sqrt(acc_x**2 + acc_y**2 + acc_z**2)\n",
    "    Azimuth = np.degrees(np.arctan2(acc_y, acc_x))\n",
    "    Elevation = np.degrees(np.arctan2(np.sqrt(acc_x**2 + acc_y**2), acc_z))\n",
    "    \n",
    "    feature_data = np.stack((Theta, Theta_XY, Magnitude, Azimuth, Elevation), axis=-1)\n",
    "    X_data = np.stack((acc_x, acc_y, acc_z), axis=-1)  # (샘플, 128, 3) 형태\n",
    "    \n",
    "    return X_data, feature_data, y_df\n",
    "\n",
    "# 데이터 로드\n",
    "X_data, feature_data, y_df = load_inertial_data()\n",
    "\n",
    "# 활동 라벨 매핑\n",
    "activity_labels = {\n",
    "    1: \"걷기\",\n",
    "    2: \"계단 오르기\",\n",
    "    3: \"계단 내리기\",\n",
    "    4: \"앉음\",\n",
    "    5: \"서있음\",\n",
    "    6: \"누움\"\n",
    "}\n",
    "y_df[\"Activity\"] = y_df[\"Activity\"].map(activity_labels)\n",
    "\n",
    "# 정적 행위만 필터링\n",
    "static_activities = [\"앉음\", \"서있음\", \"누움\"]\n",
    "filtered_indices = y_df[\"Activity\"].isin(static_activities)\n",
    "X_data = X_data[filtered_indices]\n",
    "feature_data = feature_data[filtered_indices]\n",
    "y_df = y_df[filtered_indices].reset_index(drop=True)\n",
    "\n",
    "# 라벨 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_df[\"Activity\"])\n",
    "\n",
    "# 데이터 정규화\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_data.reshape(-1, X_data.shape[-1])).reshape(X_data.shape)\n",
    "X_test_scaled = scaler.transform(X_data.reshape(-1, X_data.shape[-1])).reshape(X_data.shape)\n",
    "\n",
    "# 시퀀스 데이터 변환 함수\n",
    "def split_sequences(sequences, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix > len(sequences):\n",
    "            break\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# 시퀀스 변환 적용\n",
    "y_train_array = np.array(y_train)\n",
    "train_set = np.c_[X_train, y_train_array]\n",
    "y_test_array = np.array(y_test)\n",
    "test_set = np.c_[X_test, y_test_array]\n",
    "\n",
    "X_train_seq, y_train_seq = split_sequences(train_set, n_steps=5)\n",
    "X_test_seq, y_test_seq = split_sequences(test_set, n_steps=5)\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, F_train, F_test, y_train, y_test = train_test_split(X_train_scaled, feature_data, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "# CNN 모델 정의 (두 개의 CNN 분기)\n",
    "input_cnn = Input(shape=(X_train_seq.shape[1], X_train_seq.shape[2]))\n",
    "c1 = Conv1D(filters=64, kernel_size=3, activation='relu')(input_cnn)\n",
    "c1 = MaxPooling1D(pool_size=2)(c1)\n",
    "c1 = Conv1D(filters=128, kernel_size=3, activation='relu')(c1)\n",
    "c1 = MaxPooling1D(pool_size=2)(c1)\n",
    "c1 = Flatten()(c1)\n",
    "\n",
    "input_feature_cnn = Input(shape=(X_train_seq.shape[1], F_train.shape[1]))\n",
    "c2 = Conv1D(filters=32, kernel_size=3, activation='relu')(input_feature_cnn)\n",
    "c2 = MaxPooling1D(pool_size=2)(c2)\n",
    "c2 = Conv1D(filters=64, kernel_size=3, activation='relu')(c2)\n",
    "c2 = MaxPooling1D(pool_size=2)(c2)\n",
    "c2 = Flatten()(c2)\n",
    "\n",
    "combined = Concatenate()([c1, c2])\n",
    "dense1 = Dense(128, activation='relu')(combined)\n",
    "dropout = Dropout(0.5)(dense1)\n",
    "output = Dense(3, activation='softmax')(dropout)\n",
    "\n",
    "model = Model(inputs=[input_cnn, input_feature_cnn], outputs=output)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "model.fit([X_train_seq, F_train], y_train_seq, epochs=20, batch_size=32, validation_data=([X_test_seq, F_test], y_test_seq))\n",
    "\n",
    "# 모델 평가\n",
    "loss, accuracy = model.evaluate([X_test_seq, F_test], y_test_seq)\n",
    "print(f\"CNN + Feature CNN 기반 정적 행위 인식 정확도: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02234ab-c784-49c3-933c-9c3fcfde701f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
