{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CoDP8zQ6-0L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d749900-5eaa-44c9-db49-439026f6e69e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-b34f80922149>:9: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
            "<ipython-input-1-b34f80922149>:9: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
            "<ipython-input-1-b34f80922149>:9: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
            "<ipython-input-1-b34f80922149>:9: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
            "<ipython-input-1-b34f80922149>:9: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
            "<ipython-input-1-b34f80922149>:9: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
            "<ipython-input-1-b34f80922149>:9: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
            "<ipython-input-1-b34f80922149>:9: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
            "<ipython-input-1-b34f80922149>:9: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
            "<ipython-input-1-b34f80922149>:9: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
            "<ipython-input-1-b34f80922149>:9: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
            "<ipython-input-1-b34f80922149>:9: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
            "<ipython-input-1-b34f80922149>:9: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
            "<ipython-input-1-b34f80922149>:9: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
            "<ipython-input-1-b34f80922149>:9: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
            "<ipython-input-1-b34f80922149>:9: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
            "<ipython-input-1-b34f80922149>:9: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
            "<ipython-input-1-b34f80922149>:9: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
            "<ipython-input-1-b34f80922149>:9: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
            "<ipython-input-1-b34f80922149>:9: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4067, 9, 128]) torch.Size([4067]) torch.Size([4067, 3]) torch.Size([1560, 9, 128]) torch.Size([1560]) torch.Size([1560, 3])\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def load_file(filepath):\n",
        "    dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
        "    return dataframe.values\n",
        "\n",
        "def load_group(filenames, prefix=''):\n",
        "    loaded = []\n",
        "    for name in filenames:\n",
        "        data = load_file(prefix + name)\n",
        "        loaded.append(data)\n",
        "    loaded = np.dstack(loaded)\n",
        "    return loaded\n",
        "\n",
        "def load_dataset_group(group, prefix=''):\n",
        "    filepath = f\"{prefix}{group}/Inertial Signals/\"\n",
        "    filenames = [\n",
        "        f'total_acc_x_{group}.txt', f'total_acc_y_{group}.txt', f'total_acc_z_{group}.txt',\n",
        "        f'body_acc_x_{group}.txt', f'body_acc_y_{group}.txt', f'body_acc_z_{group}.txt',\n",
        "        f'body_gyro_x_{group}.txt', f'body_gyro_y_{group}.txt', f'body_gyro_z_{group}.txt'\n",
        "    ]\n",
        "    X = load_group(filenames, filepath)\n",
        "    y = load_file(f\"{prefix}{group}/y_{group}.txt\")\n",
        "    return X, y\n",
        "\n",
        "def filter_static_activities(X, y):\n",
        "    static_labels = [4, 5, 6]  # Sitting, Standing, Laying\n",
        "    mask = np.isin(y.flatten(), static_labels)\n",
        "    return X[mask], y[mask]\n",
        "\n",
        "def relabel_classes(y):\n",
        "    \"\"\" ÎùºÎ≤®ÏùÑ 4,5,6 -> 0,1,2 Î°ú Î≥ÄÌôò \"\"\"\n",
        "    mapping = {4: 0, 5: 1, 6: 2}\n",
        "    return np.vectorize(mapping.get)(y)\n",
        "\n",
        "def scale_data(trainX, testX):\n",
        "    \"\"\" StandardScalerÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Îç∞Ïù¥ÌÑ∞ Ïä§ÏºÄÏùºÎßÅ \"\"\"\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # (samples, timesteps, features) -> (samples * timesteps, features)\n",
        "    trainX_reshaped = trainX.reshape(-1, trainX.shape[-1])\n",
        "    testX_reshaped = testX.reshape(-1, testX.shape[-1])\n",
        "\n",
        "    # StandardScaler Ï†ÅÏö©\n",
        "    trainX_scaled = scaler.fit_transform(trainX_reshaped)\n",
        "    testX_scaled = scaler.transform(testX_reshaped)\n",
        "\n",
        "    # ÏõêÎûò ÌòïÌÉúÎ°ú Î≥µÍµ¨\n",
        "    trainX_scaled = trainX_scaled.reshape(trainX.shape)\n",
        "    testX_scaled = testX_scaled.reshape(testX.shape)\n",
        "\n",
        "    return trainX_scaled, testX_scaled\n",
        "\n",
        "def load_dataset(prefix):\n",
        "    trainX, trainy = load_dataset_group('train', prefix)\n",
        "    testX, testy = load_dataset_group('test', prefix)\n",
        "\n",
        "    trainX, trainy = filter_static_activities(trainX, trainy)\n",
        "    testX, testy = filter_static_activities(testX, testy)\n",
        "\n",
        "    # Îç∞Ïù¥ÌÑ∞ Ïä§ÏºÄÏùºÎßÅ Ï†ÅÏö©\n",
        "    trainX, testX = scale_data(trainX, testX)\n",
        "\n",
        "    # ÎùºÎ≤®ÏùÑ 0,1,2Î°ú Ïû¨Îß§Ìïë\n",
        "    trainy = relabel_classes(trainy)\n",
        "    testy = relabel_classes(testy)\n",
        "\n",
        "    trainX = torch.tensor(trainX, dtype=torch.float32).permute(0, 2, 1)\n",
        "    testX = torch.tensor(testX, dtype=torch.float32).permute(0, 2, 1)\n",
        "\n",
        "    trainy = torch.tensor(trainy, dtype=torch.long).squeeze()\n",
        "    testy = torch.tensor(testy, dtype=torch.long).squeeze()\n",
        "\n",
        "    trainy_one_hot = F.one_hot(trainy, num_classes=3).float()\n",
        "    testy_one_hot = F.one_hot(testy, num_classes=3).float()\n",
        "\n",
        "    print(trainX.shape, trainy.shape, trainy_one_hot.shape, testX.shape, testy.shape, testy_one_hot.shape)\n",
        "    return trainX, trainy, trainy_one_hot, testX, testy, testy_one_hot\n",
        "\n",
        "prefix = '/content/drive/MyDrive/Colab Notebooks/UCI-HAR/UCI-HAR/'\n",
        "\n",
        "trainX, trainy, trainy_one_hot, testX, testy, testy_one_hot = load_dataset(prefix)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RevIN(nn.Module):\n",
        "    def __init__(self, num_features, eps=1e-5, affine=True, subtract_last=True):\n",
        "        super().__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.affine = affine\n",
        "        self.subtract_last = subtract_last\n",
        "        if self.affine:\n",
        "            self.affine_weight = nn.Parameter(torch.ones(1, num_features, 1))  # Change shape to (1, num_features, 1)\n",
        "            self.affine_bias = nn.Parameter(torch.zeros(1, num_features, 1))  # Change shape to (1, num_features, 1)\n",
        "\n",
        "    def forward(self, x, mode):\n",
        "        if mode == 'norm':\n",
        "            self._get_statistics(x)\n",
        "            x = self._normalize(x)\n",
        "        elif mode == 'denorm':\n",
        "            x = self._denormalize(x)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        return x\n",
        "\n",
        "    def _get_statistics(self, x):\n",
        "        dim = tuple(range(1, x.ndim - 1))\n",
        "        if self.subtract_last:\n",
        "            self.last = x[:, -1, :].unsqueeze(1)\n",
        "        else:\n",
        "            self.mean = x.mean(dim=dim, keepdim=True).detach()\n",
        "        self.stdev = (x.var(dim=dim, keepdim=True, unbiased=False) + self.eps).sqrt().detach()\n",
        "\n",
        "    def _normalize(self, x):\n",
        "        x = (x - (self.last if self.subtract_last else self.mean)) / self.stdev\n",
        "        if self.affine:\n",
        "            x = x * self.affine_weight + self.affine_bias\n",
        "        return x\n",
        "\n",
        "    def _denormalize(self, x):\n",
        "        if self.affine:\n",
        "            x = (x - self.affine_bias) / (self.affine_weight + self.eps * self.eps)\n",
        "        x = x * self.stdev + (self.last if self.subtract_last else self.mean)\n",
        "        return x\n",
        "\n",
        "def get_conv1d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias):\n",
        "    return nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
        "                     padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
        "\n",
        "\n",
        "def get_bn(channels):\n",
        "    return nn.BatchNorm1d(channels)\n",
        "\n",
        "def conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups, dilation=1,bias=False):\n",
        "    if padding is None:\n",
        "        padding = kernel_size // 2\n",
        "    result = nn.Sequential()\n",
        "    result.add_module('conv', get_conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
        "                                         stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias))\n",
        "    result.add_module('bn', get_bn(out_channels))\n",
        "    return result\n",
        "\n",
        "def fuse_bn(conv, bn):\n",
        "\n",
        "    kernel = conv.weight\n",
        "    running_mean = bn.running_mean\n",
        "    running_var = bn.running_var\n",
        "    gamma = bn.weight\n",
        "    beta = bn.bias\n",
        "    eps = bn.eps\n",
        "    std = (running_var + eps).sqrt()\n",
        "    t = (gamma / std).reshape(-1, 1, 1)\n",
        "    return kernel * t, beta - running_mean * gamma / std\n",
        "\n",
        "class MovingAvg(nn.Module):\n",
        "    def __init__(self, kernel_size, stride=1):\n",
        "        super().__init__()\n",
        "        self.avg = nn.AvgPool1d(kernel_size, stride, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        pad = (self.avg.kernel_size - 1) // 2\n",
        "        x = F.pad(x, (pad, pad), mode='replicate')\n",
        "        return self.avg(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "class SeriesDecomp(nn.Module):\n",
        "    def __init__(self, kernel_size):\n",
        "        super().__init__()\n",
        "        self.moving_avg = MovingAvg(kernel_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        trend = self.moving_avg(x)\n",
        "        return x - trend, trend\n",
        "\n",
        "class ConvFFN(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, groups, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, out_channels, kernel_size=1, groups=groups),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Conv1d(out_channels, in_channels, kernel_size=1, groups=groups if groups != in_channels else 1), # Changed out_channels to in_channels to ensure correct channel dimensions\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.ffn(x)\n",
        "\n",
        "class ReparamLargeKernelConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, small_kernel, stride=1, groups=1, small_kernel_merged=False):\n",
        "        super().__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.small_kernel = small_kernel\n",
        "        padding = kernel_size // 2\n",
        "\n",
        "        if small_kernel_merged:\n",
        "            self.lkb_reparam = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, groups=groups, bias=True)\n",
        "        else:\n",
        "            self.lkb_origin = nn.Sequential(\n",
        "                nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, groups=groups, bias=False),\n",
        "                nn.BatchNorm1d(out_channels)\n",
        "            )\n",
        "            if small_kernel is not None:\n",
        "                assert small_kernel <= kernel_size, \"Small kernel must be smaller than large kernel\"\n",
        "                self.small_conv = nn.Sequential(\n",
        "                    nn.Conv1d(in_channels, out_channels, small_kernel, stride, small_kernel // 2, groups=groups, bias=False),\n",
        "                    nn.BatchNorm1d(out_channels)\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if hasattr(self, 'lkb_reparam'):\n",
        "            return self.lkb_reparam(x)\n",
        "        else:\n",
        "            out = self.lkb_origin(x)\n",
        "            if hasattr(self, 'small_conv'):\n",
        "                out += self.small_conv(x)\n",
        "            return out\n",
        "\n",
        "# Layer Norm Ï†ÅÏö©\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, num_features, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(num_features, eps=eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.norm(x.transpose(-1, -2)).transpose(-1, -2)\n",
        "\n",
        "class Flatten_Head(nn.Module):\n",
        "    def __init__(self, individual, n_vars, nf, target_window, head_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten(start_dim=-2)\n",
        "        self.individual = individual\n",
        "\n",
        "        if self.individual:\n",
        "            self.linear = nn.ModuleList([nn.Linear(nf, target_window) for _ in range(n_vars)])\n",
        "        else:\n",
        "            self.linear = nn.Linear(nf, target_window)\n",
        "\n",
        "        self.dropout = nn.Dropout(head_dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        if self.individual:\n",
        "            x = torch.cat([self.linear[i](x[:, i, :]) for i in range(len(self.linear))], dim=1)\n",
        "        else:\n",
        "            x = self.linear(x)\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class Stage(nn.Module):\n",
        "    def __init__(self, ffn_ratio, num_blocks, large_size, small_size, dmodel, dw_model, nvars,\n",
        "                 small_kernel_merged=False, drop=0.1):\n",
        "\n",
        "        super(Stage, self).__init__()\n",
        "        d_ffn = dmodel * ffn_ratio\n",
        "        blks = []\n",
        "        for i in range(num_blocks):\n",
        "            blk = Block(dmodel=dmodel, dff=d_ffn, nvars=nvars, small_kernel_merged=small_kernel_merged, drop=drop, large_size=large_size, small_size=small_size) # Pass large_size and small_size as keyword arguments\n",
        "            blks.append(blk)\n",
        "        self.blocks = nn.ModuleList(blks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class ReparamLargeKernelConv(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride, groups,\n",
        "                 small_kernel,\n",
        "                 small_kernel_merged=False, nvars=7):\n",
        "        super(ReparamLargeKernelConv, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.small_kernel = small_kernel\n",
        "        # We assume the conv does not change the feature map size, so padding = k//2. Otherwise, you may configure padding as you wish, and change the padding of small_conv accordingly.\n",
        "        padding = kernel_size // 2\n",
        "        if small_kernel_merged:\n",
        "            self.lkb_reparam = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
        "                                         stride=stride, padding=padding, dilation=1, groups=groups, bias=True)\n",
        "        else:\n",
        "            self.lkb_origin = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
        "                                        stride=stride, padding=padding, dilation=1, groups=groups,bias=False)\n",
        "            if small_kernel is not None:\n",
        "                assert small_kernel <= kernel_size, 'The kernel size for re-param cannot be larger than the large kernel!'\n",
        "                self.small_conv = conv_bn(in_channels=in_channels, out_channels=out_channels,\n",
        "                                            kernel_size=small_kernel,\n",
        "                                            stride=stride, padding=small_kernel // 2, groups=groups, dilation=1,bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        if hasattr(self, 'lkb_reparam'):\n",
        "            out = self.lkb_reparam(inputs)\n",
        "        else:\n",
        "            out = self.lkb_origin(inputs)\n",
        "            if hasattr(self, 'small_conv'):\n",
        "                out += self.small_conv(inputs)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, dmodel, dff, nvars, large_size, small_size, small_kernel_merged=False, drop=0.1, dims=None):\n",
        "        super().__init__()\n",
        "\n",
        "        if dims is None:\n",
        "            dims = [64, 128, 256]  # ‚úÖ Í∏∞Î≥∏Í∞í Ï∂îÍ∞Ä\n",
        "\n",
        "        kernel_size = large_size\n",
        "        small_kernel = small_size\n",
        "\n",
        "        # ‚úÖ ÏàòÏ†ïÎêú Î∂ÄÎ∂Ñ: dims[0] ÎåÄÏã† dmodel ÏÇ¨Ïö©\n",
        "        self.conv = ReparamLargeKernelConv(\n",
        "            in_channels=dmodel,\n",
        "            out_channels=dmodel,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=1,\n",
        "            groups=dmodel,\n",
        "            small_kernel=small_kernel\n",
        "        )\n",
        "\n",
        "        self.norm = LayerNorm(dmodel)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Conv1d(dmodel, dff, kernel_size=1, groups=dmodel),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop),\n",
        "            nn.Conv1d(dff, dmodel, kernel_size=1, groups=dmodel),\n",
        "            nn.Dropout(drop)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        res = x\n",
        "        x = self.norm(self.conv(x))\n",
        "        x = self.ffn(x)\n",
        "        return res + x\n",
        "\n",
        "\n",
        "class Stage(nn.Module):\n",
        "    def __init__(self, ffn_ratio, num_blocks, large_size, small_size, dmodel, dw_model, nvars,\n",
        "                 small_kernel_merged=False, drop=0.1, dims=None):  # ‚úÖ dims Ï∂îÍ∞Ä\n",
        "        super(Stage, self).__init__()\n",
        "\n",
        "        d_ffn = dmodel * ffn_ratio\n",
        "        blks = []\n",
        "        for i in range(num_blocks):\n",
        "            blk = Block(\n",
        "                dmodel=dmodel, dff=d_ffn, nvars=nvars, small_kernel_merged=small_kernel_merged,\n",
        "                drop=drop, large_size=large_size, small_size=small_size, dims=dims  # ‚úÖ dims Ï†ÑÎã¨\n",
        "            )\n",
        "            blks.append(blk)\n",
        "\n",
        "        self.blocks = nn.ModuleList(blks)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class ModernTCN(nn.Module):\n",
        "    def __init__(self, task_name, patch_size, patch_stride, stem_ratio, downsample_ratio, ffn_ratio, num_blocks,\n",
        "                 large_size, small_size, dims, dw_dims, nvars, small_kernel_merged=False, backbone_dropout=0.1,\n",
        "                 head_dropout=0.1, use_multi_scale=True, revin=True, affine=True, subtract_last=False,\n",
        "                 freq=None, seq_len=512, c_in=7, individual=False, target_window=96, class_drop=0., class_num=3):\n",
        "        super(ModernTCN, self).__init__()\n",
        "\n",
        "        # Store the task_name\n",
        "        self.patch_size = patch_size\n",
        "        self.patch_stride = patch_stride\n",
        "        self.task_name = task_name  # Add this line to store the task_name\n",
        "        self.downsample_ratio = downsample_ratio\n",
        "        self.num_stage = len(num_blocks)\n",
        "        self.class_drop = class_drop\n",
        "        self.class_num = class_num\n",
        "\n",
        "        # RevIN\n",
        "        self.revin = revin\n",
        "        if self.revin:\n",
        "            self.revin_layer = RevIN(c_in, affine=affine, subtract_last=subtract_last)\n",
        "\n",
        "        # üîπ stem Î†àÏù¥Ïñ¥ Ï†ïÏùò\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=nvars, out_channels=dims[0], kernel_size=patch_size, stride=patch_stride),\n",
        "            nn.BatchNorm1d(dims[0])\n",
        "        )\n",
        "\n",
        "        self.downsample_layers = nn.ModuleList()\n",
        "        self.downsample_layers.append(self.stem)\n",
        "\n",
        "        if self.num_stage > 1:\n",
        "            for i in range(self.num_stage - 1):\n",
        "                downsample_layer = nn.Sequential(\n",
        "                    nn.BatchNorm1d(dims[i]),\n",
        "                    nn.Conv1d(dims[i], dims[i + 1], kernel_size=downsample_ratio, stride=downsample_ratio),\n",
        "                )\n",
        "                self.downsample_layers.append(downsample_layer)\n",
        "\n",
        "        self.stages = nn.ModuleList()\n",
        "        for stage_idx in range(self.num_stage):\n",
        "            layer = Stage(ffn_ratio, num_blocks[stage_idx], large_size[stage_idx], small_size[stage_idx],\n",
        "                          dmodel=dims[stage_idx], dw_model=dw_dims[stage_idx], nvars=nvars,\n",
        "                          small_kernel_merged=small_kernel_merged, drop=backbone_dropout)\n",
        "            self.stages.append(layer)\n",
        "\n",
        "        # head Ï†ïÏùò\n",
        "        patch_num = seq_len // patch_stride\n",
        "        self.n_vars = nvars\n",
        "        self.individual = individual\n",
        "        d_model = dims[self.num_stage-1]\n",
        "\n",
        "        if use_multi_scale:\n",
        "            self.head_nf = d_model * patch_num\n",
        "            self.head = Flatten_Head(self.individual, self.n_vars, self.head_nf, target_window,\n",
        "                                     head_dropout=head_dropout)\n",
        "        else:\n",
        "            if patch_num % pow(downsample_ratio, (self.num_stage - 1)) == 0:\n",
        "                self.head_nf = d_model * patch_num // pow(downsample_ratio, (self.num_stage - 1))\n",
        "            else:\n",
        "                self.head_nf = d_model * (patch_num // pow(downsample_ratio, (self.num_stage - 1)) + 1)\n",
        "\n",
        "            self.head = Flatten_Head(self.individual, self.n_vars, self.head_nf, target_window,\n",
        "                                     head_dropout=head_dropout)\n",
        "\n",
        "        if task_name == 'classification':\n",
        "            self.act_class = F.gelu\n",
        "            self.class_dropout = nn.Dropout(self.class_drop)\n",
        "            self.head_class = nn.Linear(nvars * self.head_nf, self.class_num)\n",
        "\n",
        "    def forward_feature(self, x, te=None):\n",
        "        B, M, L = x.shape\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        for i in range(self.num_stage):\n",
        "            B, C, N = x.shape\n",
        "\n",
        "            if i == 0 and self.patch_size != self.patch_stride:\n",
        "                pad_len = self.patch_size - self.patch_stride\n",
        "                pad = x[:, :, -1:].repeat(1, 1, pad_len)\n",
        "                x = torch.cat([x, pad], dim=-1)\n",
        "\n",
        "            if N % self.downsample_ratio != 0:\n",
        "                pad_len = self.downsample_ratio - (N % self.downsample_ratio)\n",
        "                x = torch.cat([x, x[:, :, -pad_len:]], dim=-1)\n",
        "\n",
        "            # ‚úÖ ÏàòÏ†ïÎêú Î∂ÄÎ∂Ñ: BatchNorm1dÍ∞Ä ÏïÑÎãå Conv1dÏóêÏÑú in_channelsÎ•º ÌôïÏù∏\n",
        "            if i > 0 and x.shape[1] != self.downsample_layers[i][1].in_channels:  # ‚úÖ [0] -> [1]ÏúºÎ°ú Î≥ÄÍ≤Ω\n",
        "                x = x.permute(0, 2, 1)\n",
        "                x = x.permute(0, 2, 1)\n",
        "\n",
        "            x = self.downsample_layers[i](x)\n",
        "            x = self.stages[i](x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, te=None):\n",
        "        if self.task_name == 'classification':\n",
        "            x = self.forward_feature(x)\n",
        "        return x\n",
        "\n",
        "    def structural_reparam(self):\n",
        "        for m in self.modules():\n",
        "            if hasattr(m, 'merge_kernel'):\n",
        "                m.merge_kernel()\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "class DOLGLayer(nn.Module):\n",
        "    \"\"\" DOLG-style feature fusion layer \"\"\"\n",
        "    def __init__(self, input_dim, s3_dim, reduction_dim):\n",
        "        super(DOLGLayer, self).__init__()\n",
        "        self.fc_t = nn.Linear(input_dim, s3_dim, bias=True)\n",
        "        self.localmodel = nn.Linear(input_dim, s3_dim, bias=True)\n",
        "        self.fc = nn.Linear(s3_dim * 2, reduction_dim, bias=True)\n",
        "\n",
        "    def forward(self, embed1, embed2):\n",
        "        # Reshape embed1 before passing to localmodel\n",
        "        embed1 = embed1.mean(dim=-1)  # Average along the sequence dimension\n",
        "\n",
        "\n",
        "        fg = self.fc_t(embed2)  # Global feature transformation\n",
        "        fg_norm = torch.norm(fg, p=2, dim=1, keepdim=True)\n",
        "\n",
        "        fl = self.localmodel(embed1)  # Local feature transformation\n",
        "        proj = torch.sum(fg * fl, dim=1, keepdim=True) / (fg_norm ** 2 + 1e-6)\n",
        "        proj = proj * fg\n",
        "        fo = fl - proj  # Orthogonal complement\n",
        "\n",
        "        final_feat = torch.cat((fg, fo), dim=1)  # Concatenate features\n",
        "        global_feature = self.fc(final_feat)  # Final transformation\n",
        "        return global_feature\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, configs):\n",
        "        super(Model, self).__init__()  # ‚úÖ Î∂ÄÎ™® ÌÅ¥ÎûòÏä§ Ï¥àÍ∏∞Ìôî\n",
        "\n",
        "        self.revin = configs.get('revin', False)\n",
        "\n",
        "        if self.revin:\n",
        "            self.revin_layer = RevIN(configs['input_dim'], affine=configs.get('affine', True), subtract_last=configs.get('subtract_last', False))\n",
        "\n",
        "        self.tcn = ModernTCN(\n",
        "            task_name='classification',\n",
        "            nvars=configs['input_dim'],\n",
        "            class_num=configs['output_dim'],\n",
        "            patch_size=configs['kernel_size'],\n",
        "            small_size=configs['small_size'],\n",
        "            target_window=configs['target_window'],\n",
        "            num_blocks=configs['num_blocks'],\n",
        "            dims=configs['dims'],\n",
        "            dw_dims=configs['dw_dims'],\n",
        "            backbone_dropout=configs['dropout'],\n",
        "            patch_stride=configs['patch_stride'],\n",
        "            stem_ratio=configs['stem_ratio'],\n",
        "            downsample_ratio=configs['downsample_ratio'],\n",
        "            ffn_ratio=configs['ffn_ratio'],\n",
        "            large_size=configs['large_size']\n",
        "        )\n",
        "\n",
        "        self.dolg = DOLGLayer(\n",
        "            input_dim=configs['dims'][-1],  # ModernTCNÏùò ÏµúÏ¢Ö Ï∂úÎ†• Ï∞®Ïõê\n",
        "            s3_dim=configs['s3_dim'],\n",
        "            reduction_dim=configs['output_dim']\n",
        "        )\n",
        "\n",
        "\n",
        "        encoder_layers = TransformerEncoderLayer(\n",
        "            d_model=configs['dims'][-1],  # ModernTCNÏùò ÏµúÏ¢Ö Ï∂úÎ†• Ï∞®Ïõê\n",
        "            nhead=configs['num_heads'],\n",
        "            dim_feedforward=configs['dim_feedforward'],\n",
        "            dropout=configs['dropout'],\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers=int(configs['num_stages']))\n",
        "        self.projection = nn.Linear(configs['output_dim'], configs['output_dim'])\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" üîπ Î™®Îç∏Ïùò ÏàúÏ†ÑÌåå (Forward) Ìï®Ïàò Íµ¨ÌòÑ \"\"\"\n",
        "\n",
        "        if self.revin:\n",
        "            x = self.revin_layer(x, mode='norm')\n",
        "\n",
        "        # ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞Î•º Conv1dÍ∞Ä Ï≤òÎ¶¨Ìï† Ïàò ÏûàÎèÑÎ°ù Î≥ÄÌôò\n",
        "        x = x.permute(0, 2, 1)  # ‚úÖ (batch, seq_len, channels) -> (batch, channels, seq_len)\n",
        "\n",
        "        # TCN ÌÜµÍ≥º\n",
        "        x_tcn = self.tcn(x)  # ModernTCNÏùò Ï∂úÎ†•\n",
        "\n",
        "        # Transformer Encoder ÌÜµÍ≥º\n",
        "        x_transformer = self.transformer_encoder(x_tcn.permute(0, 2, 1))\n",
        "        x_transformer = x_transformer.permute(0, 2, 1).mean(dim=-1)\n",
        "\n",
        "        # DOLG Ï†ÅÏö©\n",
        "        x = self.dolg(x_tcn, x_transformer)  # Ï∞®Ïõê ÎßûÏ∂îÍ∏∞ ÌïÑÏöî\n",
        "\n",
        "        # ÏµúÏ¢Ö ÌîÑÎ°úÏ†ùÏÖò\n",
        "        x = self.projection(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "snib_i_faPv3"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    def __init__(self, smoothing=0.0):\n",
        "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
        "        self.smoothing = smoothing\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        log_prob = F.log_softmax(input, dim=-1)\n",
        "        weight = input.new_ones(input.size()) * \\\n",
        "                 self.smoothing / (input.size(-1) - 1.)\n",
        "        weight.scatter_(-1, target.unsqueeze(-1), (1. - self.smoothing))\n",
        "        loss = (-weight * log_prob).sum(dim=-1).mean()\n",
        "        return loss\n",
        "\n",
        "def train_model(model, train_loader, test_loader, epochs=20, lr=0.001, device='cuda'):\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "            total += y_batch.size(0)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}, Accuracy: {correct/total:.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "            total += y_batch.size(0)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    print(f\"Test Accuracy: {correct/total:.4f}, F1 Score: {f1:.4f}\")\n",
        "    return model\n",
        "\n",
        "# Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
        "batch_size = 32\n",
        "train_dataset = TensorDataset(trainX, trainy)\n",
        "test_dataset = TensorDataset(testX, testy)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Î™®Îç∏ Ï¥àÍ∏∞Ìôî\n",
        "configs = {\n",
        "    'input_dim': 9,\n",
        "    'output_dim': 3,\n",
        "    'kernel_size': 11,\n",
        "    'small_size': [3, 3, 3],\n",
        "    'target_window': 96,\n",
        "    'num_blocks': [2, 2, 2],\n",
        "    'dims': [64, 128, 256],  # Ensure this is divisible by num_heads\n",
        "    'dw_dims': [64, 128, 256],\n",
        "    'dropout': 0.1,\n",
        "    'patch_stride': 1,\n",
        "    'stem_ratio': 1,\n",
        "    'downsample_ratio': 2,\n",
        "    'ffn_ratio': 4,\n",
        "    'large_size': [11, 7, 5],\n",
        "    's3_dim': 128,\n",
        "    'dim_feedforward': 512,\n",
        "    'num_stages': 3,\n",
        "    'num_heads': 8,  # Changed from 9 to 8 (256 is divisible by 8)\n",
        "    'revin': True\n",
        "}\n",
        "\n",
        "# üîπ Î™®Îç∏ Ï¥àÍ∏∞Ìôî\n",
        "model = Model(configs)\n",
        "\n",
        "# Î™®Îç∏ ÌõàÎ†®\n",
        "trained_model = train_model(model, train_loader, test_loader, epochs=50, lr=0.001)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55gUo5zXW-4o",
        "outputId": "7099a82d-8aeb-4e71-bf54-61d0efe35bc9"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.5932, Accuracy: 0.8753\n",
            "Epoch [2/50], Loss: 0.5493, Accuracy: 0.9071\n",
            "Epoch [3/50], Loss: 0.5427, Accuracy: 0.9048\n",
            "Epoch [4/50], Loss: 0.5300, Accuracy: 0.9189\n",
            "Epoch [5/50], Loss: 0.5214, Accuracy: 0.9257\n",
            "Epoch [6/50], Loss: 0.5765, Accuracy: 0.8987\n",
            "Epoch [7/50], Loss: 0.5305, Accuracy: 0.9356\n",
            "Epoch [8/50], Loss: 0.5118, Accuracy: 0.9393\n",
            "Epoch [9/50], Loss: 0.4978, Accuracy: 0.9562\n",
            "Epoch [10/50], Loss: 0.4829, Accuracy: 0.9589\n",
            "Epoch [11/50], Loss: 0.4830, Accuracy: 0.9609\n",
            "Epoch [12/50], Loss: 0.4788, Accuracy: 0.9639\n",
            "Epoch [13/50], Loss: 0.4668, Accuracy: 0.9678\n",
            "Epoch [14/50], Loss: 0.4600, Accuracy: 0.9744\n",
            "Epoch [15/50], Loss: 0.4579, Accuracy: 0.9742\n",
            "Epoch [16/50], Loss: 0.4502, Accuracy: 0.9774\n",
            "Epoch [17/50], Loss: 0.4449, Accuracy: 0.9803\n",
            "Epoch [18/50], Loss: 0.4465, Accuracy: 0.9825\n",
            "Epoch [19/50], Loss: 0.4495, Accuracy: 0.9781\n",
            "Epoch [20/50], Loss: 0.4413, Accuracy: 0.9835\n",
            "Epoch [21/50], Loss: 0.4542, Accuracy: 0.9796\n",
            "Epoch [22/50], Loss: 0.4400, Accuracy: 0.9835\n",
            "Epoch [23/50], Loss: 0.4379, Accuracy: 0.9860\n",
            "Epoch [24/50], Loss: 0.4437, Accuracy: 0.9855\n",
            "Epoch [25/50], Loss: 0.4327, Accuracy: 0.9867\n",
            "Epoch [26/50], Loss: 0.4353, Accuracy: 0.9880\n",
            "Epoch [27/50], Loss: 0.4269, Accuracy: 0.9904\n",
            "Epoch [28/50], Loss: 0.4218, Accuracy: 0.9914\n",
            "Epoch [29/50], Loss: 0.4240, Accuracy: 0.9892\n",
            "Epoch [30/50], Loss: 0.4254, Accuracy: 0.9880\n",
            "Epoch [31/50], Loss: 0.4224, Accuracy: 0.9907\n",
            "Epoch [32/50], Loss: 0.4297, Accuracy: 0.9875\n",
            "Epoch [33/50], Loss: 0.4251, Accuracy: 0.9887\n",
            "Epoch [34/50], Loss: 0.4192, Accuracy: 0.9914\n",
            "Epoch [35/50], Loss: 0.4190, Accuracy: 0.9946\n",
            "Epoch [36/50], Loss: 0.4187, Accuracy: 0.9936\n",
            "Epoch [37/50], Loss: 0.4208, Accuracy: 0.9914\n",
            "Epoch [38/50], Loss: 0.4194, Accuracy: 0.9929\n",
            "Epoch [39/50], Loss: 0.4181, Accuracy: 0.9946\n",
            "Epoch [40/50], Loss: 0.4162, Accuracy: 0.9953\n",
            "Epoch [41/50], Loss: 0.4173, Accuracy: 0.9961\n",
            "Epoch [42/50], Loss: 0.4122, Accuracy: 0.9966\n",
            "Epoch [43/50], Loss: 0.4115, Accuracy: 0.9966\n",
            "Epoch [44/50], Loss: 0.4101, Accuracy: 0.9975\n",
            "Epoch [45/50], Loss: 0.4128, Accuracy: 0.9973\n",
            "Epoch [46/50], Loss: 0.4308, Accuracy: 0.9904\n",
            "Epoch [47/50], Loss: 0.4273, Accuracy: 0.9892\n",
            "Epoch [48/50], Loss: 0.4165, Accuracy: 0.9941\n",
            "Epoch [49/50], Loss: 0.4102, Accuracy: 0.9968\n",
            "Epoch [50/50], Loss: 0.4131, Accuracy: 0.9956\n",
            "Test Accuracy: 0.9750, F1 Score: 0.9745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "# Î™®Îç∏ Ïû•Ïπò ÏÑ§Ï†ï\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Î™®Îç∏ ÌèâÍ∞Ä Ìï®Ïàò\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "            total += y_batch.size(0)\n",
        "            all_preds.extend(predicted.cpu().tolist())\n",
        "            all_labels.extend(y_batch.cpu().tolist())\n",
        "\n",
        "    if total == 0:\n",
        "        print(\"ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
        "        return\n",
        "\n",
        "    # ÌèâÍ∞Ä ÏßÄÌëú Í≥ÑÏÇ∞\n",
        "    accuracy = correct / total\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    precision = precision_score(all_labels, all_preds, average='macro')\n",
        "    recall = recall_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
        "\n",
        "    # Confusion Matrix Í≥ÑÏÇ∞ Î∞è ÏãúÍ∞ÅÌôî\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    labels = np.unique(all_labels)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "# Î™®Îç∏ ÌèâÍ∞Ä Ïã§Ìñâ\n",
        "evaluate_model(model, test_loader, device)\n",
        "\n"
      ],
      "metadata": {
        "id": "nL8jxtbPX7t0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "outputId": "75878ee1-82e9-490c-f0c4-16c658c51eab"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9750\n",
            "Precision: 0.9753, Recall: 0.9741, F1 Score: 0.9745\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Î™®Îç∏ Ï†ÄÏû• (Í∞ÄÏ§ëÏπòÎßå Ï†ÄÏû•)\n",
        "torch.save(model.state_dict(), \"model_weights.pth\")\n",
        "\n",
        "# Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞ (ÎèôÏùºÌïú Î™®Îç∏ Íµ¨Ï°∞Î•º Î®ºÏ†Ä Ï†ïÏùòÌï¥Ïïº Ìï®)\n",
        "model.load_state_dict(torch.load(\"model_weights.pth\"))\n",
        "model.eval()  # ÌèâÍ∞Ä Î™®ÎìúÎ°ú ÏÑ§Ï†ï\n"
      ],
      "metadata": {
        "id": "gp41pPazX79o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "401a8b28-41fe-4b88-a7db-25b868f9ff31"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-205-a370004e32d6>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"model_weights.pth\"))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (revin_layer): RevIN()\n",
              "  (tcn): ModernTCN(\n",
              "    (revin_layer): RevIN()\n",
              "    (stem): Sequential(\n",
              "      (0): Conv1d(9, 64, kernel_size=(11,), stride=(1,))\n",
              "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (downsample_layers): ModuleList(\n",
              "      (0): Sequential(\n",
              "        (0): Conv1d(9, 64, kernel_size=(11,), stride=(1,))\n",
              "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (1): Conv1d(64, 128, kernel_size=(2,), stride=(2,))\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (1): Conv1d(128, 256, kernel_size=(2,), stride=(2,))\n",
              "      )\n",
              "    )\n",
              "    (stages): ModuleList(\n",
              "      (0): Stage(\n",
              "        (blocks): ModuleList(\n",
              "          (0-1): 2 x Block(\n",
              "            (conv): ReparamLargeKernelConv(\n",
              "              (lkb_origin): Sequential(\n",
              "                (conv): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,), groups=64, bias=False)\n",
              "                (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (small_conv): Sequential(\n",
              "                (conv): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), groups=64, bias=False)\n",
              "                (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm(\n",
              "              (norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
              "            )\n",
              "            (ffn): Sequential(\n",
              "              (0): Conv1d(64, 256, kernel_size=(1,), stride=(1,), groups=64)\n",
              "              (1): GELU(approximate='none')\n",
              "              (2): Dropout(p=0.1, inplace=False)\n",
              "              (3): Conv1d(256, 64, kernel_size=(1,), stride=(1,), groups=64)\n",
              "              (4): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): Stage(\n",
              "        (blocks): ModuleList(\n",
              "          (0-1): 2 x Block(\n",
              "            (conv): ReparamLargeKernelConv(\n",
              "              (lkb_origin): Sequential(\n",
              "                (conv): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,), groups=128, bias=False)\n",
              "                (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (small_conv): Sequential(\n",
              "                (conv): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), groups=128, bias=False)\n",
              "                (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm(\n",
              "              (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
              "            )\n",
              "            (ffn): Sequential(\n",
              "              (0): Conv1d(128, 512, kernel_size=(1,), stride=(1,), groups=128)\n",
              "              (1): GELU(approximate='none')\n",
              "              (2): Dropout(p=0.1, inplace=False)\n",
              "              (3): Conv1d(512, 128, kernel_size=(1,), stride=(1,), groups=128)\n",
              "              (4): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): Stage(\n",
              "        (blocks): ModuleList(\n",
              "          (0-1): 2 x Block(\n",
              "            (conv): ReparamLargeKernelConv(\n",
              "              (lkb_origin): Sequential(\n",
              "                (conv): Conv1d(256, 256, kernel_size=(5,), stride=(1,), padding=(2,), groups=256, bias=False)\n",
              "                (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "              (small_conv): Sequential(\n",
              "                (conv): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), groups=256, bias=False)\n",
              "                (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "              )\n",
              "            )\n",
              "            (norm): LayerNorm(\n",
              "              (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
              "            )\n",
              "            (ffn): Sequential(\n",
              "              (0): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), groups=256)\n",
              "              (1): GELU(approximate='none')\n",
              "              (2): Dropout(p=0.1, inplace=False)\n",
              "              (3): Conv1d(1024, 256, kernel_size=(1,), stride=(1,), groups=256)\n",
              "              (4): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (head): Flatten_Head(\n",
              "      (flatten): Flatten(start_dim=-2, end_dim=-1)\n",
              "      (linear): Linear(in_features=131072, out_features=96, bias=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (class_dropout): Dropout(p=0.0, inplace=False)\n",
              "    (head_class): Linear(in_features=1179648, out_features=3, bias=True)\n",
              "  )\n",
              "  (dolg): DOLGLayer(\n",
              "    (fc_t): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (localmodel): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (fc): Linear(in_features=256, out_features=3, bias=True)\n",
              "  )\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-2): 3 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
              "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (projection): Linear(in_features=3, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    }
  ]
}